INFO 09-02 09:33:46 llm_engine.py:169] Initializing an LLM engine (v0.5.0) with config: model='facebook/opt-6.7b', speculative_config=None, tokenizer='facebook/opt-6.7b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=facebook/opt-6.7b)
INFO 09-02 09:33:47 selector.py:127] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
INFO 09-02 09:33:47 selector.py:55] Using XFormers backend.
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:33:49 selector.py:127] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:33:49 selector.py:55] Using XFormers backend.
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:33:50 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
INFO 09-02 09:33:50 selector.py:127] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
INFO 09-02 09:33:50 selector.py:55] Using XFormers backend.
INFO 09-02 09:33:51 weight_utils.py:218] Using model weights format ['*.bin']
INFO 09-02 09:34:02 model_runner.py:218] Loading model weights took 12.4036 GB
INFO 09-02 09:34:03 distributed_gpu_executor.py:59] # GPU blocks: 1889, # CPU blocks: 512
INFO 09-02 09:34:03 selector.py:127] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
INFO 09-02 09:34:03 selector.py:55] Using XFormers backend.
INFO 09-02 09:34:06 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:34:06 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:06 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:06 pynccl.py:65] vLLM is using nccl==2.20.5
INFO 09-02 09:34:06 custom_all_reduce_utils.py:179] reading GPU P2P access cache from /home/lrq619/.config/vllm/gpu_p2p_access_cache_for_0,1.json
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:06 custom_all_reduce_utils.py:179] reading GPU P2P access cache from /home/lrq619/.config/vllm/gpu_p2p_access_cache_for_0,1.json
WARNING 09-02 09:34:06 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:34:06 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:34:06 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:34:06 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:06 selector.py:127] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:06 selector.py:55] Using XFormers backend.
It takes: 0.79s to send model shards
After sending shards, there are 8.39GB remaining on GPU0
INFO 09-02 09:34:07 worker.py:151] send weights shards takes: 0.86s, sent out: 6.21GB, sent bw: 7.25GB/s
INFO 09-02 09:34:07 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.08s to init model weights
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.71s to recv shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes 0.02 to load shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 24.580383GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:07 selector.py:127] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:07 selector.py:55] Using XFormers backend.
INFO 09-02 09:34:08 cache_engine.py:163] send kvc shards takes: 0.75s, sent out: 7.38GB, sent bw: 9.88GB/s
INFO 09-02 09:34:08 cache_engine.py:196] After deleting layer's shard, free mem: 8594.31MB
INFO 09-02 09:34:08 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:34:08 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 16.00 GB
INFO 09-02 09:34:08 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #5251
available space before extending GPU blocks: 16.000GB
available space after extending GPU blocks: 2.643GB, free_mem decreased: 13.357GB
INFO 09-02 09:34:08 worker.py:209] extend gpu in worker takes: 0.45s
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 17.143GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.018GB, free_mem decreased: 13.125GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:08 worker.py:209] extend gpu in worker takes: 0.45s
INFO 09-02 09:34:08 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.45s, there are 2.643GB left on GPU 0
do_liquid latency: 2.88s
INFO 09-02 09:34:08 llm_engine.py:757] Finished liquid for 0 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.78s, update worker latency: 0.00s, liquid model weights latency: 0.86s, init mem latency: 0.00s, liquid kvc latency: 0.92s;, current free mem on GPU0: 2.64GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:34:11 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:34:11 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.73s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.30GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:12 worker.py:151] send weights shards takes: 0.76s, sent out: 6.19GB, sent bw: 8.10GB/s
free mem after recving shards_weights: 10.597961GB
free mem after appending shards_weights: 9.322571GB
free mem after deleting shards_weights: 9.322571GB
INFO 09-02 09:34:12 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:12 cache_engine.py:163] send kvc shards takes: 0.79s, sent out: 7.38GB, sent bw: 9.33GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:12 cache_engine.py:196] After deleting layer's shard, free mem: 23864.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:13 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:34:13 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.72 GB
do_liquid latency: 3.39s
INFO 09-02 09:34:13 llm_engine.py:757] Finished liquid for 1 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.11s, update worker latency: 0.00s, liquid model weights latency: 0.84s, init mem latency: 0.00s, liquid kvc latency: 1.27s;, current free mem on GPU0: 1.72GB
INFO 09-02 09:34:14 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:14 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:34:14 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:14 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:34:14 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-02 09:34:14 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:34:14 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:34:14 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.68s to send model shards
After sending shards, there are 8.48GB remaining on GPU0
INFO 09-02 09:34:15 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.34GB/s
INFO 09-02 09:34:15 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:34:15 cache_engine.py:163] send kvc shards takes: 0.75s, sent out: 7.38GB, sent bw: 9.83GB/s
INFO 09-02 09:34:15 cache_engine.py:196] After deleting layer's shard, free mem: 8686.31MB
INFO 09-02 09:34:15 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:34:15 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 15.86 GB
INFO 09-02 09:34:15 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #5214
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.545227GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 24.117493GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 24.117493GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.680GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 3.744GB, free_mem decreased: 12.936GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:16 worker.py:209] extend gpu in worker takes: 0.41s
available space before extending GPU blocks: 15.858GB
available space after extending GPU blocks: 2.922GB, free_mem decreased: 12.936GB
INFO 09-02 09:34:16 worker.py:209] extend gpu in worker takes: 0.42s
INFO 09-02 09:34:16 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.42s, there are 2.922GB left on GPU 0
do_liquid latency: 2.70s
INFO 09-02 09:34:16 llm_engine.py:757] Finished liquid for 2 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.69s, update worker latency: 0.00s, liquid model weights latency: 0.75s, init mem latency: 0.00s, liquid kvc latency: 0.94s;, current free mem on GPU0: 2.92GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:34:18 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:34:18 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.68s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.30GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:18 worker.py:151] send weights shards takes: 0.72s, sent out: 6.19GB, sent bw: 8.66GB/s
free mem after recving shards_weights: 10.601868GB
free mem after appending shards_weights: 9.467102GB
free mem after deleting shards_weights: 9.467102GB
INFO 09-02 09:34:18 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:19 cache_engine.py:163] send kvc shards takes: 0.79s, sent out: 7.38GB, sent bw: 9.35GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:19 cache_engine.py:196] After deleting layer's shard, free mem: 23860.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:19 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:34:20 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 2.09 GB
do_liquid latency: 3.33s
INFO 09-02 09:34:20 llm_engine.py:757] Finished liquid for 3 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.01s, update worker latency: 0.00s, liquid model weights latency: 0.79s, init mem latency: 0.00s, liquid kvc latency: 1.22s;, current free mem on GPU0: 2.09GB
INFO 09-02 09:34:20 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:20 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:34:20 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:20 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:34:20 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-02 09:34:20 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:34:20 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:34:20 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.68s to send model shards
After sending shards, there are 8.11GB remaining on GPU0
INFO 09-02 09:34:21 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.36GB/s
INFO 09-02 09:34:21 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:34:22 cache_engine.py:163] send kvc shards takes: 0.74s, sent out: 7.38GB, sent bw: 9.91GB/s
INFO 09-02 09:34:22 cache_engine.py:196] After deleting layer's shard, free mem: 8302.31MB
INFO 09-02 09:34:22 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:34:22 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 15.48 GB
INFO 09-02 09:34:22 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #5118
available space before extending GPU blocks: 15.483GB
available space after extending GPU blocks: 2.920GB, free_mem decreased: 12.562GB
INFO 09-02 09:34:22 worker.py:209] extend gpu in worker takes: 0.44s
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.541321GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 24.097961GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 24.097961GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.660GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.098GB, free_mem decreased: 12.562GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:22 worker.py:209] extend gpu in worker takes: 0.44s
INFO 09-02 09:34:22 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.44s, there are 2.920GB left on GPU 0
do_liquid latency: 2.67s
INFO 09-02 09:34:22 llm_engine.py:757] Finished liquid for 4 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.64s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.90s;, current free mem on GPU0: 2.92GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:34:24 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:34:24 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.67s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.30GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:25 worker.py:151] send weights shards takes: 0.70s, sent out: 6.19GB, sent bw: 8.79GB/s
free mem after recving shards_weights: 10.601868GB
free mem after appending shards_weights: 9.435852GB
free mem after deleting shards_weights: 9.435852GB
INFO 09-02 09:34:25 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:26 cache_engine.py:163] send kvc shards takes: 0.82s, sent out: 7.38GB, sent bw: 9.02GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:26 cache_engine.py:196] After deleting layer's shard, free mem: 23856.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:26 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:34:26 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 2.06 GB
do_liquid latency: 3.25s
INFO 09-02 09:34:26 llm_engine.py:757] Finished liquid for 5 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.03s, update worker latency: 0.00s, liquid model weights latency: 0.78s, init mem latency: 0.00s, liquid kvc latency: 1.25s;, current free mem on GPU0: 2.06GB
INFO 09-02 09:34:27 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:27 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:34:27 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:27 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:34:27 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-02 09:34:27 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:34:27 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:34:27 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.67s to send model shards
After sending shards, there are 8.26GB remaining on GPU0
INFO 09-02 09:34:27 worker.py:151] send weights shards takes: 0.73s, sent out: 6.19GB, sent bw: 8.46GB/s
INFO 09-02 09:34:27 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:34:28 cache_engine.py:163] send kvc shards takes: 0.74s, sent out: 7.38GB, sent bw: 10.02GB/s
INFO 09-02 09:34:28 cache_engine.py:196] After deleting layer's shard, free mem: 8458.31MB
INFO 09-02 09:34:28 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:34:28 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 15.64 GB
INFO 09-02 09:34:28 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #5157
available space before extending GPU blocks: 15.635GB
available space after extending GPU blocks: 2.885GB, free_mem decreased: 12.750GB
INFO 09-02 09:34:29 worker.py:209] extend gpu in worker takes: 0.45s
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.537415GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 24.109680GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 24.109680GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.672GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 3.922GB, free_mem decreased: 12.750GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:29 worker.py:209] extend gpu in worker takes: 0.45s
INFO 09-02 09:34:29 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.45s, there are 2.885GB left on GPU 0
do_liquid latency: 2.65s
INFO 09-02 09:34:29 llm_engine.py:757] Finished liquid for 6 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.62s, update worker latency: 0.00s, liquid model weights latency: 0.73s, init mem latency: 0.00s, liquid kvc latency: 0.88s;, current free mem on GPU0: 2.89GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:34:30 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:34:30 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.64s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.29GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:31 worker.py:151] send weights shards takes: 0.68s, sent out: 6.19GB, sent bw: 9.11GB/s
free mem after recving shards_weights: 10.597961GB
free mem after appending shards_weights: 9.806946GB
free mem after deleting shards_weights: 9.806946GB
INFO 09-02 09:34:31 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:32 cache_engine.py:163] send kvc shards takes: 0.79s, sent out: 7.38GB, sent bw: 9.32GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:32 cache_engine.py:196] After deleting layer's shard, free mem: 23852.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:32 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:34:32 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 2.43 GB
do_liquid latency: 3.23s
INFO 09-02 09:34:32 llm_engine.py:757] Finished liquid for 7 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.99s, update worker latency: 0.00s, liquid model weights latency: 0.75s, init mem latency: 0.00s, liquid kvc latency: 1.24s;, current free mem on GPU0: 2.43GB
INFO 09-02 09:34:33 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:33 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:34:33 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:33 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:34:33 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-02 09:34:33 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:34:33 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:34:33 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.68s to send model shards
After sending shards, there are 8.38GB remaining on GPU0
INFO 09-02 09:34:33 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.34GB/s
INFO 09-02 09:34:33 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:34:34 cache_engine.py:163] send kvc shards takes: 0.74s, sent out: 7.38GB, sent bw: 10.00GB/s
INFO 09-02 09:34:34 cache_engine.py:196] After deleting layer's shard, free mem: 8582.31MB
INFO 09-02 09:34:34 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:34:34 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 15.76 GB
INFO 09-02 09:34:34 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #5188
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.533508GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 24.105774GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 24.105774GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.668GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 3.793GB, free_mem decreased: 12.875GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:35 worker.py:209] extend gpu in worker takes: 0.45s
available space before extending GPU blocks: 15.756GB
available space after extending GPU blocks: 2.881GB, free_mem decreased: 12.875GB
INFO 09-02 09:34:35 worker.py:209] extend gpu in worker takes: 0.46s
INFO 09-02 09:34:35 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.46s, there are 2.881GB left on GPU 0
do_liquid latency: 2.70s
INFO 09-02 09:34:35 llm_engine.py:757] Finished liquid for 8 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.62s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.88s;, current free mem on GPU0: 2.88GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:34:36 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:34:36 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.67s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.29GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:37 worker.py:151] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.78GB/s
free mem after recving shards_weights: 10.594055GB
free mem after appending shards_weights: 9.271790GB
free mem after deleting shards_weights: 9.271790GB
INFO 09-02 09:34:37 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:38 cache_engine.py:163] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.17GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:38 cache_engine.py:196] After deleting layer's shard, free mem: 23848.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:38 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:34:38 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.90 GB
do_liquid latency: 3.25s
INFO 09-02 09:34:38 llm_engine.py:757] Finished liquid for 9 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.03s, update worker latency: 0.00s, liquid model weights latency: 0.78s, init mem latency: 0.00s, liquid kvc latency: 1.25s;, current free mem on GPU0: 1.90GB
INFO 09-02 09:34:39 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:39 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:34:39 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:39 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:34:39 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-02 09:34:39 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:34:39 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:34:39 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.68s to send model shards
After sending shards, there are 8.22GB remaining on GPU0
INFO 09-02 09:34:40 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.36GB/s
INFO 09-02 09:34:40 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:34:40 cache_engine.py:163] send kvc shards takes: 0.74s, sent out: 7.38GB, sent bw: 9.91GB/s
INFO 09-02 09:34:40 cache_engine.py:196] After deleting layer's shard, free mem: 8418.31MB
INFO 09-02 09:34:40 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:34:40 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 15.60 GB
INFO 09-02 09:34:40 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #5147
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.529602GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 24.101868GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 24.101868GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.664GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 3.977GB, free_mem decreased: 12.688GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:41 worker.py:209] extend gpu in worker takes: 0.48s
available space before extending GPU blocks: 15.596GB
available space after extending GPU blocks: 2.909GB, free_mem decreased: 12.688GB
INFO 09-02 09:34:41 worker.py:209] extend gpu in worker takes: 0.48s
INFO 09-02 09:34:41 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.48s, there are 2.909GB left on GPU 0
do_liquid latency: 2.68s
INFO 09-02 09:34:41 llm_engine.py:757] Finished liquid for 10 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.61s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.87s;, current free mem on GPU0: 2.91GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:34:42 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:34:42 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.66s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.29GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:43 worker.py:151] send weights shards takes: 0.70s, sent out: 6.19GB, sent bw: 8.83GB/s
free mem after recving shards_weights: 10.590149GB
free mem after appending shards_weights: 9.299133GB
free mem after deleting shards_weights: 9.299133GB
INFO 09-02 09:34:43 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:44 cache_engine.py:163] send kvc shards takes: 0.79s, sent out: 7.38GB, sent bw: 9.32GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:44 cache_engine.py:196] After deleting layer's shard, free mem: 23844.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:44 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:34:44 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.92 GB
do_liquid latency: 3.21s
INFO 09-02 09:34:44 llm_engine.py:757] Finished liquid for 11 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.99s, update worker latency: 0.00s, liquid model weights latency: 0.77s, init mem latency: 0.00s, liquid kvc latency: 1.21s;, current free mem on GPU0: 1.92GB
INFO 09-02 09:34:45 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:45 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:34:45 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:45 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-02 09:34:45 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:34:45 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:34:45 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:34:45 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.68s to send model shards
After sending shards, there are 8.12GB remaining on GPU0
INFO 09-02 09:34:46 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.39GB/s
INFO 09-02 09:34:46 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:34:46 cache_engine.py:163] send kvc shards takes: 0.75s, sent out: 7.38GB, sent bw: 9.86GB/s
INFO 09-02 09:34:46 cache_engine.py:196] After deleting layer's shard, free mem: 8318.31MB
INFO 09-02 09:34:47 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:34:47 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 15.50 GB
INFO 09-02 09:34:47 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #5122
available space before extending GPU blocks: 15.498GB
available space after extending GPU blocks: 2.873GB, free_mem decreased: 12.625GB
INFO 09-02 09:34:47 worker.py:209] extend gpu in worker takes: 0.43s
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.525696GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 24.097961GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 24.097961GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.660GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.034GB, free_mem decreased: 12.627GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:47 worker.py:209] extend gpu in worker takes: 0.43s
INFO 09-02 09:34:47 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.43s, there are 2.873GB left on GPU 0
do_liquid latency: 2.66s
INFO 09-02 09:34:47 llm_engine.py:757] Finished liquid for 12 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.65s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.91s;, current free mem on GPU0: 2.87GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:34:48 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:34:48 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.68s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.28GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:49 worker.py:151] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.70GB/s
free mem after recving shards_weights: 10.586243GB
free mem after appending shards_weights: 9.201477GB
free mem after deleting shards_weights: 9.201477GB
INFO 09-02 09:34:49 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:50 cache_engine.py:163] send kvc shards takes: 0.79s, sent out: 7.38GB, sent bw: 9.29GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:50 cache_engine.py:196] After deleting layer's shard, free mem: 23838.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:50 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:34:51 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.83 GB
do_liquid latency: 3.33s
INFO 09-02 09:34:51 llm_engine.py:757] Finished liquid for 13 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.02s, update worker latency: 0.00s, liquid model weights latency: 0.78s, init mem latency: 0.00s, liquid kvc latency: 1.25s;, current free mem on GPU0: 1.83GB
INFO 09-02 09:34:51 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:51 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:34:51 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:51 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-02 09:34:51 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:34:51 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:34:51 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:34:51 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.69s to send model shards
After sending shards, there are 8.02GB remaining on GPU0
INFO 09-02 09:34:52 worker.py:151] send weights shards takes: 0.75s, sent out: 6.19GB, sent bw: 8.27GB/s
INFO 09-02 09:34:52 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:34:53 cache_engine.py:163] send kvc shards takes: 0.74s, sent out: 7.38GB, sent bw: 9.91GB/s
INFO 09-02 09:34:53 cache_engine.py:196] After deleting layer's shard, free mem: 8216.31MB
INFO 09-02 09:34:53 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:34:53 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 15.40 GB
INFO 09-02 09:34:53 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #5097
available space before extending GPU blocks: 15.399GB
available space after extending GPU blocks: 2.899GB, free_mem decreased: 12.500GB
INFO 09-02 09:34:53 worker.py:209] extend gpu in worker takes: 0.46s
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.521790GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 24.094055GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 24.094055GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.655GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.155GB, free_mem decreased: 12.500GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:53 worker.py:209] extend gpu in worker takes: 0.47s
INFO 09-02 09:34:53 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.47s, there are 2.899GB left on GPU 0
do_liquid latency: 2.69s
INFO 09-02 09:34:53 llm_engine.py:757] Finished liquid for 14 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.64s, update worker latency: 0.00s, liquid model weights latency: 0.75s, init mem latency: 0.00s, liquid kvc latency: 0.88s;, current free mem on GPU0: 2.90GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:34:55 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:34:55 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.65s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.28GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:55 worker.py:151] send weights shards takes: 0.69s, sent out: 6.19GB, sent bw: 9.03GB/s
free mem after recving shards_weights: 10.580383GB
free mem after appending shards_weights: 9.101868GB
free mem after deleting shards_weights: 9.101868GB
INFO 09-02 09:34:56 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:56 cache_engine.py:163] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.28GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:56 cache_engine.py:196] After deleting layer's shard, free mem: 23834.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:56 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:34:57 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.73 GB
do_liquid latency: 3.37s
INFO 09-02 09:34:57 llm_engine.py:757] Finished liquid for 15 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.01s, update worker latency: 0.00s, liquid model weights latency: 0.75s, init mem latency: 0.00s, liquid kvc latency: 1.26s;, current free mem on GPU0: 1.73GB
INFO 09-02 09:34:57 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:34:57 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:57 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:34:57 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:34:57 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-02 09:34:57 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:34:57 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:34:57 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.68s to send model shards
After sending shards, there are 8.05GB remaining on GPU0
INFO 09-02 09:34:58 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.35GB/s
INFO 09-02 09:34:58 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:34:59 cache_engine.py:163] send kvc shards takes: 0.74s, sent out: 7.38GB, sent bw: 10.04GB/s
INFO 09-02 09:34:59 cache_engine.py:196] After deleting layer's shard, free mem: 8244.31MB
INFO 09-02 09:34:59 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:34:59 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 15.43 GB
INFO 09-02 09:34:59 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #5104
available space before extending GPU blocks: 15.426GB
available space after extending GPU blocks: 2.926GB, free_mem decreased: 12.500GB
INFO 09-02 09:35:00 worker.py:209] extend gpu in worker takes: 0.48s
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.515930GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 24.072571GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 24.072571GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.635GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.135GB, free_mem decreased: 12.500GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:00 worker.py:209] extend gpu in worker takes: 0.48s
INFO 09-02 09:35:00 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.48s, there are 2.926GB left on GPU 0
do_liquid latency: 2.66s
INFO 09-02 09:35:00 llm_engine.py:757] Finished liquid for 16 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.60s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.85s;, current free mem on GPU0: 2.93GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:35:01 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:35:01 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.67s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.27GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:02 worker.py:151] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.75GB/s
free mem after recving shards_weights: 10.574524GB
free mem after appending shards_weights: 9.189758GB
free mem after deleting shards_weights: 9.189758GB
INFO 09-02 09:35:02 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:03 cache_engine.py:163] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.19GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:03 cache_engine.py:196] After deleting layer's shard, free mem: 23828.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:03 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:35:03 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.82 GB
do_liquid latency: 3.37s
INFO 09-02 09:35:03 llm_engine.py:757] Finished liquid for 17 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.05s, update worker latency: 0.00s, liquid model weights latency: 0.78s, init mem latency: 0.00s, liquid kvc latency: 1.27s;, current free mem on GPU0: 1.82GB
INFO 09-02 09:35:04 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:04 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:35:04 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:04 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-02 09:35:04 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:35:04 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:35:04 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:35:04 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.70s to send model shards
After sending shards, there are 8.01GB remaining on GPU0
INFO 09-02 09:35:04 worker.py:151] send weights shards takes: 0.76s, sent out: 6.19GB, sent bw: 8.11GB/s
INFO 09-02 09:35:04 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:35:05 cache_engine.py:163] send kvc shards takes: 0.75s, sent out: 7.38GB, sent bw: 9.90GB/s
INFO 09-02 09:35:05 cache_engine.py:196] After deleting layer's shard, free mem: 8206.31MB
INFO 09-02 09:35:05 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:35:05 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 15.39 GB
INFO 09-02 09:35:05 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #5094
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.510071GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 24.082336GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 24.082336GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.645GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.147GB, free_mem decreased: 12.498GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:06 worker.py:209] extend gpu in worker takes: 0.48s
available space before extending GPU blocks: 15.389GB
available space after extending GPU blocks: 2.891GB, free_mem decreased: 12.498GB
INFO 09-02 09:35:06 worker.py:209] extend gpu in worker takes: 0.48s
INFO 09-02 09:35:06 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.48s, there are 2.891GB left on GPU 0
do_liquid latency: 2.72s
INFO 09-02 09:35:06 llm_engine.py:757] Finished liquid for 18 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.64s, update worker latency: 0.00s, liquid model weights latency: 0.77s, init mem latency: 0.00s, liquid kvc latency: 0.87s;, current free mem on GPU0: 2.89GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:35:07 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:35:07 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.66s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.27GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:08 worker.py:151] send weights shards takes: 0.70s, sent out: 6.19GB, sent bw: 8.90GB/s
free mem after recving shards_weights: 10.570618GB
free mem after appending shards_weights: 9.092102GB
free mem after deleting shards_weights: 9.092102GB
INFO 09-02 09:35:08 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:09 cache_engine.py:163] send kvc shards takes: 0.79s, sent out: 7.38GB, sent bw: 9.29GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:09 cache_engine.py:196] After deleting layer's shard, free mem: 23824.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:09 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:35:09 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.72 GB
do_liquid latency: 3.26s
INFO 09-02 09:35:09 llm_engine.py:757] Finished liquid for 19 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.03s, update worker latency: 0.00s, liquid model weights latency: 0.76s, init mem latency: 0.00s, liquid kvc latency: 1.27s;, current free mem on GPU0: 1.72GB
INFO 09-02 09:35:10 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:10 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:35:10 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:10 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:35:10 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-02 09:35:10 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:35:10 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:35:10 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.67s to send model shards
After sending shards, there are 7.79GB remaining on GPU0
INFO 09-02 09:35:11 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.39GB/s
INFO 09-02 09:35:11 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:35:11 cache_engine.py:163] send kvc shards takes: 0.74s, sent out: 7.38GB, sent bw: 9.91GB/s
INFO 09-02 09:35:11 cache_engine.py:196] After deleting layer's shard, free mem: 7978.31MB
INFO 09-02 09:35:12 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:35:12 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 15.17 GB
INFO 09-02 09:35:12 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #5037
available space before extending GPU blocks: 15.166GB
available space after extending GPU blocks: 2.916GB, free_mem decreased: 12.250GB
INFO 09-02 09:35:12 worker.py:209] extend gpu in worker takes: 0.47s
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.506165GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 24.078430GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 24.078430GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.641GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.391GB, free_mem decreased: 12.250GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:12 worker.py:209] extend gpu in worker takes: 0.48s
INFO 09-02 09:35:12 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.48s, there are 2.916GB left on GPU 0
do_liquid latency: 2.67s
INFO 09-02 09:35:12 llm_engine.py:757] Finished liquid for 20 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.60s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.86s;, current free mem on GPU0: 2.92GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:35:13 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:35:13 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.64s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.26GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:14 worker.py:151] send weights shards takes: 0.67s, sent out: 6.19GB, sent bw: 9.22GB/s
free mem after recving shards_weights: 10.566711GB
free mem after appending shards_weights: 8.963196GB
free mem after deleting shards_weights: 8.963196GB
INFO 09-02 09:35:14 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:15 cache_engine.py:163] send kvc shards takes: 0.79s, sent out: 7.38GB, sent bw: 9.29GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:15 cache_engine.py:196] After deleting layer's shard, free mem: 23820.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:15 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:35:15 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.59 GB
do_liquid latency: 3.35s
INFO 09-02 09:35:15 llm_engine.py:757] Finished liquid for 21 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.99s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 1.25s;, current free mem on GPU0: 1.59GB
INFO 09-02 09:35:16 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:16 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:35:16 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:16 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-02 09:35:16 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:35:16 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:35:16 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:35:16 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.68s to send model shards
After sending shards, there are 7.79GB remaining on GPU0
INFO 09-02 09:35:17 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.38GB/s
INFO 09-02 09:35:17 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:35:18 cache_engine.py:163] send kvc shards takes: 0.74s, sent out: 7.38GB, sent bw: 9.98GB/s
INFO 09-02 09:35:18 cache_engine.py:196] After deleting layer's shard, free mem: 7974.31MB
INFO 09-02 09:35:18 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:35:18 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 15.16 GB
INFO 09-02 09:35:18 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #5036
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.502258GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 24.074524GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 24.074524GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.637GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.387GB, free_mem decreased: 12.250GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:18 worker.py:209] extend gpu in worker takes: 0.47s
available space before extending GPU blocks: 15.162GB
available space after extending GPU blocks: 2.912GB, free_mem decreased: 12.250GB
INFO 09-02 09:35:18 worker.py:209] extend gpu in worker takes: 0.47s
INFO 09-02 09:35:18 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.47s, there are 2.912GB left on GPU 0
do_liquid latency: 2.66s
INFO 09-02 09:35:18 llm_engine.py:757] Finished liquid for 22 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.61s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.87s;, current free mem on GPU0: 2.91GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:35:20 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:35:20 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.68s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.26GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:20 worker.py:151] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.66GB/s
free mem after recving shards_weights: 10.562805GB
free mem after appending shards_weights: 9.115540GB
free mem after deleting shards_weights: 9.115540GB
INFO 09-02 09:35:20 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:21 cache_engine.py:163] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.23GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:21 cache_engine.py:196] After deleting layer's shard, free mem: 23816.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:21 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:35:22 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.74 GB
do_liquid latency: 3.37s
INFO 09-02 09:35:22 llm_engine.py:757] Finished liquid for 23 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.05s, update worker latency: 0.00s, liquid model weights latency: 0.78s, init mem latency: 0.00s, liquid kvc latency: 1.27s;, current free mem on GPU0: 1.74GB
INFO 09-02 09:35:22 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:22 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:35:22 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:22 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-02 09:35:22 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:35:22 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:35:22 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:35:22 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.67s to send model shards
After sending shards, there are 7.91GB remaining on GPU0
INFO 09-02 09:35:23 worker.py:151] send weights shards takes: 0.73s, sent out: 6.19GB, sent bw: 8.43GB/s
INFO 09-02 09:35:23 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:35:24 cache_engine.py:163] send kvc shards takes: 0.74s, sent out: 7.38GB, sent bw: 10.00GB/s
INFO 09-02 09:35:24 cache_engine.py:196] After deleting layer's shard, free mem: 8098.31MB
INFO 09-02 09:35:24 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:35:24 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 15.28 GB
INFO 09-02 09:35:24 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #5067
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.498352GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 24.070618GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 24.070618GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.633GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.258GB, free_mem decreased: 12.375GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:24 worker.py:209] extend gpu in worker takes: 0.46s
available space before extending GPU blocks: 15.284GB
available space after extending GPU blocks: 2.909GB, free_mem decreased: 12.375GB
INFO 09-02 09:35:24 worker.py:209] extend gpu in worker takes: 0.46s
INFO 09-02 09:35:24 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.46s, there are 2.909GB left on GPU 0
do_liquid latency: 2.67s
INFO 09-02 09:35:24 llm_engine.py:757] Finished liquid for 24 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.62s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.88s;, current free mem on GPU0: 2.91GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:35:26 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:35:26 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.64s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.25GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:27 worker.py:151] send weights shards takes: 0.68s, sent out: 6.19GB, sent bw: 9.16GB/s
free mem after recving shards_weights: 10.558899GB
free mem after appending shards_weights: 9.017883GB
free mem after deleting shards_weights: 9.017883GB
INFO 09-02 09:35:27 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:27 cache_engine.py:163] send kvc shards takes: 0.79s, sent out: 7.38GB, sent bw: 9.36GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:27 cache_engine.py:196] After deleting layer's shard, free mem: 23812.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:28 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:35:28 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.64 GB
do_liquid latency: 3.34s
INFO 09-02 09:35:28 llm_engine.py:757] Finished liquid for 25 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.99s, update worker latency: 0.00s, liquid model weights latency: 0.75s, init mem latency: 0.00s, liquid kvc latency: 1.24s;, current free mem on GPU0: 1.64GB
INFO 09-02 09:35:29 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:35:29 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:29 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:29 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:35:29 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-02 09:35:29 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:35:29 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:35:29 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.68s to send model shards
After sending shards, there are 7.84GB remaining on GPU0
INFO 09-02 09:35:29 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.36GB/s
INFO 09-02 09:35:29 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:35:30 cache_engine.py:163] send kvc shards takes: 0.75s, sent out: 7.38GB, sent bw: 9.87GB/s
INFO 09-02 09:35:30 cache_engine.py:196] After deleting layer's shard, free mem: 8030.31MB
INFO 09-02 09:35:30 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:35:30 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 15.22 GB
INFO 09-02 09:35:30 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #5050
available space before extending GPU blocks: 15.217GB
available space after extending GPU blocks: 2.905GB, free_mem decreased: 12.312GB
INFO 09-02 09:35:31 worker.py:209] extend gpu in worker takes: 0.46s
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.494446GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 24.066711GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 24.066711GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.629GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.317GB, free_mem decreased: 12.312GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:31 worker.py:209] extend gpu in worker takes: 0.46s
INFO 09-02 09:35:31 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.46s, there are 2.905GB left on GPU 0
do_liquid latency: 2.68s
INFO 09-02 09:35:31 llm_engine.py:757] Finished liquid for 26 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.63s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.89s;, current free mem on GPU0: 2.90GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:35:32 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:35:32 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.66s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.25GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:33 worker.py:151] send weights shards takes: 0.70s, sent out: 6.19GB, sent bw: 8.82GB/s
free mem after recving shards_weights: 10.554993GB
free mem after appending shards_weights: 8.920227GB
free mem after deleting shards_weights: 8.920227GB
INFO 09-02 09:35:33 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:34 cache_engine.py:163] send kvc shards takes: 0.79s, sent out: 7.38GB, sent bw: 9.33GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:34 cache_engine.py:196] After deleting layer's shard, free mem: 23808.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:34 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:35:34 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.55 GB
do_liquid latency: 3.38s
INFO 09-02 09:35:34 llm_engine.py:757] Finished liquid for 27 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.04s, update worker latency: 0.00s, liquid model weights latency: 0.77s, init mem latency: 0.00s, liquid kvc latency: 1.26s;, current free mem on GPU0: 1.55GB
INFO 09-02 09:35:35 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:35 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:35:35 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:35 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:35:35 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-02 09:35:35 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:35:35 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:35:35 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.67s to send model shards
After sending shards, there are 7.87GB remaining on GPU0
INFO 09-02 09:35:36 worker.py:151] send weights shards takes: 0.73s, sent out: 6.19GB, sent bw: 8.45GB/s
INFO 09-02 09:35:36 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:35:36 cache_engine.py:163] send kvc shards takes: 0.74s, sent out: 7.38GB, sent bw: 9.93GB/s
INFO 09-02 09:35:36 cache_engine.py:196] After deleting layer's shard, free mem: 8056.31MB
INFO 09-02 09:35:36 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:35:36 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 15.24 GB
INFO 09-02 09:35:36 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #5057
available space before extending GPU blocks: 15.242GB
available space after extending GPU blocks: 2.867GB, free_mem decreased: 12.375GB
INFO 09-02 09:35:37 worker.py:209] extend gpu in worker takes: 0.46s
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.490540GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 24.062805GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 24.062805GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.625GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.250GB, free_mem decreased: 12.375GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:37 worker.py:209] extend gpu in worker takes: 0.46s
INFO 09-02 09:35:37 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.46s, there are 2.867GB left on GPU 0
do_liquid latency: 2.67s
INFO 09-02 09:35:37 llm_engine.py:757] Finished liquid for 28 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.62s, update worker latency: 0.00s, liquid model weights latency: 0.73s, init mem latency: 0.00s, liquid kvc latency: 0.88s;, current free mem on GPU0: 2.87GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:35:38 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:35:38 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.65s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.25GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:39 worker.py:151] send weights shards takes: 0.68s, sent out: 6.19GB, sent bw: 9.08GB/s
free mem after recving shards_weights: 10.549133GB
free mem after appending shards_weights: 8.914368GB
free mem after deleting shards_weights: 8.914368GB
INFO 09-02 09:35:39 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:40 cache_engine.py:163] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.27GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:40 cache_engine.py:196] After deleting layer's shard, free mem: 23804.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:40 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:35:40 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.54 GB
do_liquid latency: 3.39s
INFO 09-02 09:35:40 llm_engine.py:757] Finished liquid for 29 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.02s, update worker latency: 0.00s, liquid model weights latency: 0.75s, init mem latency: 0.00s, liquid kvc latency: 1.26s;, current free mem on GPU0: 1.54GB
INFO 09-02 09:35:41 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:41 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:35:41 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:41 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-02 09:35:41 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:35:41 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:35:41 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:35:41 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.68s to send model shards
After sending shards, there are 7.86GB remaining on GPU0
INFO 09-02 09:35:42 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.37GB/s
INFO 09-02 09:35:42 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:35:43 cache_engine.py:163] send kvc shards takes: 0.76s, sent out: 7.38GB, sent bw: 9.73GB/s
INFO 09-02 09:35:43 cache_engine.py:196] After deleting layer's shard, free mem: 8052.31MB
INFO 09-02 09:35:43 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:35:43 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 15.24 GB
INFO 09-02 09:35:43 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #5056
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.486633GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 24.058899GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 24.058899GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.621GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.309GB, free_mem decreased: 12.312GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:43 worker.py:209] extend gpu in worker takes: 0.47s
available space before extending GPU blocks: 15.239GB
available space after extending GPU blocks: 2.926GB, free_mem decreased: 12.312GB
INFO 09-02 09:35:43 worker.py:209] extend gpu in worker takes: 0.47s
INFO 09-02 09:35:43 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.47s, there are 2.926GB left on GPU 0
do_liquid latency: 2.70s
INFO 09-02 09:35:43 llm_engine.py:757] Finished liquid for 30 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.63s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.89s;, current free mem on GPU0: 2.93GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:35:45 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:35:45 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.65s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.24GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:45 worker.py:151] send weights shards takes: 0.69s, sent out: 6.19GB, sent bw: 9.02GB/s
free mem after recving shards_weights: 10.545227GB
free mem after appending shards_weights: 9.160461GB
free mem after deleting shards_weights: 9.160461GB
INFO 09-02 09:35:45 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:46 cache_engine.py:163] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.17GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:46 cache_engine.py:196] After deleting layer's shard, free mem: 23800.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:46 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:35:47 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.79 GB
do_liquid latency: 3.37s
INFO 09-02 09:35:47 llm_engine.py:757] Finished liquid for 31 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.03s, update worker latency: 0.00s, liquid model weights latency: 0.76s, init mem latency: 0.00s, liquid kvc latency: 1.27s;, current free mem on GPU0: 1.79GB
INFO 09-02 09:35:47 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:47 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:35:47 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:47 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-02 09:35:47 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:35:47 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:35:47 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:35:47 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.67s to send model shards
After sending shards, there are 8.11GB remaining on GPU0
INFO 09-02 09:35:48 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.39GB/s
INFO 09-02 09:35:48 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:35:49 cache_engine.py:163] send kvc shards takes: 0.75s, sent out: 7.38GB, sent bw: 9.86GB/s
INFO 09-02 09:35:49 cache_engine.py:196] After deleting layer's shard, free mem: 8304.31MB
INFO 09-02 09:35:49 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:35:49 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 15.48 GB
INFO 09-02 09:35:49 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #5119
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.482727GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 24.054993GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 24.054993GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.617GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.055GB, free_mem decreased: 12.562GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:49 worker.py:209] extend gpu in worker takes: 0.48s
available space before extending GPU blocks: 15.485GB
available space after extending GPU blocks: 2.922GB, free_mem decreased: 12.562GB
INFO 09-02 09:35:49 worker.py:209] extend gpu in worker takes: 0.48s
INFO 09-02 09:35:49 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.48s, there are 2.922GB left on GPU 0
do_liquid latency: 2.68s
INFO 09-02 09:35:49 llm_engine.py:757] Finished liquid for 32 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.61s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.87s;, current free mem on GPU0: 2.92GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:35:51 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:35:51 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.65s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.24GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:52 worker.py:151] send weights shards takes: 0.68s, sent out: 6.19GB, sent bw: 9.08GB/s
free mem after recving shards_weights: 10.539368GB
free mem after appending shards_weights: 9.123352GB
free mem after deleting shards_weights: 9.123352GB
INFO 09-02 09:35:52 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:53 cache_engine.py:163] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.25GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:53 cache_engine.py:196] After deleting layer's shard, free mem: 23794.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:53 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:35:53 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.75 GB
do_liquid latency: 3.35s
INFO 09-02 09:35:53 llm_engine.py:757] Finished liquid for 33 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.99s, update worker latency: 0.00s, liquid model weights latency: 0.75s, init mem latency: 0.00s, liquid kvc latency: 1.24s;, current free mem on GPU0: 1.75GB
INFO 09-02 09:35:54 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:54 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:35:54 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:54 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:35:54 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-02 09:35:54 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:35:54 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:35:54 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.68s to send model shards
After sending shards, there are 8.20GB remaining on GPU0
INFO 09-02 09:35:54 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.35GB/s
INFO 09-02 09:35:54 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:35:55 cache_engine.py:163] send kvc shards takes: 0.74s, sent out: 7.38GB, sent bw: 9.97GB/s
INFO 09-02 09:35:55 cache_engine.py:196] After deleting layer's shard, free mem: 8394.31MB
INFO 09-02 09:35:55 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:35:55 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 15.57 GB
INFO 09-02 09:35:55 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #5141
available space before extending GPU blocks: 15.573GB
available space after extending GPU blocks: 2.887GB, free_mem decreased: 12.686GB
INFO 09-02 09:35:56 worker.py:209] extend gpu in worker takes: 0.48s
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.476868GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 24.049133GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 24.049133GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.612GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 3.926GB, free_mem decreased: 12.686GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:56 worker.py:209] extend gpu in worker takes: 0.48s
INFO 09-02 09:35:56 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.48s, there are 2.887GB left on GPU 0
do_liquid latency: 2.67s
INFO 09-02 09:35:56 llm_engine.py:757] Finished liquid for 34 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.61s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.86s;, current free mem on GPU0: 2.89GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:35:57 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:35:57 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.65s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.23GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:58 worker.py:151] send weights shards takes: 0.69s, sent out: 6.19GB, sent bw: 9.03GB/s
free mem after recving shards_weights: 10.535461GB
free mem after appending shards_weights: 9.213196GB
free mem after deleting shards_weights: 9.213196GB
INFO 09-02 09:35:58 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:59 cache_engine.py:163] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.22GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:59 cache_engine.py:196] After deleting layer's shard, free mem: 23790.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:35:59 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:35:59 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.84 GB
do_liquid latency: 3.36s
INFO 09-02 09:35:59 llm_engine.py:757] Finished liquid for 35 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.02s, update worker latency: 0.00s, liquid model weights latency: 0.76s, init mem latency: 0.00s, liquid kvc latency: 1.26s;, current free mem on GPU0: 1.84GB
INFO 09-02 09:36:00 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:00 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:36:00 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:00 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:36:00 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-02 09:36:00 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:36:00 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:36:00 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.67s to send model shards
After sending shards, there are 8.16GB remaining on GPU0
INFO 09-02 09:36:01 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.40GB/s
INFO 09-02 09:36:01 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:36:01 cache_engine.py:163] send kvc shards takes: 0.74s, sent out: 7.38GB, sent bw: 9.91GB/s
INFO 09-02 09:36:01 cache_engine.py:196] After deleting layer's shard, free mem: 8358.31MB
INFO 09-02 09:36:01 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:36:01 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 15.54 GB
INFO 09-02 09:36:01 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #5132
available space before extending GPU blocks: 15.537GB
available space after extending GPU blocks: 2.912GB, free_mem decreased: 12.625GB
INFO 09-02 09:36:02 worker.py:209] extend gpu in worker takes: 0.46s
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.472961GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 24.045227GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 24.045227GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.608GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 3.983GB, free_mem decreased: 12.625GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:02 worker.py:209] extend gpu in worker takes: 0.46s
INFO 09-02 09:36:02 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.46s, there are 2.912GB left on GPU 0
do_liquid latency: 2.68s
INFO 09-02 09:36:02 llm_engine.py:757] Finished liquid for 36 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.62s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.88s;, current free mem on GPU0: 2.91GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:36:03 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:36:03 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.67s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.23GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:04 worker.py:151] send weights shards takes: 0.70s, sent out: 6.19GB, sent bw: 8.80GB/s
free mem after recving shards_weights: 10.531555GB
free mem after appending shards_weights: 9.178040GB
free mem after deleting shards_weights: 9.178040GB
INFO 09-02 09:36:04 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:05 cache_engine.py:163] send kvc shards takes: 0.81s, sent out: 7.38GB, sent bw: 9.15GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:05 cache_engine.py:196] After deleting layer's shard, free mem: 23786.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:05 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:36:05 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.80 GB
do_liquid latency: 3.28s
INFO 09-02 09:36:05 llm_engine.py:757] Finished liquid for 37 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.06s, update worker latency: 0.00s, liquid model weights latency: 0.78s, init mem latency: 0.00s, liquid kvc latency: 1.28s;, current free mem on GPU0: 1.80GB
INFO 09-02 09:36:06 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:06 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:36:06 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:06 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-02 09:36:06 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:36:06 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:36:06 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:36:06 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.68s to send model shards
After sending shards, there are 8.00GB remaining on GPU0
INFO 09-02 09:36:07 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.36GB/s
INFO 09-02 09:36:07 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:36:08 cache_engine.py:163] send kvc shards takes: 0.74s, sent out: 7.38GB, sent bw: 10.00GB/s
INFO 09-02 09:36:08 cache_engine.py:196] After deleting layer's shard, free mem: 8194.31MB
INFO 09-02 09:36:08 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:36:08 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 15.38 GB
INFO 09-02 09:36:08 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #5091
available space before extending GPU blocks: 15.377GB
available space after extending GPU blocks: 2.877GB, free_mem decreased: 12.500GB
INFO 09-02 09:36:08 worker.py:209] extend gpu in worker takes: 0.45s
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.469055GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 24.072571GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 24.072571GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.635GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.135GB, free_mem decreased: 12.500GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:08 worker.py:209] extend gpu in worker takes: 0.45s
INFO 09-02 09:36:08 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.45s, there are 2.877GB left on GPU 0
do_liquid latency: 2.67s
INFO 09-02 09:36:08 llm_engine.py:757] Finished liquid for 38 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.64s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.89s;, current free mem on GPU0: 2.88GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:36:10 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:36:10 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.64s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.22GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:10 worker.py:151] send weights shards takes: 0.68s, sent out: 6.19GB, sent bw: 9.16GB/s
free mem after recving shards_weights: 10.527649GB
free mem after appending shards_weights: 9.142883GB
free mem after deleting shards_weights: 9.142883GB
INFO 09-02 09:36:10 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:11 cache_engine.py:163] send kvc shards takes: 0.79s, sent out: 7.38GB, sent bw: 9.29GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:11 cache_engine.py:196] After deleting layer's shard, free mem: 23782.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:11 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:36:12 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.77 GB
do_liquid latency: 3.37s
INFO 09-02 09:36:12 llm_engine.py:757] Finished liquid for 39 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.01s, update worker latency: 0.00s, liquid model weights latency: 0.75s, init mem latency: 0.00s, liquid kvc latency: 1.26s;, current free mem on GPU0: 1.77GB
INFO 09-02 09:36:12 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:12 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:36:12 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:12 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-02 09:36:12 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:36:12 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:36:12 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:36:12 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.68s to send model shards
After sending shards, there are 7.97GB remaining on GPU0
INFO 09-02 09:36:13 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.35GB/s
INFO 09-02 09:36:13 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:36:14 cache_engine.py:163] send kvc shards takes: 0.75s, sent out: 7.38GB, sent bw: 9.88GB/s
INFO 09-02 09:36:14 cache_engine.py:196] After deleting layer's shard, free mem: 8158.31MB
INFO 09-02 09:36:14 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:36:14 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 15.34 GB
INFO 09-02 09:36:14 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #5082
available space before extending GPU blocks: 15.342GB
available space after extending GPU blocks: 2.905GB, free_mem decreased: 12.438GB
INFO 09-02 09:36:15 worker.py:209] extend gpu in worker takes: 0.48s
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.465149GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 24.037415GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 24.037415GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.600GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.162GB, free_mem decreased: 12.438GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:15 worker.py:209] extend gpu in worker takes: 0.48s
INFO 09-02 09:36:15 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.48s, there are 2.905GB left on GPU 0
do_liquid latency: 2.68s
INFO 09-02 09:36:15 llm_engine.py:757] Finished liquid for 40 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.61s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.87s;, current free mem on GPU0: 2.90GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:36:16 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:36:16 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.66s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.22GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:17 worker.py:151] send weights shards takes: 0.70s, sent out: 6.19GB, sent bw: 8.83GB/s
free mem after recving shards_weights: 10.523743GB
free mem after appending shards_weights: 8.982727GB
free mem after deleting shards_weights: 8.982727GB
INFO 09-02 09:36:17 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:18 cache_engine.py:163] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.24GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:18 cache_engine.py:196] After deleting layer's shard, free mem: 23778.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:18 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:36:18 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.61 GB
do_liquid latency: 3.30s
INFO 09-02 09:36:18 llm_engine.py:757] Finished liquid for 41 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.05s, update worker latency: 0.00s, liquid model weights latency: 0.77s, init mem latency: 0.00s, liquid kvc latency: 1.28s;, current free mem on GPU0: 1.61GB
INFO 09-02 09:36:19 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:19 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:36:19 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:19 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-02 09:36:19 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:36:19 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:36:19 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:36:19 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.67s to send model shards
After sending shards, there are 7.93GB remaining on GPU0
INFO 09-02 09:36:19 worker.py:151] send weights shards takes: 0.73s, sent out: 6.19GB, sent bw: 8.43GB/s
INFO 09-02 09:36:19 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:36:20 cache_engine.py:163] send kvc shards takes: 0.74s, sent out: 7.38GB, sent bw: 9.99GB/s
INFO 09-02 09:36:20 cache_engine.py:196] After deleting layer's shard, free mem: 8120.31MB
INFO 09-02 09:36:20 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:36:20 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 15.30 GB
INFO 09-02 09:36:20 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #5073
available space before extending GPU blocks: 15.305GB
available space after extending GPU blocks: 2.867GB, free_mem decreased: 12.438GB
INFO 09-02 09:36:21 worker.py:209] extend gpu in worker takes: 0.47s
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.461243GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 24.033508GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 24.033508GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.596GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.157GB, free_mem decreased: 12.439GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:21 worker.py:209] extend gpu in worker takes: 0.47s
INFO 09-02 09:36:21 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.47s, there are 2.867GB left on GPU 0
do_liquid latency: 2.67s
INFO 09-02 09:36:21 llm_engine.py:757] Finished liquid for 42 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.61s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.87s;, current free mem on GPU0: 2.87GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:36:22 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:36:22 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.65s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.22GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:23 worker.py:151] send weights shards takes: 0.69s, sent out: 6.19GB, sent bw: 8.99GB/s
free mem after recving shards_weights: 10.517883GB
free mem after appending shards_weights: 9.008118GB
free mem after deleting shards_weights: 9.008118GB
INFO 09-02 09:36:23 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:24 cache_engine.py:163] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.17GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:24 cache_engine.py:196] After deleting layer's shard, free mem: 23772.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:24 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:36:24 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.63 GB
do_liquid latency: 3.40s
INFO 09-02 09:36:24 llm_engine.py:757] Finished liquid for 43 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.04s, update worker latency: 0.00s, liquid model weights latency: 0.76s, init mem latency: 0.00s, liquid kvc latency: 1.28s;, current free mem on GPU0: 1.63GB
INFO 09-02 09:36:25 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:25 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:36:25 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:25 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-02 09:36:25 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:36:25 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:36:25 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:36:25 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.67s to send model shards
After sending shards, there are 7.96GB remaining on GPU0
INFO 09-02 09:36:26 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.42GB/s
INFO 09-02 09:36:26 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:36:26 cache_engine.py:163] send kvc shards takes: 0.74s, sent out: 7.38GB, sent bw: 10.01GB/s
INFO 09-02 09:36:26 cache_engine.py:196] After deleting layer's shard, free mem: 8148.31MB
INFO 09-02 09:36:27 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:36:27 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 15.33 GB
INFO 09-02 09:36:27 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #5080
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.455383GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 24.027649GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 24.027649GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.590GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.153GB, free_mem decreased: 12.438GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:27 worker.py:209] extend gpu in worker takes: 0.44s
available space before extending GPU blocks: 15.332GB
available space after extending GPU blocks: 2.895GB, free_mem decreased: 12.438GB
INFO 09-02 09:36:27 worker.py:209] extend gpu in worker takes: 0.45s
INFO 09-02 09:36:27 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.45s, there are 2.895GB left on GPU 0
do_liquid latency: 2.66s
INFO 09-02 09:36:27 llm_engine.py:757] Finished liquid for 44 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.63s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.89s;, current free mem on GPU0: 2.89GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:36:28 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:36:28 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.63s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.21GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:29 worker.py:151] send weights shards takes: 0.67s, sent out: 6.19GB, sent bw: 9.27GB/s
free mem after recving shards_weights: 10.513977GB
free mem after appending shards_weights: 9.035461GB
free mem after deleting shards_weights: 9.035461GB
INFO 09-02 09:36:29 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:30 cache_engine.py:163] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.25GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:30 cache_engine.py:196] After deleting layer's shard, free mem: 23768.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:30 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:36:31 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.66 GB
do_liquid latency: 3.39s
INFO 09-02 09:36:31 llm_engine.py:757] Finished liquid for 45 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.02s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 1.27s;, current free mem on GPU0: 1.66GB
INFO 09-02 09:36:31 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:36:31 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:31 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:31 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-02 09:36:31 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:36:31 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:36:31 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:36:31 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.68s to send model shards
After sending shards, there are 7.86GB remaining on GPU0
INFO 09-02 09:36:32 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.33GB/s
INFO 09-02 09:36:32 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:36:33 cache_engine.py:163] send kvc shards takes: 0.74s, sent out: 7.38GB, sent bw: 9.91GB/s
INFO 09-02 09:36:33 cache_engine.py:196] After deleting layer's shard, free mem: 8048.31MB
INFO 09-02 09:36:33 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:36:33 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 15.23 GB
INFO 09-02 09:36:33 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #5055
available space before extending GPU blocks: 15.235GB
available space after extending GPU blocks: 2.922GB, free_mem decreased: 12.312GB
INFO 09-02 09:36:33 worker.py:209] extend gpu in worker takes: 0.46s
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.451477GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 24.023743GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 24.023743GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.586GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.274GB, free_mem decreased: 12.312GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:33 worker.py:209] extend gpu in worker takes: 0.46s
INFO 09-02 09:36:33 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.46s, there are 2.922GB left on GPU 0
do_liquid latency: 2.68s
INFO 09-02 09:36:33 llm_engine.py:757] Finished liquid for 46 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.62s, update worker latency: 0.00s, liquid model weights latency: 0.75s, init mem latency: 0.00s, liquid kvc latency: 0.87s;, current free mem on GPU0: 2.92GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:36:35 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:36:35 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.67s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.21GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:35 worker.py:151] send weights shards takes: 0.70s, sent out: 6.19GB, sent bw: 8.81GB/s
free mem after recving shards_weights: 10.510071GB
free mem after appending shards_weights: 9.062805GB
free mem after deleting shards_weights: 9.062805GB
INFO 09-02 09:36:35 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:36 cache_engine.py:163] send kvc shards takes: 0.79s, sent out: 7.38GB, sent bw: 9.29GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:36 cache_engine.py:196] After deleting layer's shard, free mem: 23764.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:36 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:36:37 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.69 GB
do_liquid latency: 3.35s
INFO 09-02 09:36:37 llm_engine.py:757] Finished liquid for 47 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.03s, update worker latency: 0.00s, liquid model weights latency: 0.77s, init mem latency: 0.00s, liquid kvc latency: 1.26s;, current free mem on GPU0: 1.69GB
INFO 09-02 09:36:37 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:37 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:37 pynccl.py:65] vLLM is using nccl==2.20.5
INFO 09-02 09:36:37 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:36:37 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-02 09:36:37 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:36:37 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:36:37 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.67s to send model shards
After sending shards, there are 7.89GB remaining on GPU0
INFO 09-02 09:36:38 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.42GB/s
INFO 09-02 09:36:38 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:36:39 cache_engine.py:163] send kvc shards takes: 0.74s, sent out: 7.38GB, sent bw: 9.92GB/s
INFO 09-02 09:36:39 cache_engine.py:196] After deleting layer's shard, free mem: 8076.31MB
INFO 09-02 09:36:39 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:36:39 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 15.26 GB
INFO 09-02 09:36:39 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #5062
available space before extending GPU blocks: 15.262GB
available space after extending GPU blocks: 2.887GB, free_mem decreased: 12.375GB
INFO 09-02 09:36:39 worker.py:209] extend gpu in worker takes: 0.47s
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.447571GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 24.019836GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 24.019836GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.582GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.207GB, free_mem decreased: 12.375GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:39 worker.py:209] extend gpu in worker takes: 0.47s
INFO 09-02 09:36:39 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.47s, there are 2.887GB left on GPU 0
do_liquid latency: 2.67s
INFO 09-02 09:36:39 llm_engine.py:757] Finished liquid for 48 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.60s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.87s;, current free mem on GPU0: 2.89GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:36:41 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:36:41 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.65s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.20GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:42 worker.py:151] send weights shards takes: 0.69s, sent out: 6.19GB, sent bw: 8.98GB/s
free mem after recving shards_weights: 10.502258GB
free mem after appending shards_weights: 8.836243GB
free mem after deleting shards_weights: 8.836243GB
INFO 09-02 09:36:42 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:43 cache_engine.py:163] send kvc shards takes: 0.79s, sent out: 7.38GB, sent bw: 9.30GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:43 cache_engine.py:196] After deleting layer's shard, free mem: 23756.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:43 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:36:43 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.46 GB
do_liquid latency: 3.37s
INFO 09-02 09:36:43 llm_engine.py:757] Finished liquid for 49 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.02s, update worker latency: 0.00s, liquid model weights latency: 0.76s, init mem latency: 0.00s, liquid kvc latency: 1.26s;, current free mem on GPU0: 1.46GB
INFO 09-02 09:36:44 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:44 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:36:44 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:44 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:36:44 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-02 09:36:44 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:36:44 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:36:44 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.67s to send model shards
After sending shards, there are 7.79GB remaining on GPU0
INFO 09-02 09:36:44 worker.py:151] send weights shards takes: 0.73s, sent out: 6.19GB, sent bw: 8.43GB/s
INFO 09-02 09:36:44 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:36:45 cache_engine.py:163] send kvc shards takes: 0.74s, sent out: 7.38GB, sent bw: 9.92GB/s
INFO 09-02 09:36:45 cache_engine.py:196] After deleting layer's shard, free mem: 7972.31MB
INFO 09-02 09:36:45 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:36:45 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 15.16 GB
INFO 09-02 09:36:45 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #5036
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.439758GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 24.012024GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 24.012024GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.575GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.326GB, free_mem decreased: 12.248GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:46 worker.py:209] extend gpu in worker takes: 0.48s
available space before extending GPU blocks: 15.160GB
available space after extending GPU blocks: 2.912GB, free_mem decreased: 12.248GB
INFO 09-02 09:36:46 worker.py:209] extend gpu in worker takes: 0.48s
INFO 09-02 09:36:46 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.48s, there are 2.912GB left on GPU 0
do_liquid latency: 2.67s
INFO 09-02 09:36:46 llm_engine.py:757] Finished liquid for 50 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.60s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.86s;, current free mem on GPU0: 2.91GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:36:47 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:36:47 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.65s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.20GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:48 worker.py:151] send weights shards takes: 0.69s, sent out: 6.19GB, sent bw: 9.01GB/s
free mem after recving shards_weights: 10.498352GB
free mem after appending shards_weights: 8.957336GB
free mem after deleting shards_weights: 8.957336GB
INFO 09-02 09:36:48 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:49 cache_engine.py:163] send kvc shards takes: 0.79s, sent out: 7.38GB, sent bw: 9.31GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:49 cache_engine.py:196] After deleting layer's shard, free mem: 23752.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:49 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:36:49 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.58 GB
do_liquid latency: 3.37s
INFO 09-02 09:36:49 llm_engine.py:757] Finished liquid for 51 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.03s, update worker latency: 0.00s, liquid model weights latency: 0.76s, init mem latency: 0.00s, liquid kvc latency: 1.26s;, current free mem on GPU0: 1.58GB
INFO 09-02 09:36:50 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:50 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:36:50 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:50 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-02 09:36:50 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:36:50 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:36:50 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:36:50 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.67s to send model shards
After sending shards, there are 7.78GB remaining on GPU0
INFO 09-02 09:36:51 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.42GB/s
INFO 09-02 09:36:51 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:36:51 cache_engine.py:163] send kvc shards takes: 0.74s, sent out: 7.38GB, sent bw: 9.92GB/s
INFO 09-02 09:36:51 cache_engine.py:196] After deleting layer's shard, free mem: 7968.31MB
INFO 09-02 09:36:52 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:36:52 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 15.16 GB
INFO 09-02 09:36:52 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #5035
available space before extending GPU blocks: 15.157GB
available space after extending GPU blocks: 2.907GB, free_mem decreased: 12.250GB
INFO 09-02 09:36:52 worker.py:209] extend gpu in worker takes: 0.47s
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.435852GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 24.008118GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 24.008118GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.571GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.321GB, free_mem decreased: 12.250GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:52 worker.py:209] extend gpu in worker takes: 0.47s
INFO 09-02 09:36:52 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.47s, there are 2.907GB left on GPU 0
do_liquid latency: 2.66s
INFO 09-02 09:36:52 llm_engine.py:757] Finished liquid for 52 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.60s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.87s;, current free mem on GPU0: 2.91GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:36:53 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:36:53 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.64s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.19GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:54 worker.py:151] send weights shards takes: 0.68s, sent out: 6.19GB, sent bw: 9.15GB/s
free mem after recving shards_weights: 10.494446GB
free mem after appending shards_weights: 8.609680GB
free mem after deleting shards_weights: 8.609680GB
INFO 09-02 09:36:54 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:55 cache_engine.py:163] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.18GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:55 cache_engine.py:196] After deleting layer's shard, free mem: 23748.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:55 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:36:56 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.23 GB
do_liquid latency: 3.42s
INFO 09-02 09:36:56 llm_engine.py:757] Finished liquid for 53 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.10s, update worker latency: 0.00s, liquid model weights latency: 0.75s, init mem latency: 0.00s, liquid kvc latency: 1.35s;, current free mem on GPU0: 1.23GB
INFO 09-02 09:36:56 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:56 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:36:56 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:56 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-02 09:36:56 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:36:56 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:36:56 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:36:56 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.68s to send model shards
After sending shards, there are 7.68GB remaining on GPU0
INFO 09-02 09:36:57 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.36GB/s
INFO 09-02 09:36:57 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:36:58 cache_engine.py:163] send kvc shards takes: 0.75s, sent out: 7.38GB, sent bw: 9.89GB/s
INFO 09-02 09:36:58 cache_engine.py:196] After deleting layer's shard, free mem: 7868.31MB
INFO 09-02 09:36:58 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:36:58 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 15.06 GB
INFO 09-02 09:36:58 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #5010
available space before extending GPU blocks: 15.059GB
available space after extending GPU blocks: 2.871GB, free_mem decreased: 12.188GB
INFO 09-02 09:36:58 worker.py:209] extend gpu in worker takes: 0.46s
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.431946GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 24.004211GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 24.004211GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.567GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.379GB, free_mem decreased: 12.188GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:36:58 worker.py:209] extend gpu in worker takes: 0.46s
INFO 09-02 09:36:58 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.46s, there are 2.871GB left on GPU 0
do_liquid latency: 2.66s
INFO 09-02 09:36:58 llm_engine.py:757] Finished liquid for 54 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.62s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.88s;, current free mem on GPU0: 2.87GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:37:00 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:37:00 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.65s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.19GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:00 worker.py:151] send weights shards takes: 0.69s, sent out: 6.19GB, sent bw: 8.99GB/s
free mem after recving shards_weights: 10.490540GB
free mem after appending shards_weights: 8.980774GB
free mem after deleting shards_weights: 8.980774GB
INFO 09-02 09:37:00 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:01 cache_engine.py:163] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.22GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:01 cache_engine.py:196] After deleting layer's shard, free mem: 23744.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:01 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:37:02 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.61 GB
do_liquid latency: 3.37s
INFO 09-02 09:37:02 llm_engine.py:757] Finished liquid for 55 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.04s, update worker latency: 0.00s, liquid model weights latency: 0.76s, init mem latency: 0.00s, liquid kvc latency: 1.28s;, current free mem on GPU0: 1.61GB
INFO 09-02 09:37:02 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:02 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:37:02 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:02 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:37:02 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-02 09:37:02 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:37:02 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:37:02 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.68s to send model shards
After sending shards, there are 7.68GB remaining on GPU0
INFO 09-02 09:37:03 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.36GB/s
INFO 09-02 09:37:03 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:37:04 cache_engine.py:163] send kvc shards takes: 0.74s, sent out: 7.38GB, sent bw: 9.92GB/s
INFO 09-02 09:37:04 cache_engine.py:196] After deleting layer's shard, free mem: 7862.31MB
INFO 09-02 09:37:04 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:37:04 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 15.05 GB
INFO 09-02 09:37:04 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #5008
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.428040GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 24.000305GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 24.000305GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.563GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.438GB, free_mem decreased: 12.125GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:04 worker.py:209] extend gpu in worker takes: 0.45s
available space before extending GPU blocks: 15.053GB
available space after extending GPU blocks: 2.928GB, free_mem decreased: 12.125GB
INFO 09-02 09:37:04 worker.py:209] extend gpu in worker takes: 0.46s
INFO 09-02 09:37:04 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.46s, there are 2.928GB left on GPU 0
do_liquid latency: 2.65s
INFO 09-02 09:37:04 llm_engine.py:757] Finished liquid for 56 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.61s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.87s;, current free mem on GPU0: 2.93GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:37:06 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:37:06 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.67s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.18GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:07 worker.py:151] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.75GB/s
free mem after recving shards_weights: 10.484680GB
free mem after appending shards_weights: 8.881165GB
free mem after deleting shards_weights: 8.881165GB
INFO 09-02 09:37:07 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:07 cache_engine.py:163] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.21GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:07 cache_engine.py:196] After deleting layer's shard, free mem: 23740.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:08 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:37:08 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.51 GB
do_liquid latency: 3.34s
INFO 09-02 09:37:08 llm_engine.py:757] Finished liquid for 57 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.06s, update worker latency: 0.00s, liquid model weights latency: 0.79s, init mem latency: 0.00s, liquid kvc latency: 1.27s;, current free mem on GPU0: 1.51GB
INFO 09-02 09:37:09 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:09 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:37:09 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:09 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:37:09 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-02 09:37:09 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:37:09 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:37:09 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.68s to send model shards
After sending shards, there are 7.58GB remaining on GPU0
INFO 09-02 09:37:09 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.35GB/s
INFO 09-02 09:37:09 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:37:10 cache_engine.py:163] send kvc shards takes: 0.74s, sent out: 7.38GB, sent bw: 9.93GB/s
INFO 09-02 09:37:10 cache_engine.py:196] After deleting layer's shard, free mem: 7762.31MB
INFO 09-02 09:37:10 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:37:10 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 14.96 GB
INFO 09-02 09:37:10 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #4983
available space before extending GPU blocks: 14.955GB
available space after extending GPU blocks: 2.893GB, free_mem decreased: 12.062GB
INFO 09-02 09:37:11 worker.py:209] extend gpu in worker takes: 0.47s
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.424133GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 23.996399GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 23.996399GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.559GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.496GB, free_mem decreased: 12.062GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:11 worker.py:209] extend gpu in worker takes: 0.47s
INFO 09-02 09:37:11 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.47s, there are 2.893GB left on GPU 0
do_liquid latency: 2.67s
INFO 09-02 09:37:11 llm_engine.py:757] Finished liquid for 58 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.61s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.86s;, current free mem on GPU0: 2.89GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:37:12 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:37:12 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.64s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.18GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:13 worker.py:151] send weights shards takes: 0.68s, sent out: 6.19GB, sent bw: 9.13GB/s
free mem after recving shards_weights: 10.480774GB
free mem after appending shards_weights: 8.752258GB
free mem after deleting shards_weights: 8.752258GB
INFO 09-02 09:37:13 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:14 cache_engine.py:163] send kvc shards takes: 0.79s, sent out: 7.38GB, sent bw: 9.35GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:14 cache_engine.py:196] After deleting layer's shard, free mem: 23736.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:14 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:37:14 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.38 GB
do_liquid latency: 3.42s
INFO 09-02 09:37:14 llm_engine.py:757] Finished liquid for 59 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.07s, update worker latency: 0.00s, liquid model weights latency: 0.75s, init mem latency: 0.00s, liquid kvc latency: 1.33s;, current free mem on GPU0: 1.38GB
INFO 09-02 09:37:15 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:15 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:37:15 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:15 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-02 09:37:15 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:37:15 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:37:15 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:37:15 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.68s to send model shards
After sending shards, there are 7.80GB remaining on GPU0
INFO 09-02 09:37:16 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.36GB/s
INFO 09-02 09:37:16 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:37:16 cache_engine.py:163] send kvc shards takes: 0.76s, sent out: 7.38GB, sent bw: 9.77GB/s
INFO 09-02 09:37:16 cache_engine.py:196] After deleting layer's shard, free mem: 7982.31MB
INFO 09-02 09:37:17 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:37:17 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 15.17 GB
INFO 09-02 09:37:17 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #5038
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.420227GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 23.992493GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 23.992493GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.555GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.305GB, free_mem decreased: 12.250GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:17 worker.py:209] extend gpu in worker takes: 0.47s
available space before extending GPU blocks: 15.170GB
available space after extending GPU blocks: 2.920GB, free_mem decreased: 12.250GB
INFO 09-02 09:37:17 worker.py:209] extend gpu in worker takes: 0.47s
INFO 09-02 09:37:17 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.47s, there are 2.920GB left on GPU 0
do_liquid latency: 2.68s
INFO 09-02 09:37:17 llm_engine.py:757] Finished liquid for 60 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.62s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.88s;, current free mem on GPU0: 2.92GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:37:18 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:37:18 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.65s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.18GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:19 worker.py:151] send weights shards takes: 0.69s, sent out: 6.19GB, sent bw: 9.00GB/s
free mem after recving shards_weights: 10.476868GB
free mem after appending shards_weights: 8.748352GB
free mem after deleting shards_weights: 8.748352GB
INFO 09-02 09:37:19 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:20 cache_engine.py:163] send kvc shards takes: 0.81s, sent out: 7.38GB, sent bw: 9.11GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:20 cache_engine.py:196] After deleting layer's shard, free mem: 23732.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:20 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:37:21 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.37 GB
do_liquid latency: 3.44s
INFO 09-02 09:37:21 llm_engine.py:757] Finished liquid for 61 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.08s, update worker latency: 0.00s, liquid model weights latency: 0.76s, init mem latency: 0.00s, liquid kvc latency: 1.31s;, current free mem on GPU0: 1.37GB
INFO 09-02 09:37:21 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:21 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:37:21 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:21 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-02 09:37:21 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:37:21 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:37:21 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:37:21 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.68s to send model shards
After sending shards, there are 7.70GB remaining on GPU0
INFO 09-02 09:37:22 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.35GB/s
INFO 09-02 09:37:22 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:37:23 cache_engine.py:163] send kvc shards takes: 0.73s, sent out: 7.38GB, sent bw: 10.04GB/s
INFO 09-02 09:37:23 cache_engine.py:196] After deleting layer's shard, free mem: 7882.31MB
INFO 09-02 09:37:23 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:37:23 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 15.07 GB
INFO 09-02 09:37:23 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #5013
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.416321GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 23.988586GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 23.988586GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.551GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.364GB, free_mem decreased: 12.188GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:23 worker.py:209] extend gpu in worker takes: 0.48s
available space before extending GPU blocks: 15.073GB
available space after extending GPU blocks: 2.885GB, free_mem decreased: 12.188GB
INFO 09-02 09:37:23 worker.py:209] extend gpu in worker takes: 0.48s
INFO 09-02 09:37:23 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.48s, there are 2.885GB left on GPU 0
do_liquid latency: 2.67s
INFO 09-02 09:37:23 llm_engine.py:757] Finished liquid for 62 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.60s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.86s;, current free mem on GPU0: 2.89GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:37:25 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:37:25 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.66s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.17GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:25 worker.py:151] send weights shards takes: 0.69s, sent out: 6.19GB, sent bw: 8.92GB/s
free mem after recving shards_weights: 10.472961GB
free mem after appending shards_weights: 9.150696GB
free mem after deleting shards_weights: 9.150696GB
INFO 09-02 09:37:26 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:26 cache_engine.py:163] send kvc shards takes: 0.82s, sent out: 7.38GB, sent bw: 8.98GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:26 cache_engine.py:196] After deleting layer's shard, free mem: 23728.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:26 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:37:27 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.78 GB
do_liquid latency: 3.37s
INFO 09-02 09:37:27 llm_engine.py:757] Finished liquid for 63 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.04s, update worker latency: 0.00s, liquid model weights latency: 0.77s, init mem latency: 0.00s, liquid kvc latency: 1.27s;, current free mem on GPU0: 1.78GB
INFO 09-02 09:37:27 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:27 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:37:27 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:27 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-02 09:37:27 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:37:27 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:37:27 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:37:27 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.68s to send model shards
After sending shards, there are 8.10GB remaining on GPU0
INFO 09-02 09:37:28 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.34GB/s
INFO 09-02 09:37:28 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:37:29 cache_engine.py:163] send kvc shards takes: 0.74s, sent out: 7.38GB, sent bw: 9.97GB/s
INFO 09-02 09:37:29 cache_engine.py:196] After deleting layer's shard, free mem: 8294.31MB
INFO 09-02 09:37:29 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:37:29 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 15.47 GB
INFO 09-02 09:37:29 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #5116
available space before extending GPU blocks: 15.475GB
available space after extending GPU blocks: 2.912GB, free_mem decreased: 12.562GB
INFO 09-02 09:37:30 worker.py:209] extend gpu in worker takes: 0.50s
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.412415GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 23.984680GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 23.984680GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.547GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 3.985GB, free_mem decreased: 12.562GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:30 worker.py:209] extend gpu in worker takes: 0.50s
INFO 09-02 09:37:30 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.50s, there are 2.912GB left on GPU 0
do_liquid latency: 2.66s
INFO 09-02 09:37:30 llm_engine.py:757] Finished liquid for 64 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.58s, update worker latency: 0.00s, liquid model weights latency: 0.75s, init mem latency: 0.00s, liquid kvc latency: 0.83s;, current free mem on GPU0: 2.91GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:37:31 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:37:31 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.67s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.17GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:32 worker.py:151] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.75GB/s
free mem after recving shards_weights: 10.467102GB
free mem after appending shards_weights: 9.238586GB
free mem after deleting shards_weights: 9.238586GB
INFO 09-02 09:37:32 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:33 cache_engine.py:163] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.27GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:33 cache_engine.py:196] After deleting layer's shard, free mem: 23722.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:33 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:37:33 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.87 GB
do_liquid latency: 3.37s
INFO 09-02 09:37:33 llm_engine.py:757] Finished liquid for 65 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.04s, update worker latency: 0.00s, liquid model weights latency: 0.78s, init mem latency: 0.00s, liquid kvc latency: 1.26s;, current free mem on GPU0: 1.87GB
INFO 09-02 09:37:34 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:34 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:37:34 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:34 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-02 09:37:34 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:37:34 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:37:34 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:37:34 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.67s to send model shards
After sending shards, there are 8.06GB remaining on GPU0
INFO 09-02 09:37:34 worker.py:151] send weights shards takes: 0.73s, sent out: 6.19GB, sent bw: 8.44GB/s
INFO 09-02 09:37:34 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:37:35 cache_engine.py:163] send kvc shards takes: 0.74s, sent out: 7.38GB, sent bw: 9.97GB/s
INFO 09-02 09:37:35 cache_engine.py:196] After deleting layer's shard, free mem: 8256.31MB
INFO 09-02 09:37:35 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:37:35 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 15.44 GB
INFO 09-02 09:37:35 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #5107
available space before extending GPU blocks: 15.438GB
available space after extending GPU blocks: 2.877GB, free_mem decreased: 12.561GB
INFO 09-02 09:37:36 worker.py:209] extend gpu in worker takes: 0.47s
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.406555GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 23.994446GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 23.994446GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.557GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 3.996GB, free_mem decreased: 12.561GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:36 worker.py:209] extend gpu in worker takes: 0.48s
INFO 09-02 09:37:36 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.48s, there are 2.877GB left on GPU 0
do_liquid latency: 2.66s
INFO 09-02 09:37:36 llm_engine.py:757] Finished liquid for 66 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.60s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.86s;, current free mem on GPU0: 2.88GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:37:37 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:37:37 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.67s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.16GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:38 worker.py:151] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.69GB/s
free mem after recving shards_weights: 10.463196GB
free mem after appending shards_weights: 9.047180GB
free mem after deleting shards_weights: 9.047180GB
INFO 09-02 09:37:38 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:39 cache_engine.py:163] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.23GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:39 cache_engine.py:196] After deleting layer's shard, free mem: 23718.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:39 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:37:39 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.67 GB
do_liquid latency: 3.39s
INFO 09-02 09:37:39 llm_engine.py:757] Finished liquid for 67 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.05s, update worker latency: 0.00s, liquid model weights latency: 0.78s, init mem latency: 0.00s, liquid kvc latency: 1.27s;, current free mem on GPU0: 1.67GB
INFO 09-02 09:37:40 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:40 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:37:40 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:40 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:37:40 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-02 09:37:40 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:37:40 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:37:40 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.69s to send model shards
After sending shards, there are 8.00GB remaining on GPU0
INFO 09-02 09:37:41 worker.py:151] send weights shards takes: 0.76s, sent out: 6.19GB, sent bw: 8.20GB/s
INFO 09-02 09:37:41 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:37:42 cache_engine.py:163] send kvc shards takes: 0.75s, sent out: 7.38GB, sent bw: 9.81GB/s
INFO 09-02 09:37:42 cache_engine.py:196] After deleting layer's shard, free mem: 8188.31MB
INFO 09-02 09:37:42 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:37:42 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 15.37 GB
INFO 09-02 09:37:42 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #5090
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.402649GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 23.974915GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 23.974915GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.537GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.037GB, free_mem decreased: 12.500GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:42 worker.py:209] extend gpu in worker takes: 0.47s
available space before extending GPU blocks: 15.371GB
available space after extending GPU blocks: 2.871GB, free_mem decreased: 12.500GB
INFO 09-02 09:37:42 worker.py:209] extend gpu in worker takes: 0.47s
INFO 09-02 09:37:42 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.47s, there are 2.871GB left on GPU 0
do_liquid latency: 2.70s
INFO 09-02 09:37:42 llm_engine.py:757] Finished liquid for 68 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.64s, update worker latency: 0.00s, liquid model weights latency: 0.76s, init mem latency: 0.00s, liquid kvc latency: 0.88s;, current free mem on GPU0: 2.87GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:37:43 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:37:43 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.67s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.16GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:44 worker.py:151] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.72GB/s
free mem after recving shards_weights: 10.459290GB
free mem after appending shards_weights: 8.949524GB
free mem after deleting shards_weights: 8.949524GB
INFO 09-02 09:37:44 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:45 cache_engine.py:163] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.20GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:45 cache_engine.py:196] After deleting layer's shard, free mem: 23714.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:45 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:37:46 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.57 GB
do_liquid latency: 3.30s
INFO 09-02 09:37:46 llm_engine.py:757] Finished liquid for 69 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.07s, update worker latency: 0.00s, liquid model weights latency: 0.78s, init mem latency: 0.00s, liquid kvc latency: 1.29s;, current free mem on GPU0: 1.57GB
INFO 09-02 09:37:46 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:46 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:37:46 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:46 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:37:46 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-02 09:37:46 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:37:46 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:37:46 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.67s to send model shards
After sending shards, there are 7.77GB remaining on GPU0
INFO 09-02 09:37:47 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.39GB/s
INFO 09-02 09:37:47 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:37:48 cache_engine.py:163] send kvc shards takes: 0.74s, sent out: 7.38GB, sent bw: 10.00GB/s
INFO 09-02 09:37:48 cache_engine.py:196] After deleting layer's shard, free mem: 7958.31MB
INFO 09-02 09:37:48 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:37:48 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 15.15 GB
INFO 09-02 09:37:48 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #5032
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.398743GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 24.002258GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 24.002258GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.565GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.313GB, free_mem decreased: 12.252GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:48 worker.py:209] extend gpu in worker takes: 0.46s
available space before extending GPU blocks: 15.147GB
available space after extending GPU blocks: 2.897GB, free_mem decreased: 12.250GB
INFO 09-02 09:37:48 worker.py:209] extend gpu in worker takes: 0.46s
INFO 09-02 09:37:48 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.47s, there are 2.897GB left on GPU 0
do_liquid latency: 2.65s
INFO 09-02 09:37:48 llm_engine.py:757] Finished liquid for 70 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.60s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.86s;, current free mem on GPU0: 2.90GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:37:50 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:37:50 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.64s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.15GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:50 worker.py:151] send weights shards takes: 0.68s, sent out: 6.19GB, sent bw: 9.14GB/s
free mem after recving shards_weights: 10.453430GB
free mem after appending shards_weights: 8.818665GB
free mem after deleting shards_weights: 8.818665GB
INFO 09-02 09:37:50 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:51 cache_engine.py:163] send kvc shards takes: 0.79s, sent out: 7.38GB, sent bw: 9.31GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:51 cache_engine.py:196] After deleting layer's shard, free mem: 23708.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:51 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:37:52 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.44 GB
do_liquid latency: 3.34s
INFO 09-02 09:37:52 llm_engine.py:757] Finished liquid for 71 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.02s, update worker latency: 0.00s, liquid model weights latency: 0.75s, init mem latency: 0.00s, liquid kvc latency: 1.28s;, current free mem on GPU0: 1.44GB
INFO 09-02 09:37:52 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:52 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:37:52 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:52 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-02 09:37:52 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:37:52 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:37:52 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:37:52 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.67s to send model shards
After sending shards, there are 7.77GB remaining on GPU0
INFO 09-02 09:37:53 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.41GB/s
INFO 09-02 09:37:53 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:37:54 cache_engine.py:163] send kvc shards takes: 0.74s, sent out: 7.38GB, sent bw: 10.00GB/s
INFO 09-02 09:37:54 cache_engine.py:196] After deleting layer's shard, free mem: 7954.31MB
INFO 09-02 09:37:54 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:37:54 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 15.14 GB
INFO 09-02 09:37:54 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #5031
available space before extending GPU blocks: 15.143GB
available space after extending GPU blocks: 2.893GB, free_mem decreased: 12.250GB
INFO 09-02 09:37:54 worker.py:209] extend gpu in worker takes: 0.44s
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.392883GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 23.965149GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 23.965149GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.528GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.278GB, free_mem decreased: 12.250GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:54 worker.py:209] extend gpu in worker takes: 0.44s
INFO 09-02 09:37:54 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.44s, there are 2.893GB left on GPU 0
do_liquid latency: 2.66s
INFO 09-02 09:37:54 llm_engine.py:757] Finished liquid for 72 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.63s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.89s;, current free mem on GPU0: 2.89GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:37:56 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:37:56 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.65s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.15GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:57 worker.py:151] send weights shards takes: 0.69s, sent out: 6.19GB, sent bw: 9.03GB/s
free mem after recving shards_weights: 10.449524GB
free mem after appending shards_weights: 8.752258GB
free mem after deleting shards_weights: 8.752258GB
INFO 09-02 09:37:57 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:58 cache_engine.py:163] send kvc shards takes: 0.79s, sent out: 7.38GB, sent bw: 9.32GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:58 cache_engine.py:196] After deleting layer's shard, free mem: 23704.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:58 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:37:58 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.38 GB
do_liquid latency: 3.42s
INFO 09-02 09:37:58 llm_engine.py:757] Finished liquid for 73 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.07s, update worker latency: 0.00s, liquid model weights latency: 0.75s, init mem latency: 0.00s, liquid kvc latency: 1.32s;, current free mem on GPU0: 1.38GB
INFO 09-02 09:37:59 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:37:59 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:59 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:37:59 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:37:59 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-02 09:37:59 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:37:59 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:37:59 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.68s to send model shards
After sending shards, there are 7.58GB remaining on GPU0
INFO 09-02 09:37:59 worker.py:151] send weights shards takes: 0.75s, sent out: 6.19GB, sent bw: 8.28GB/s
INFO 09-02 09:37:59 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:38:00 cache_engine.py:163] send kvc shards takes: 0.74s, sent out: 7.38GB, sent bw: 10.01GB/s
INFO 09-02 09:38:00 cache_engine.py:196] After deleting layer's shard, free mem: 7758.31MB
INFO 09-02 09:38:00 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:38:00 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 14.95 GB
INFO 09-02 09:38:00 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #4982
available space before extending GPU blocks: 14.951GB
available space after extending GPU blocks: 2.889GB, free_mem decreased: 12.062GB
INFO 09-02 09:38:01 worker.py:209] extend gpu in worker takes: 0.46s
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.388977GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 23.961243GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 23.961243GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.524GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.461GB, free_mem decreased: 12.062GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:01 worker.py:209] extend gpu in worker takes: 0.46s
INFO 09-02 09:38:01 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.47s, there are 2.889GB left on GPU 0
do_liquid latency: 2.67s
INFO 09-02 09:38:01 llm_engine.py:757] Finished liquid for 74 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.62s, update worker latency: 0.00s, liquid model weights latency: 0.75s, init mem latency: 0.00s, liquid kvc latency: 0.87s;, current free mem on GPU0: 2.89GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:38:02 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:38:02 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.65s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.14GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:03 worker.py:151] send weights shards takes: 0.69s, sent out: 6.19GB, sent bw: 9.02GB/s
free mem after recving shards_weights: 10.445618GB
free mem after appending shards_weights: 8.779602GB
free mem after deleting shards_weights: 8.779602GB
INFO 09-02 09:38:03 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:04 cache_engine.py:163] send kvc shards takes: 0.81s, sent out: 7.38GB, sent bw: 9.14GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:04 cache_engine.py:196] After deleting layer's shard, free mem: 23700.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:04 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:38:04 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.40 GB
do_liquid latency: 3.44s
INFO 09-02 09:38:04 llm_engine.py:757] Finished liquid for 75 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.07s, update worker latency: 0.00s, liquid model weights latency: 0.76s, init mem latency: 0.00s, liquid kvc latency: 1.32s;, current free mem on GPU0: 1.40GB
INFO 09-02 09:38:05 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:05 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:38:05 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:05 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:38:05 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-02 09:38:05 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:38:05 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:38:05 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.68s to send model shards
After sending shards, there are 7.60GB remaining on GPU0
INFO 09-02 09:38:06 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.38GB/s
INFO 09-02 09:38:06 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:38:07 cache_engine.py:163] send kvc shards takes: 0.74s, sent out: 7.38GB, sent bw: 9.91GB/s
INFO 09-02 09:38:07 cache_engine.py:196] After deleting layer's shard, free mem: 7786.31MB
INFO 09-02 09:38:07 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:38:07 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 14.98 GB
INFO 09-02 09:38:07 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #4989
available space before extending GPU blocks: 14.979GB
available space after extending GPU blocks: 2.916GB, free_mem decreased: 12.062GB
INFO 09-02 09:38:07 worker.py:209] extend gpu in worker takes: 0.48s
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.385071GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 23.957336GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 23.957336GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.520GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.457GB, free_mem decreased: 12.062GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:07 worker.py:209] extend gpu in worker takes: 0.48s
INFO 09-02 09:38:07 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.48s, there are 2.916GB left on GPU 0
do_liquid latency: 2.67s
INFO 09-02 09:38:07 llm_engine.py:757] Finished liquid for 76 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.60s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.85s;, current free mem on GPU0: 2.92GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:38:09 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:38:09 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.63s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.14GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:09 worker.py:151] send weights shards takes: 0.67s, sent out: 6.19GB, sent bw: 9.24GB/s
free mem after recving shards_weights: 10.441711GB
free mem after appending shards_weights: 8.869446GB
free mem after deleting shards_weights: 8.869446GB
INFO 09-02 09:38:09 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:10 cache_engine.py:163] send kvc shards takes: 0.79s, sent out: 7.38GB, sent bw: 9.38GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:10 cache_engine.py:196] After deleting layer's shard, free mem: 23696.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:10 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:38:11 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.49 GB
do_liquid latency: 3.37s
INFO 09-02 09:38:11 llm_engine.py:757] Finished liquid for 77 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.00s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 1.26s;, current free mem on GPU0: 1.49GB
INFO 09-02 09:38:11 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:11 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:38:11 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:11 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:38:11 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-02 09:38:11 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:38:11 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:38:11 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.68s to send model shards
After sending shards, there are 7.57GB remaining on GPU0
INFO 09-02 09:38:12 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.33GB/s
INFO 09-02 09:38:12 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:38:13 cache_engine.py:163] send kvc shards takes: 0.76s, sent out: 7.38GB, sent bw: 9.66GB/s
INFO 09-02 09:38:13 cache_engine.py:196] After deleting layer's shard, free mem: 7750.31MB
INFO 09-02 09:38:13 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:38:13 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 14.94 GB
INFO 09-02 09:38:13 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #4980
available space before extending GPU blocks: 14.944GB
available space after extending GPU blocks: 2.881GB, free_mem decreased: 12.062GB
INFO 09-02 09:38:13 worker.py:209] extend gpu in worker takes: 0.44s
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.381165GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 23.984680GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 23.984680GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.547GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.485GB, free_mem decreased: 12.062GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:13 worker.py:209] extend gpu in worker takes: 0.44s
INFO 09-02 09:38:13 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.44s, there are 2.881GB left on GPU 0
do_liquid latency: 2.69s
INFO 09-02 09:38:13 llm_engine.py:757] Finished liquid for 78 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.66s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.92s;, current free mem on GPU0: 2.88GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:38:15 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:38:15 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.64s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.14GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:16 worker.py:151] send weights shards takes: 0.68s, sent out: 6.19GB, sent bw: 9.12GB/s
free mem after recving shards_weights: 10.437805GB
free mem after appending shards_weights: 8.678040GB
free mem after deleting shards_weights: 8.678040GB
INFO 09-02 09:38:16 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:16 cache_engine.py:163] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.26GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:16 cache_engine.py:196] After deleting layer's shard, free mem: 23692.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:17 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:38:17 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.30 GB
do_liquid latency: 3.32s
INFO 09-02 09:38:17 llm_engine.py:757] Finished liquid for 79 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.07s, update worker latency: 0.00s, liquid model weights latency: 0.75s, init mem latency: 0.00s, liquid kvc latency: 1.32s;, current free mem on GPU0: 1.30GB
INFO 09-02 09:38:18 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:18 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:38:18 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:18 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:38:18 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-02 09:38:18 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:38:18 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:38:18 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.67s to send model shards
After sending shards, there are 7.50GB remaining on GPU0
INFO 09-02 09:38:18 worker.py:151] send weights shards takes: 0.73s, sent out: 6.19GB, sent bw: 8.43GB/s
INFO 09-02 09:38:18 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:38:19 cache_engine.py:163] send kvc shards takes: 0.74s, sent out: 7.38GB, sent bw: 9.97GB/s
INFO 09-02 09:38:19 cache_engine.py:196] After deleting layer's shard, free mem: 7682.31MB
INFO 09-02 09:38:19 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:38:19 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 14.88 GB
INFO 09-02 09:38:19 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #4963
available space before extending GPU blocks: 14.877GB
available space after extending GPU blocks: 2.877GB, free_mem decreased: 12.000GB
INFO 09-02 09:38:20 worker.py:209] extend gpu in worker takes: 0.48s
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.377258GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 23.949524GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 23.949524GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.512GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.512GB, free_mem decreased: 12.000GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:20 worker.py:209] extend gpu in worker takes: 0.48s
INFO 09-02 09:38:20 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.48s, there are 2.877GB left on GPU 0
do_liquid latency: 2.66s
INFO 09-02 09:38:20 llm_engine.py:757] Finished liquid for 80 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.60s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.86s;, current free mem on GPU0: 2.88GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:38:21 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:38:21 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.65s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.13GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:22 worker.py:151] send weights shards takes: 0.69s, sent out: 6.19GB, sent bw: 9.02GB/s
free mem after recving shards_weights: 10.431946GB
free mem after appending shards_weights: 8.797180GB
free mem after deleting shards_weights: 8.797180GB
INFO 09-02 09:38:22 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:23 cache_engine.py:163] send kvc shards takes: 0.79s, sent out: 7.38GB, sent bw: 9.32GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:23 cache_engine.py:196] After deleting layer's shard, free mem: 23686.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:23 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:38:23 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.42 GB
do_liquid latency: 3.39s
INFO 09-02 09:38:23 llm_engine.py:757] Finished liquid for 81 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.06s, update worker latency: 0.00s, liquid model weights latency: 0.75s, init mem latency: 0.00s, liquid kvc latency: 1.30s;, current free mem on GPU0: 1.42GB
INFO 09-02 09:38:24 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:24 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:38:24 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:24 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-02 09:38:24 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:38:24 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:38:24 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:38:24 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.67s to send model shards
After sending shards, there are 7.59GB remaining on GPU0
INFO 09-02 09:38:25 worker.py:151] send weights shards takes: 0.73s, sent out: 6.19GB, sent bw: 8.47GB/s
INFO 09-02 09:38:25 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:38:25 cache_engine.py:163] send kvc shards takes: 0.75s, sent out: 7.38GB, sent bw: 9.85GB/s
INFO 09-02 09:38:25 cache_engine.py:196] After deleting layer's shard, free mem: 7772.31MB
INFO 09-02 09:38:25 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:38:25 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 14.97 GB
INFO 09-02 09:38:25 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #4986
available space before extending GPU blocks: 14.965GB
available space after extending GPU blocks: 2.905GB, free_mem decreased: 12.061GB
INFO 09-02 09:38:26 worker.py:209] extend gpu in worker takes: 0.50s
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.371399GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 23.943665GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 23.943665GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.506GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.446GB, free_mem decreased: 12.061GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:26 worker.py:209] extend gpu in worker takes: 0.50s
INFO 09-02 09:38:26 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.50s, there are 2.905GB left on GPU 0
do_liquid latency: 2.68s
INFO 09-02 09:38:26 llm_engine.py:757] Finished liquid for 82 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.59s, update worker latency: 0.00s, liquid model weights latency: 0.73s, init mem latency: 0.00s, liquid kvc latency: 0.85s;, current free mem on GPU0: 2.90GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:38:27 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:38:27 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.66s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.13GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:28 worker.py:151] send weights shards takes: 0.70s, sent out: 6.19GB, sent bw: 8.86GB/s
free mem after recving shards_weights: 10.428040GB
free mem after appending shards_weights: 8.637024GB
free mem after deleting shards_weights: 8.637024GB
INFO 09-02 09:38:28 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:29 cache_engine.py:163] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.24GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:29 cache_engine.py:196] After deleting layer's shard, free mem: 23682.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:29 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:38:29 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.26 GB
do_liquid latency: 3.34s
INFO 09-02 09:38:29 llm_engine.py:757] Finished liquid for 83 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.11s, update worker latency: 0.00s, liquid model weights latency: 0.77s, init mem latency: 0.00s, liquid kvc latency: 1.35s;, current free mem on GPU0: 1.26GB
INFO 09-02 09:38:30 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:30 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:38:30 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:30 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-02 09:38:30 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:38:30 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:38:30 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:38:30 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.68s to send model shards
After sending shards, there are 7.58GB remaining on GPU0
INFO 09-02 09:38:31 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.32GB/s
INFO 09-02 09:38:31 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:38:32 cache_engine.py:163] send kvc shards takes: 0.75s, sent out: 7.38GB, sent bw: 9.85GB/s
INFO 09-02 09:38:32 cache_engine.py:196] After deleting layer's shard, free mem: 7766.31MB
INFO 09-02 09:38:32 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:38:32 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 14.96 GB
INFO 09-02 09:38:32 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #4984
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.367493GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 23.939758GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 23.939758GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.502GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.440GB, free_mem decreased: 12.062GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:32 worker.py:209] extend gpu in worker takes: 0.49s
available space before extending GPU blocks: 14.959GB
available space after extending GPU blocks: 2.897GB, free_mem decreased: 12.062GB
INFO 09-02 09:38:32 worker.py:209] extend gpu in worker takes: 0.49s
INFO 09-02 09:38:32 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.50s, there are 2.897GB left on GPU 0
do_liquid latency: 2.68s
INFO 09-02 09:38:32 llm_engine.py:757] Finished liquid for 84 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.59s, update worker latency: 0.00s, liquid model weights latency: 0.75s, init mem latency: 0.00s, liquid kvc latency: 0.84s;, current free mem on GPU0: 2.90GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:38:34 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:38:34 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.65s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.12GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:34 worker.py:151] send weights shards takes: 0.69s, sent out: 6.19GB, sent bw: 8.97GB/s
free mem after recving shards_weights: 10.422180GB
free mem after appending shards_weights: 8.818665GB
free mem after deleting shards_weights: 8.818665GB
INFO 09-02 09:38:34 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:35 cache_engine.py:163] send kvc shards takes: 0.79s, sent out: 7.38GB, sent bw: 9.29GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:35 cache_engine.py:196] After deleting layer's shard, free mem: 23678.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:35 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:38:36 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.44 GB
do_liquid latency: 3.27s
INFO 09-02 09:38:36 llm_engine.py:757] Finished liquid for 85 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.04s, update worker latency: 0.00s, liquid model weights latency: 0.76s, init mem latency: 0.00s, liquid kvc latency: 1.28s;, current free mem on GPU0: 1.44GB
INFO 09-02 09:38:36 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:36 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:38:36 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:36 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-02 09:38:36 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:38:36 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:38:36 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:38:36 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.68s to send model shards
After sending shards, there are 7.52GB remaining on GPU0
INFO 09-02 09:38:37 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.38GB/s
INFO 09-02 09:38:37 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:38:38 cache_engine.py:163] send kvc shards takes: 0.75s, sent out: 7.38GB, sent bw: 9.90GB/s
INFO 09-02 09:38:38 cache_engine.py:196] After deleting layer's shard, free mem: 7698.31MB
INFO 09-02 09:38:38 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:38:38 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 14.89 GB
INFO 09-02 09:38:38 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #4967
available space before extending GPU blocks: 14.893GB
available space after extending GPU blocks: 2.893GB, free_mem decreased: 12.000GB
INFO 09-02 09:38:38 worker.py:209] extend gpu in worker takes: 0.46s
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.363586GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 23.935852GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 23.935852GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.498GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.498GB, free_mem decreased: 12.000GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:38 worker.py:209] extend gpu in worker takes: 0.47s
INFO 09-02 09:38:38 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.47s, there are 2.893GB left on GPU 0
do_liquid latency: 2.67s
INFO 09-02 09:38:38 llm_engine.py:757] Finished liquid for 86 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.61s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.87s;, current free mem on GPU0: 2.89GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:38:40 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:38:40 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.65s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.12GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:41 worker.py:151] send weights shards takes: 0.69s, sent out: 6.19GB, sent bw: 9.01GB/s
free mem after recving shards_weights: 10.418274GB
free mem after appending shards_weights: 8.783508GB
free mem after deleting shards_weights: 8.783508GB
INFO 09-02 09:38:41 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:41 cache_engine.py:163] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.23GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:41 cache_engine.py:196] After deleting layer's shard, free mem: 23674.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:41 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:38:42 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.41 GB
do_liquid latency: 3.41s
INFO 09-02 09:38:42 llm_engine.py:757] Finished liquid for 87 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.06s, update worker latency: 0.00s, liquid model weights latency: 0.75s, init mem latency: 0.00s, liquid kvc latency: 1.31s;, current free mem on GPU0: 1.41GB
INFO 09-02 09:38:42 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:42 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:38:42 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:42 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-02 09:38:43 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:38:43 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:38:43 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:38:43 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.68s to send model shards
After sending shards, there are 7.61GB remaining on GPU0
INFO 09-02 09:38:43 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.38GB/s
INFO 09-02 09:38:43 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:38:44 cache_engine.py:163] send kvc shards takes: 0.75s, sent out: 7.38GB, sent bw: 9.80GB/s
INFO 09-02 09:38:44 cache_engine.py:196] After deleting layer's shard, free mem: 7790.31MB
INFO 09-02 09:38:44 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:38:44 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 14.98 GB
INFO 09-02 09:38:44 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #4990
available space before extending GPU blocks: 14.983GB
available space after extending GPU blocks: 2.920GB, free_mem decreased: 12.062GB
INFO 09-02 09:38:45 worker.py:209] extend gpu in worker takes: 0.46s
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.359680GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 23.931946GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 23.931946GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.494GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.432GB, free_mem decreased: 12.062GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:45 worker.py:209] extend gpu in worker takes: 0.46s
INFO 09-02 09:38:45 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.46s, there are 2.920GB left on GPU 0
do_liquid latency: 2.69s
INFO 09-02 09:38:45 llm_engine.py:757] Finished liquid for 88 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.63s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.89s;, current free mem on GPU0: 2.92GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:38:46 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:38:46 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.67s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.12GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:47 worker.py:151] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.76GB/s
free mem after recving shards_weights: 10.414368GB
free mem after appending shards_weights: 8.748352GB
free mem after deleting shards_weights: 8.748352GB
INFO 09-02 09:38:47 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:48 cache_engine.py:163] send kvc shards takes: 0.79s, sent out: 7.38GB, sent bw: 9.36GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:48 cache_engine.py:196] After deleting layer's shard, free mem: 23670.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:48 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:38:48 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.37 GB
do_liquid latency: 3.42s
INFO 09-02 09:38:48 llm_engine.py:757] Finished liquid for 89 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.09s, update worker latency: 0.00s, liquid model weights latency: 0.77s, init mem latency: 0.00s, liquid kvc latency: 1.32s;, current free mem on GPU0: 1.37GB
INFO 09-02 09:38:49 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:49 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:38:49 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:49 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-02 09:38:49 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:38:49 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:38:49 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:38:49 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.68s to send model shards
After sending shards, there are 7.70GB remaining on GPU0
INFO 09-02 09:38:50 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.36GB/s
INFO 09-02 09:38:50 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:38:50 cache_engine.py:163] send kvc shards takes: 0.74s, sent out: 7.38GB, sent bw: 10.03GB/s
INFO 09-02 09:38:50 cache_engine.py:196] After deleting layer's shard, free mem: 7882.31MB
INFO 09-02 09:38:50 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:38:50 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 15.07 GB
INFO 09-02 09:38:50 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #5013
available space before extending GPU blocks: 15.073GB
available space after extending GPU blocks: 2.885GB, free_mem decreased: 12.188GB
INFO 09-02 09:38:51 worker.py:209] extend gpu in worker takes: 0.45s
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.355774GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 23.928040GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 23.928040GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.491GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.303GB, free_mem decreased: 12.188GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:51 worker.py:209] extend gpu in worker takes: 0.45s
INFO 09-02 09:38:51 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.45s, there are 2.885GB left on GPU 0
do_liquid latency: 2.66s
INFO 09-02 09:38:51 llm_engine.py:757] Finished liquid for 90 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.61s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.87s;, current free mem on GPU0: 2.89GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:38:52 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:38:52 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.64s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.11GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:53 worker.py:151] send weights shards takes: 0.67s, sent out: 6.19GB, sent bw: 9.18GB/s
free mem after recving shards_weights: 10.410461GB
free mem after appending shards_weights: 8.713196GB
free mem after deleting shards_weights: 8.713196GB
INFO 09-02 09:38:53 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:54 cache_engine.py:163] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.24GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:54 cache_engine.py:196] After deleting layer's shard, free mem: 23666.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:54 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:38:55 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.34 GB
do_liquid latency: 3.43s
INFO 09-02 09:38:55 llm_engine.py:757] Finished liquid for 91 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.07s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 1.33s;, current free mem on GPU0: 1.34GB
INFO 09-02 09:38:55 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:38:55 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:55 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:55 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-02 09:38:55 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:38:55 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:38:55 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:38:55 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.67s to send model shards
After sending shards, there are 7.54GB remaining on GPU0
INFO 09-02 09:38:56 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.39GB/s
INFO 09-02 09:38:56 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:38:57 cache_engine.py:163] send kvc shards takes: 0.74s, sent out: 7.38GB, sent bw: 10.02GB/s
INFO 09-02 09:38:57 cache_engine.py:196] After deleting layer's shard, free mem: 7718.31MB
INFO 09-02 09:38:57 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:38:57 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 14.91 GB
INFO 09-02 09:38:57 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #4972
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.351868GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 23.924133GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 23.924133GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.487GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.487GB, free_mem decreased: 12.000GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:57 worker.py:209] extend gpu in worker takes: 0.50s
available space before extending GPU blocks: 14.912GB
available space after extending GPU blocks: 2.912GB, free_mem decreased: 12.000GB
INFO 09-02 09:38:57 worker.py:209] extend gpu in worker takes: 0.50s
INFO 09-02 09:38:57 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.50s, there are 2.912GB left on GPU 0
do_liquid latency: 2.67s
INFO 09-02 09:38:57 llm_engine.py:757] Finished liquid for 92 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.58s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.84s;, current free mem on GPU0: 2.91GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:38:59 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:38:59 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.66s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.11GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:38:59 worker.py:151] send weights shards takes: 0.70s, sent out: 6.19GB, sent bw: 8.87GB/s
free mem after recving shards_weights: 10.406555GB
free mem after appending shards_weights: 8.678040GB
free mem after deleting shards_weights: 8.678040GB
INFO 09-02 09:38:59 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:00 cache_engine.py:163] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.28GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:00 cache_engine.py:196] After deleting layer's shard, free mem: 23662.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:00 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:39:01 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.30 GB
do_liquid latency: 3.43s
INFO 09-02 09:39:01 llm_engine.py:757] Finished liquid for 93 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.11s, update worker latency: 0.00s, liquid model weights latency: 0.77s, init mem latency: 0.00s, liquid kvc latency: 1.34s;, current free mem on GPU0: 1.30GB
INFO 09-02 09:39:01 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:01 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:39:01 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:01 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-02 09:39:01 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:39:01 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:39:01 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:39:01 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.67s to send model shards
After sending shards, there are 7.63GB remaining on GPU0
INFO 09-02 09:39:02 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.39GB/s
INFO 09-02 09:39:02 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:39:03 cache_engine.py:163] send kvc shards takes: 0.74s, sent out: 7.38GB, sent bw: 9.91GB/s
INFO 09-02 09:39:03 cache_engine.py:196] After deleting layer's shard, free mem: 7810.31MB
INFO 09-02 09:39:03 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:39:03 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 15.00 GB
INFO 09-02 09:39:03 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #4995
available space before extending GPU blocks: 15.002GB
available space after extending GPU blocks: 2.877GB, free_mem decreased: 12.125GB
INFO 09-02 09:39:04 worker.py:209] extend gpu in worker takes: 0.48s
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.347961GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 23.920227GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 23.920227GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.483GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.358GB, free_mem decreased: 12.125GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:04 worker.py:209] extend gpu in worker takes: 0.48s
INFO 09-02 09:39:04 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.48s, there are 2.877GB left on GPU 0
do_liquid latency: 2.67s
INFO 09-02 09:39:04 llm_engine.py:757] Finished liquid for 94 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.60s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.85s;, current free mem on GPU0: 2.88GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:39:05 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:39:05 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.63s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.10GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:06 worker.py:151] send weights shards takes: 0.66s, sent out: 6.19GB, sent bw: 9.34GB/s
free mem after recving shards_weights: 10.402649GB
free mem after appending shards_weights: 8.924133GB
free mem after deleting shards_weights: 8.924133GB
INFO 09-02 09:39:06 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:07 cache_engine.py:163] send kvc shards takes: 0.79s, sent out: 7.38GB, sent bw: 9.30GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:07 cache_engine.py:196] After deleting layer's shard, free mem: 23658.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:07 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:39:07 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.55 GB
do_liquid latency: 3.34s
INFO 09-02 09:39:07 llm_engine.py:757] Finished liquid for 95 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.99s, update worker latency: 0.00s, liquid model weights latency: 0.73s, init mem latency: 0.00s, liquid kvc latency: 1.26s;, current free mem on GPU0: 1.55GB
INFO 09-02 09:39:08 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:08 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:39:08 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:08 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-02 09:39:08 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:39:08 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:39:08 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:39:08 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.68s to send model shards
After sending shards, there are 7.62GB remaining on GPU0
INFO 09-02 09:39:08 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.39GB/s
INFO 09-02 09:39:08 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:39:09 cache_engine.py:163] send kvc shards takes: 0.74s, sent out: 7.38GB, sent bw: 9.91GB/s
INFO 09-02 09:39:09 cache_engine.py:196] After deleting layer's shard, free mem: 7806.31MB
INFO 09-02 09:39:09 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:39:09 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 15.00 GB
INFO 09-02 09:39:09 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #4994
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.344055GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 23.916321GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 23.916321GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.479GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.354GB, free_mem decreased: 12.125GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:10 worker.py:209] extend gpu in worker takes: 0.48s
available space before extending GPU blocks: 14.998GB
available space after extending GPU blocks: 2.873GB, free_mem decreased: 12.125GB
INFO 09-02 09:39:10 worker.py:209] extend gpu in worker takes: 0.48s
INFO 09-02 09:39:10 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.48s, there are 2.873GB left on GPU 0
do_liquid latency: 2.68s
INFO 09-02 09:39:10 llm_engine.py:757] Finished liquid for 96 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.60s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.86s;, current free mem on GPU0: 2.87GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:39:11 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:39:11 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.66s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.10GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:12 worker.py:151] send weights shards takes: 0.70s, sent out: 6.19GB, sent bw: 8.88GB/s
free mem after recving shards_weights: 10.396790GB
free mem after appending shards_weights: 8.699524GB
free mem after deleting shards_weights: 8.699524GB
INFO 09-02 09:39:12 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:13 cache_engine.py:163] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.27GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:13 cache_engine.py:196] After deleting layer's shard, free mem: 23652.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:13 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:39:13 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.33 GB
do_liquid latency: 3.42s
INFO 09-02 09:39:13 llm_engine.py:757] Finished liquid for 97 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.08s, update worker latency: 0.00s, liquid model weights latency: 0.77s, init mem latency: 0.00s, liquid kvc latency: 1.32s;, current free mem on GPU0: 1.33GB
INFO 09-02 09:39:14 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:14 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:39:14 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:14 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:39:14 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-02 09:39:14 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:39:14 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:39:14 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.67s to send model shards
After sending shards, there are 7.52GB remaining on GPU0
INFO 09-02 09:39:15 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.42GB/s
INFO 09-02 09:39:15 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:39:15 cache_engine.py:163] send kvc shards takes: 0.74s, sent out: 7.38GB, sent bw: 9.91GB/s
INFO 09-02 09:39:15 cache_engine.py:196] After deleting layer's shard, free mem: 7704.31MB
INFO 09-02 09:39:16 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:39:16 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 14.90 GB
INFO 09-02 09:39:16 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #4969
available space before extending GPU blocks: 14.899GB
available space after extending GPU blocks: 2.899GB, free_mem decreased: 12.000GB
INFO 09-02 09:39:16 worker.py:209] extend gpu in worker takes: 0.48s
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.338196GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 23.910461GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 23.910461GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.473GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.473GB, free_mem decreased: 12.000GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:16 worker.py:209] extend gpu in worker takes: 0.48s
INFO 09-02 09:39:16 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.48s, there are 2.899GB left on GPU 0
do_liquid latency: 2.66s
INFO 09-02 09:39:16 llm_engine.py:757] Finished liquid for 98 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.60s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.86s;, current free mem on GPU0: 2.90GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:39:18 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:39:18 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.65s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.09GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:18 worker.py:151] send weights shards takes: 0.69s, sent out: 6.19GB, sent bw: 8.96GB/s
free mem after recving shards_weights: 10.390930GB
free mem after appending shards_weights: 8.537415GB
free mem after deleting shards_weights: 8.537415GB
INFO 09-02 09:39:18 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:19 cache_engine.py:163] send kvc shards takes: 0.79s, sent out: 7.38GB, sent bw: 9.30GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:19 cache_engine.py:196] After deleting layer's shard, free mem: 23646.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:19 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:39:20 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.16 GB
do_liquid latency: 3.43s
INFO 09-02 09:39:20 llm_engine.py:757] Finished liquid for 99 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.09s, update worker latency: 0.00s, liquid model weights latency: 0.76s, init mem latency: 0.00s, liquid kvc latency: 1.34s;, current free mem on GPU0: 1.16GB
INFO 09-02 09:39:20 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:39:20 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:20 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:20 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-02 09:39:20 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:39:20 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:39:20 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:39:20 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.67s to send model shards
After sending shards, there are 7.58GB remaining on GPU0
INFO 09-02 09:39:21 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.42GB/s
INFO 09-02 09:39:21 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:39:22 cache_engine.py:163] send kvc shards takes: 0.74s, sent out: 7.38GB, sent bw: 9.92GB/s
INFO 09-02 09:39:22 cache_engine.py:196] After deleting layer's shard, free mem: 7762.31MB
INFO 09-02 09:39:22 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:39:22 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 14.96 GB
INFO 09-02 09:39:22 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #4983
available space before extending GPU blocks: 14.955GB
available space after extending GPU blocks: 2.893GB, free_mem decreased: 12.062GB
INFO 09-02 09:39:22 worker.py:209] extend gpu in worker takes: 0.46s
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.334290GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 23.906555GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 23.906555GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.467GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.405GB, free_mem decreased: 12.062GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:22 worker.py:209] extend gpu in worker takes: 0.46s
INFO 09-02 09:39:22 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.46s, there are 2.893GB left on GPU 0
do_liquid latency: 2.67s
INFO 09-02 09:39:22 llm_engine.py:757] Finished liquid for 100 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.62s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.88s;, current free mem on GPU0: 2.89GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:39:24 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:39:24 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.63s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.09GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:25 worker.py:151] send weights shards takes: 0.67s, sent out: 6.19GB, sent bw: 9.28GB/s
free mem after recving shards_weights: 10.387024GB
free mem after appending shards_weights: 8.689758GB
free mem after deleting shards_weights: 8.689758GB
INFO 09-02 09:39:25 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:25 cache_engine.py:163] send kvc shards takes: 0.79s, sent out: 7.38GB, sent bw: 9.29GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:25 cache_engine.py:196] After deleting layer's shard, free mem: 23642.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:26 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:39:26 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.31 GB
do_liquid latency: 3.33s
INFO 09-02 09:39:26 llm_engine.py:757] Finished liquid for 101 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.06s, update worker latency: 0.00s, liquid model weights latency: 0.73s, init mem latency: 0.00s, liquid kvc latency: 1.33s;, current free mem on GPU0: 1.31GB
INFO 09-02 09:39:27 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:39:27 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:27 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:27 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-02 09:39:27 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:39:27 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:39:27 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:39:27 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.67s to send model shards
After sending shards, there are 7.73GB remaining on GPU0
INFO 09-02 09:39:27 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.40GB/s
INFO 09-02 09:39:27 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:39:28 cache_engine.py:163] send kvc shards takes: 0.73s, sent out: 7.38GB, sent bw: 10.07GB/s
INFO 09-02 09:39:28 cache_engine.py:196] After deleting layer's shard, free mem: 7918.31MB
INFO 09-02 09:39:28 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:39:28 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 15.11 GB
INFO 09-02 09:39:28 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #5022
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.328430GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 23.900696GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 23.900696GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.463GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.276GB, free_mem decreased: 12.188GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:29 worker.py:209] extend gpu in worker takes: 0.45s
available space before extending GPU blocks: 15.108GB
available space after extending GPU blocks: 2.920GB, free_mem decreased: 12.188GB
INFO 09-02 09:39:29 worker.py:209] extend gpu in worker takes: 0.45s
INFO 09-02 09:39:29 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.45s, there are 2.920GB left on GPU 0
do_liquid latency: 2.66s
INFO 09-02 09:39:29 llm_engine.py:757] Finished liquid for 102 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.62s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.88s;, current free mem on GPU0: 2.92GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:39:30 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:39:30 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.64s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.08GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:31 worker.py:151] send weights shards takes: 0.68s, sent out: 6.19GB, sent bw: 9.11GB/s
free mem after recving shards_weights: 10.383118GB
free mem after appending shards_weights: 8.560852GB
free mem after deleting shards_weights: 8.560852GB
INFO 09-02 09:39:31 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:32 cache_engine.py:163] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.21GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:32 cache_engine.py:196] After deleting layer's shard, free mem: 23638.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:32 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:39:32 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.19 GB
do_liquid latency: 3.44s
INFO 09-02 09:39:32 llm_engine.py:757] Finished liquid for 103 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.09s, update worker latency: 0.00s, liquid model weights latency: 0.75s, init mem latency: 0.00s, liquid kvc latency: 1.34s;, current free mem on GPU0: 1.19GB
INFO 09-02 09:39:33 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:39:33 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:33 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:33 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-02 09:39:33 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:39:33 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:39:33 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:39:33 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.67s to send model shards
After sending shards, there are 7.51GB remaining on GPU0
INFO 09-02 09:39:34 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.36GB/s
INFO 09-02 09:39:34 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:39:34 cache_engine.py:163] send kvc shards takes: 0.75s, sent out: 7.38GB, sent bw: 9.89GB/s
INFO 09-02 09:39:34 cache_engine.py:196] After deleting layer's shard, free mem: 7690.31MB
INFO 09-02 09:39:34 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:39:34 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 14.89 GB
INFO 09-02 09:39:34 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #4965
available space before extending GPU blocks: 14.885GB
available space after extending GPU blocks: 2.885GB, free_mem decreased: 12.000GB
INFO 09-02 09:39:35 worker.py:209] extend gpu in worker takes: 0.49s
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.324524GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 23.896790GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 23.896790GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.459GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.459GB, free_mem decreased: 12.000GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:35 worker.py:209] extend gpu in worker takes: 0.49s
INFO 09-02 09:39:35 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.49s, there are 2.885GB left on GPU 0
do_liquid latency: 2.67s
INFO 09-02 09:39:35 llm_engine.py:757] Finished liquid for 104 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.60s, update worker latency: 0.00s, liquid model weights latency: 0.75s, init mem latency: 0.00s, liquid kvc latency: 0.85s;, current free mem on GPU0: 2.89GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:39:36 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:39:36 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.66s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.08GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:37 worker.py:151] send weights shards takes: 0.70s, sent out: 6.19GB, sent bw: 8.78GB/s
free mem after recving shards_weights: 10.379211GB
free mem after appending shards_weights: 8.806946GB
free mem after deleting shards_weights: 8.806946GB
INFO 09-02 09:39:37 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:38 cache_engine.py:163] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.19GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:38 cache_engine.py:196] After deleting layer's shard, free mem: 23634.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:38 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:39:39 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.43 GB
do_liquid latency: 3.43s
INFO 09-02 09:39:39 llm_engine.py:757] Finished liquid for 105 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.08s, update worker latency: 0.00s, liquid model weights latency: 0.77s, init mem latency: 0.00s, liquid kvc latency: 1.31s;, current free mem on GPU0: 1.43GB
INFO 09-02 09:39:39 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:39 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:39:39 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:39 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-02 09:39:39 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:39:39 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:39:39 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:39:39 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.67s to send model shards
After sending shards, there are 7.51GB remaining on GPU0
INFO 09-02 09:39:40 worker.py:151] send weights shards takes: 0.73s, sent out: 6.19GB, sent bw: 8.45GB/s
INFO 09-02 09:39:40 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:39:41 cache_engine.py:163] send kvc shards takes: 0.74s, sent out: 7.38GB, sent bw: 9.98GB/s
INFO 09-02 09:39:41 cache_engine.py:196] After deleting layer's shard, free mem: 7686.31MB
INFO 09-02 09:39:41 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:39:41 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 14.88 GB
INFO 09-02 09:39:41 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #4964
available space before extending GPU blocks: 14.881GB
available space after extending GPU blocks: 2.881GB, free_mem decreased: 12.000GB
INFO 09-02 09:39:41 worker.py:209] extend gpu in worker takes: 0.44s
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.320618GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 23.892883GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 23.892883GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.455GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.455GB, free_mem decreased: 12.000GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:41 worker.py:209] extend gpu in worker takes: 0.44s
INFO 09-02 09:39:41 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.44s, there are 2.881GB left on GPU 0
do_liquid latency: 2.66s
INFO 09-02 09:39:41 llm_engine.py:757] Finished liquid for 106 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.63s, update worker latency: 0.00s, liquid model weights latency: 0.73s, init mem latency: 0.00s, liquid kvc latency: 0.90s;, current free mem on GPU0: 2.88GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:39:43 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:39:43 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.66s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.08GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:43 worker.py:151] send weights shards takes: 0.69s, sent out: 6.19GB, sent bw: 8.95GB/s
free mem after recving shards_weights: 10.375305GB
free mem after appending shards_weights: 8.584290GB
free mem after deleting shards_weights: 8.584290GB
INFO 09-02 09:39:44 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:44 cache_engine.py:163] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.19GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:44 cache_engine.py:196] After deleting layer's shard, free mem: 23630.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:44 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:39:45 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.21 GB
do_liquid latency: 3.40s
INFO 09-02 09:39:45 llm_engine.py:757] Finished liquid for 107 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.07s, update worker latency: 0.00s, liquid model weights latency: 0.76s, init mem latency: 0.00s, liquid kvc latency: 1.31s;, current free mem on GPU0: 1.21GB
INFO 09-02 09:39:45 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:45 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:39:45 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:45 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-02 09:39:45 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:39:45 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:39:45 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:39:45 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.67s to send model shards
After sending shards, there are 7.53GB remaining on GPU0
INFO 09-02 09:39:46 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.41GB/s
INFO 09-02 09:39:46 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:39:47 cache_engine.py:163] send kvc shards takes: 0.74s, sent out: 7.38GB, sent bw: 9.93GB/s
INFO 09-02 09:39:47 cache_engine.py:196] After deleting layer's shard, free mem: 7714.31MB
INFO 09-02 09:39:47 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:39:47 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 14.91 GB
INFO 09-02 09:39:47 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #4971
available space before extending GPU blocks: 14.909GB
available space after extending GPU blocks: 2.909GB, free_mem decreased: 12.000GB
INFO 09-02 09:39:48 worker.py:209] extend gpu in worker takes: 0.48s
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.316711GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 23.888977GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 23.888977GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.451GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.451GB, free_mem decreased: 12.000GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:48 worker.py:209] extend gpu in worker takes: 0.49s
INFO 09-02 09:39:48 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.49s, there are 2.909GB left on GPU 0
do_liquid latency: 2.66s
INFO 09-02 09:39:48 llm_engine.py:757] Finished liquid for 108 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.58s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.85s;, current free mem on GPU0: 2.91GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:39:49 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:39:49 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.64s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.07GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:50 worker.py:151] send weights shards takes: 0.67s, sent out: 6.19GB, sent bw: 9.21GB/s
free mem after recving shards_weights: 10.371399GB
free mem after appending shards_weights: 8.611633GB
free mem after deleting shards_weights: 8.611633GB
INFO 09-02 09:39:50 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:51 cache_engine.py:163] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.25GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:51 cache_engine.py:196] After deleting layer's shard, free mem: 23626.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:51 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:39:51 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.24 GB
do_liquid latency: 3.42s
INFO 09-02 09:39:51 llm_engine.py:757] Finished liquid for 109 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.07s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 1.33s;, current free mem on GPU0: 1.24GB
INFO 09-02 09:39:52 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:39:52 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:52 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:52 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-02 09:39:52 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:39:52 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:39:52 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:39:52 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.68s to send model shards
After sending shards, there are 7.44GB remaining on GPU0
INFO 09-02 09:39:53 worker.py:151] send weights shards takes: 0.75s, sent out: 6.19GB, sent bw: 8.31GB/s
INFO 09-02 09:39:53 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:39:53 cache_engine.py:163] send kvc shards takes: 0.74s, sent out: 7.38GB, sent bw: 9.91GB/s
INFO 09-02 09:39:53 cache_engine.py:196] After deleting layer's shard, free mem: 7614.31MB
INFO 09-02 09:39:53 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:39:53 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 14.81 GB
INFO 09-02 09:39:53 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #4946
available space before extending GPU blocks: 14.811GB
available space after extending GPU blocks: 2.873GB, free_mem decreased: 11.938GB
INFO 09-02 09:39:54 worker.py:209] extend gpu in worker takes: 0.45s
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.312805GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 23.885071GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 23.885071GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.448GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.510GB, free_mem decreased: 11.938GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:54 worker.py:209] extend gpu in worker takes: 0.45s
INFO 09-02 09:39:54 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.45s, there are 2.873GB left on GPU 0
do_liquid latency: 2.68s
INFO 09-02 09:39:54 llm_engine.py:757] Finished liquid for 110 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.64s, update worker latency: 0.00s, liquid model weights latency: 0.75s, init mem latency: 0.00s, liquid kvc latency: 0.89s;, current free mem on GPU0: 2.87GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:39:55 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:39:55 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.64s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.07GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:56 worker.py:151] send weights shards takes: 0.67s, sent out: 6.19GB, sent bw: 9.20GB/s
free mem after recving shards_weights: 10.367493GB
free mem after appending shards_weights: 8.482727GB
free mem after deleting shards_weights: 8.482727GB
INFO 09-02 09:39:56 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:57 cache_engine.py:163] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.24GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:57 cache_engine.py:196] After deleting layer's shard, free mem: 23622.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:57 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:39:57 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.11 GB
do_liquid latency: 3.34s
INFO 09-02 09:39:57 llm_engine.py:757] Finished liquid for 111 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.08s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 1.34s;, current free mem on GPU0: 1.11GB
INFO 09-02 09:39:58 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:58 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:39:58 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:39:58 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-02 09:39:58 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:39:58 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:39:58 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:39:58 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.67s to send model shards
After sending shards, there are 7.43GB remaining on GPU0
INFO 09-02 09:39:59 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.40GB/s
INFO 09-02 09:39:59 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:39:59 cache_engine.py:163] send kvc shards takes: 0.75s, sent out: 7.38GB, sent bw: 9.85GB/s
INFO 09-02 09:39:59 cache_engine.py:196] After deleting layer's shard, free mem: 7610.31MB
INFO 09-02 09:40:00 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:40:00 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 14.81 GB
INFO 09-02 09:40:00 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #4945
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.308899GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 23.881165GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 23.881165GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.444GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.506GB, free_mem decreased: 11.938GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:00 worker.py:209] extend gpu in worker takes: 0.47s
available space before extending GPU blocks: 14.807GB
available space after extending GPU blocks: 2.867GB, free_mem decreased: 11.939GB
INFO 09-02 09:40:00 worker.py:209] extend gpu in worker takes: 0.47s
INFO 09-02 09:40:00 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.47s, there are 2.867GB left on GPU 0
do_liquid latency: 2.68s
INFO 09-02 09:40:00 llm_engine.py:757] Finished liquid for 112 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.62s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.88s;, current free mem on GPU0: 2.87GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:40:02 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:40:02 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.65s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.06GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:02 worker.py:151] send weights shards takes: 0.68s, sent out: 6.19GB, sent bw: 9.06GB/s
free mem after recving shards_weights: 10.357727GB
free mem after appending shards_weights: 8.660461GB
free mem after deleting shards_weights: 8.660461GB
INFO 09-02 09:40:02 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:03 cache_engine.py:163] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.18GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:03 cache_engine.py:196] After deleting layer's shard, free mem: 23614.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:03 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:40:04 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.29 GB
do_liquid latency: 3.38s
INFO 09-02 09:40:04 llm_engine.py:757] Finished liquid for 113 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.07s, update worker latency: 0.00s, liquid model weights latency: 0.75s, init mem latency: 0.00s, liquid kvc latency: 1.32s;, current free mem on GPU0: 1.29GB
INFO 09-02 09:40:04 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:04 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:40:04 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:04 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:40:04 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-02 09:40:04 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:40:04 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:40:04 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.68s to send model shards
After sending shards, there are 7.48GB remaining on GPU0
INFO 09-02 09:40:05 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.33GB/s
INFO 09-02 09:40:05 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:40:06 cache_engine.py:163] send kvc shards takes: 0.75s, sent out: 7.38GB, sent bw: 9.88GB/s
INFO 09-02 09:40:06 cache_engine.py:196] After deleting layer's shard, free mem: 7664.31MB
INFO 09-02 09:40:06 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:40:06 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 14.86 GB
INFO 09-02 09:40:06 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #4959
available space before extending GPU blocks: 14.860GB
available space after extending GPU blocks: 2.924GB, free_mem decreased: 11.936GB
INFO 09-02 09:40:06 worker.py:209] extend gpu in worker takes: 0.45s
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.301086GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 23.873352GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 23.873352GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.436GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.500GB, free_mem decreased: 11.936GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:06 worker.py:209] extend gpu in worker takes: 0.45s
INFO 09-02 09:40:06 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.46s, there are 2.924GB left on GPU 0
do_liquid latency: 2.67s
INFO 09-02 09:40:06 llm_engine.py:757] Finished liquid for 114 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.63s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.89s;, current free mem on GPU0: 2.92GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:40:08 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:40:08 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.65s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.06GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:09 worker.py:151] send weights shards takes: 0.69s, sent out: 6.19GB, sent bw: 8.98GB/s
free mem after recving shards_weights: 10.353821GB
free mem after appending shards_weights: 8.594055GB
free mem after deleting shards_weights: 8.594055GB
INFO 09-02 09:40:09 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:09 cache_engine.py:163] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.19GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:09 cache_engine.py:196] After deleting layer's shard, free mem: 23610.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:10 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:40:10 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.22 GB
do_liquid latency: 3.42s
INFO 09-02 09:40:10 llm_engine.py:757] Finished liquid for 115 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.09s, update worker latency: 0.00s, liquid model weights latency: 0.76s, init mem latency: 0.00s, liquid kvc latency: 1.33s;, current free mem on GPU0: 1.22GB
INFO 09-02 09:40:11 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:11 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:40:11 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:11 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-02 09:40:11 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:40:11 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:40:11 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:40:11 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.68s to send model shards
After sending shards, there are 7.54GB remaining on GPU0
INFO 09-02 09:40:11 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.37GB/s
INFO 09-02 09:40:11 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:40:12 cache_engine.py:163] send kvc shards takes: 0.74s, sent out: 7.38GB, sent bw: 9.99GB/s
INFO 09-02 09:40:12 cache_engine.py:196] After deleting layer's shard, free mem: 7724.31MB
INFO 09-02 09:40:12 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:40:12 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 14.92 GB
INFO 09-02 09:40:12 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #4974
available space before extending GPU blocks: 14.918GB
available space after extending GPU blocks: 2.918GB, free_mem decreased: 12.000GB
INFO 09-02 09:40:13 worker.py:209] extend gpu in worker takes: 0.50s
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.297180GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 23.869446GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 23.869446GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.432GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.432GB, free_mem decreased: 12.000GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:13 worker.py:209] extend gpu in worker takes: 0.50s
INFO 09-02 09:40:13 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.50s, there are 2.918GB left on GPU 0
do_liquid latency: 2.66s
INFO 09-02 09:40:13 llm_engine.py:757] Finished liquid for 116 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.57s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.83s;, current free mem on GPU0: 2.92GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:40:14 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:40:14 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.64s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.05GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:15 worker.py:151] send weights shards takes: 0.68s, sent out: 6.19GB, sent bw: 9.11GB/s
free mem after recving shards_weights: 10.349915GB
free mem after appending shards_weights: 8.840149GB
free mem after deleting shards_weights: 8.840149GB
INFO 09-02 09:40:15 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:16 cache_engine.py:163] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.23GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:16 cache_engine.py:196] After deleting layer's shard, free mem: 23606.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:16 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:40:16 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.47 GB
do_liquid latency: 3.34s
INFO 09-02 09:40:16 llm_engine.py:757] Finished liquid for 117 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.02s, update worker latency: 0.00s, liquid model weights latency: 0.75s, init mem latency: 0.00s, liquid kvc latency: 1.27s;, current free mem on GPU0: 1.47GB
INFO 09-02 09:40:17 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:17 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:40:17 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:17 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-02 09:40:17 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:40:17 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:40:17 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:40:17 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.67s to send model shards
After sending shards, there are 7.54GB remaining on GPU0
INFO 09-02 09:40:18 worker.py:151] send weights shards takes: 0.73s, sent out: 6.19GB, sent bw: 8.43GB/s
INFO 09-02 09:40:18 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:40:18 cache_engine.py:163] send kvc shards takes: 0.74s, sent out: 7.38GB, sent bw: 9.96GB/s
INFO 09-02 09:40:18 cache_engine.py:196] After deleting layer's shard, free mem: 7720.31MB
INFO 09-02 09:40:18 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:40:18 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 14.91 GB
INFO 09-02 09:40:18 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #4973
available space before extending GPU blocks: 14.914GB
available space after extending GPU blocks: 2.914GB, free_mem decreased: 12.000GB
INFO 09-02 09:40:19 worker.py:209] extend gpu in worker takes: 0.47s
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.293274GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 23.865540GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 23.865540GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.428GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.428GB, free_mem decreased: 12.000GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:19 worker.py:209] extend gpu in worker takes: 0.47s
INFO 09-02 09:40:19 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.47s, there are 2.914GB left on GPU 0
do_liquid latency: 2.66s
INFO 09-02 09:40:19 llm_engine.py:757] Finished liquid for 118 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.60s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.86s;, current free mem on GPU0: 2.91GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:40:20 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:40:20 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.63s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.05GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:21 worker.py:151] send weights shards takes: 0.66s, sent out: 6.19GB, sent bw: 9.32GB/s
free mem after recving shards_weights: 10.346008GB
free mem after appending shards_weights: 8.679993GB
free mem after deleting shards_weights: 8.679993GB
INFO 09-02 09:40:21 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:22 cache_engine.py:163] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.21GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:22 cache_engine.py:196] After deleting layer's shard, free mem: 23602.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:22 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:40:22 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.30 GB
do_liquid latency: 3.39s
INFO 09-02 09:40:22 llm_engine.py:757] Finished liquid for 119 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.04s, update worker latency: 0.00s, liquid model weights latency: 0.73s, init mem latency: 0.00s, liquid kvc latency: 1.31s;, current free mem on GPU0: 1.30GB
INFO 09-02 09:40:23 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:23 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:23 pynccl.py:65] vLLM is using nccl==2.20.5
INFO 09-02 09:40:23 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-02 09:40:23 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:40:23 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:40:23 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:40:23 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.69s to send model shards
After sending shards, there are 7.50GB remaining on GPU0
INFO 09-02 09:40:24 worker.py:151] send weights shards takes: 0.75s, sent out: 6.19GB, sent bw: 8.21GB/s
INFO 09-02 09:40:24 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:40:25 cache_engine.py:163] send kvc shards takes: 0.74s, sent out: 7.38GB, sent bw: 9.94GB/s
INFO 09-02 09:40:25 cache_engine.py:196] After deleting layer's shard, free mem: 7684.31MB
INFO 09-02 09:40:25 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:40:25 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 14.88 GB
INFO 09-02 09:40:25 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #4964
available space before extending GPU blocks: 14.879GB
available space after extending GPU blocks: 2.879GB, free_mem decreased: 12.000GB
INFO 09-02 09:40:25 worker.py:209] extend gpu in worker takes: 0.49s
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.289368GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 23.861633GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 23.861633GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.424GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.424GB, free_mem decreased: 12.000GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:25 worker.py:209] extend gpu in worker takes: 0.49s
INFO 09-02 09:40:25 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.49s, there are 2.879GB left on GPU 0
do_liquid latency: 2.71s
INFO 09-02 09:40:25 llm_engine.py:757] Finished liquid for 120 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.60s, update worker latency: 0.00s, liquid model weights latency: 0.76s, init mem latency: 0.00s, liquid kvc latency: 0.85s;, current free mem on GPU0: 2.88GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:40:27 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:40:27 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.64s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.05GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:27 worker.py:151] send weights shards takes: 0.67s, sent out: 6.19GB, sent bw: 9.20GB/s
free mem after recving shards_weights: 10.342102GB
free mem after appending shards_weights: 8.769836GB
free mem after deleting shards_weights: 8.769836GB
INFO 09-02 09:40:27 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:28 cache_engine.py:163] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.22GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:28 cache_engine.py:196] After deleting layer's shard, free mem: 23598.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:28 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:40:29 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.39 GB
do_liquid latency: 3.34s
INFO 09-02 09:40:29 llm_engine.py:757] Finished liquid for 121 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.07s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 1.33s;, current free mem on GPU0: 1.39GB
INFO 09-02 09:40:29 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:29 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:29 pynccl.py:65] vLLM is using nccl==2.20.5
INFO 09-02 09:40:29 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:40:29 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-02 09:40:29 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:40:29 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:40:29 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.67s to send model shards
After sending shards, there are 7.56GB remaining on GPU0
INFO 09-02 09:40:30 worker.py:151] send weights shards takes: 0.73s, sent out: 6.19GB, sent bw: 8.43GB/s
INFO 09-02 09:40:30 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:40:31 cache_engine.py:163] send kvc shards takes: 0.75s, sent out: 7.38GB, sent bw: 9.89GB/s
INFO 09-02 09:40:31 cache_engine.py:196] After deleting layer's shard, free mem: 7744.31MB
INFO 09-02 09:40:31 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:40:31 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 14.94 GB
INFO 09-02 09:40:31 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #4979
available space before extending GPU blocks: 14.938GB
available space after extending GPU blocks: 2.875GB, free_mem decreased: 12.062GB
INFO 09-02 09:40:31 worker.py:209] extend gpu in worker takes: 0.51s
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.285461GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 23.857727GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 23.857727GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.420GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.358GB, free_mem decreased: 12.062GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:31 worker.py:209] extend gpu in worker takes: 0.51s
INFO 09-02 09:40:31 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.51s, there are 2.875GB left on GPU 0
do_liquid latency: 2.67s
INFO 09-02 09:40:31 llm_engine.py:757] Finished liquid for 122 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.57s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.83s;, current free mem on GPU0: 2.88GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:40:33 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:40:33 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.66s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.04GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:34 worker.py:151] send weights shards takes: 0.70s, sent out: 6.19GB, sent bw: 8.89GB/s
free mem after recving shards_weights: 10.338196GB
free mem after appending shards_weights: 8.734680GB
free mem after deleting shards_weights: 8.734680GB
INFO 09-02 09:40:34 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:35 cache_engine.py:163] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.20GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:35 cache_engine.py:196] After deleting layer's shard, free mem: 23594.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:35 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:40:35 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.36 GB
do_liquid latency: 3.41s
INFO 09-02 09:40:35 llm_engine.py:757] Finished liquid for 123 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.08s, update worker latency: 0.00s, liquid model weights latency: 0.77s, init mem latency: 0.00s, liquid kvc latency: 1.32s;, current free mem on GPU0: 1.36GB
INFO 09-02 09:40:36 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:36 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:40:36 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:36 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:40:36 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-02 09:40:36 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:40:36 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:40:36 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.67s to send model shards
After sending shards, there are 7.56GB remaining on GPU0
INFO 09-02 09:40:36 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.40GB/s
INFO 09-02 09:40:36 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:40:37 cache_engine.py:163] send kvc shards takes: 0.75s, sent out: 7.38GB, sent bw: 9.90GB/s
INFO 09-02 09:40:37 cache_engine.py:196] After deleting layer's shard, free mem: 7740.31MB
INFO 09-02 09:40:37 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:40:37 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 14.93 GB
INFO 09-02 09:40:37 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #4978
available space before extending GPU blocks: 14.934GB
available space after extending GPU blocks: 2.871GB, free_mem decreased: 12.062GB
INFO 09-02 09:40:38 worker.py:209] extend gpu in worker takes: 0.49s
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.281555GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 23.853821GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 23.853821GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.416GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.354GB, free_mem decreased: 12.062GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:38 worker.py:209] extend gpu in worker takes: 0.49s
INFO 09-02 09:40:38 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.49s, there are 2.871GB left on GPU 0
do_liquid latency: 2.67s
INFO 09-02 09:40:38 llm_engine.py:757] Finished liquid for 124 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.59s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.85s;, current free mem on GPU0: 2.87GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:40:39 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:40:39 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.67s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.04GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:40 worker.py:151] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.78GB/s
free mem after recving shards_weights: 10.334290GB
free mem after appending shards_weights: 8.668274GB
free mem after deleting shards_weights: 8.668274GB
INFO 09-02 09:40:40 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:41 cache_engine.py:163] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.27GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:41 cache_engine.py:196] After deleting layer's shard, free mem: 23590.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:41 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:40:41 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.29 GB
do_liquid latency: 3.45s
INFO 09-02 09:40:41 llm_engine.py:757] Finished liquid for 125 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.11s, update worker latency: 0.00s, liquid model weights latency: 0.77s, init mem latency: 0.00s, liquid kvc latency: 1.34s;, current free mem on GPU0: 1.29GB
INFO 09-02 09:40:42 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:42 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-02 09:40:42 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:42 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=297778)[0;0m WARNING 09-02 09:40:42 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-02 09:40:42 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Before liquid, allocated space on GPU 0: 27.67 GB
INFO 09-02 09:40:42 multiproc_gpu_executor.py:213] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-02 09:40:42 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 27.67 GB
It takes: 0.67s to send model shards
After sending shards, there are 7.49GB remaining on GPU0
INFO 09-02 09:40:43 worker.py:151] send weights shards takes: 0.74s, sent out: 6.19GB, sent bw: 8.39GB/s
INFO 09-02 09:40:43 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 21.48 GB
INFO 09-02 09:40:43 cache_engine.py:163] send kvc shards takes: 0.74s, sent out: 7.38GB, sent bw: 9.93GB/s
INFO 09-02 09:40:44 cache_engine.py:196] After deleting layer's shard, free mem: 7672.31MB
INFO 09-02 09:40:44 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 1
INFO 09-02 09:40:44 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 14.87 GB
INFO 09-02 09:40:44 multiproc_gpu_executor.py:165] After scale out, num_gpu_blocks: #4961
available space before extending GPU blocks: 14.867GB
available space after extending GPU blocks: 2.866GB, free_mem decreased: 12.002GB
INFO 09-02 09:40:44 worker.py:209] extend gpu in worker takes: 0.51s
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after recving shards_weights: 24.277649GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after appending shards_weights: 23.849915GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m free mem after deleting shards_weights: 23.849915GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space before extending GPU blocks: 16.412GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m available space after extending GPU blocks: 4.410GB, free_mem decreased: 12.002GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:44 worker.py:209] extend gpu in worker takes: 0.51s
INFO 09-02 09:40:44 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.51s, there are 2.866GB left on GPU 0
do_liquid latency: 2.68s
INFO 09-02 09:40:44 llm_engine.py:757] Finished liquid for 126 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.57s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 0.83s;, current free mem on GPU0: 2.87GB
Before liquid, allocated space on GPU 0: 14.10 GB
INFO 09-02 09:40:46 multiproc_gpu_executor.py:213] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-02 09:40:46 multiproc_gpu_executor.py:226] Before liquid model weights, allocated space on GPU 0: 14.10 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m It takes: 0.67s to send model shards
[1;36m(VllmWorkerProcess pid=297778)[0;0m After sending shards, there are 23.03GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:46 worker.py:151] send weights shards takes: 0.70s, sent out: 6.19GB, sent bw: 8.81GB/s
free mem after recving shards_weights: 10.330383GB
free mem after appending shards_weights: 8.570618GB
free mem after deleting shards_weights: 8.570618GB
INFO 09-02 09:40:46 multiproc_gpu_executor.py:232] After liquid model weights, allocated space on GPU 0: 20.29 GB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:47 cache_engine.py:163] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.25GB/s
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:47 cache_engine.py:196] After deleting layer's shard, free mem: 23584.31MB
[1;36m(VllmWorkerProcess pid=297778)[0;0m INFO 09-02 09:40:47 cache_engine.py:169] Successfully send kv cache shards: [1] to rank: 0
INFO 09-02 09:40:48 multiproc_gpu_executor.py:243] After liquid model kvc, remaining space on GPU 0: 1.20 GB
do_liquid latency: 3.42s
INFO 09-02 09:40:48 llm_engine.py:757] Finished liquid for 127 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.08s, update worker latency: 0.00s, liquid model weights latency: 0.77s, init mem latency: 0.00s, liquid kvc latency: 1.31s;, current free mem on GPU0: 1.20GB
output: what is LLM?what is LLM?what is LLM?what is LLM?what is LLM?what is LLM?what is LLM?what is LLM?what is LLM?what is LLM?what is LLM?what is LLM?what is LLM?what is LLM?what is LLM?what is LLM?what is LLM?what is LLM?what is LLM?what is LLM?what is LLM?what is LLM?what is LLM?what is LLM?what is LLM?what is LL
INFO 09-02 09:40:48 multiproc_worker_utils.py:123] Killing local vLLM worker processes
