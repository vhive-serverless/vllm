INFO 09-05 07:04:02 llm_engine.py:169] Initializing an LLM engine (v0.5.0) with config: model='facebook/opt-6.7b', speculative_config=None, tokenizer='facebook/opt-6.7b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=facebook/opt-6.7b)
[1;36m(VllmWorkerProcess pid=369938)[0;0m INFO 09-05 07:04:05 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=369940)[0;0m INFO 09-05 07:04:05 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=369939)[0;0m INFO 09-05 07:04:05 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=369939)[0;0m ERROR 09-05 07:04:05 multiproc_worker_utils.py:227] Exception in worker VllmWorkerProcess while processing method init_device: CUDA error: out of memory
[1;36m(VllmWorkerProcess pid=369939)[0;0m ERROR 09-05 07:04:05 multiproc_worker_utils.py:227] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[1;36m(VllmWorkerProcess pid=369939)[0;0m ERROR 09-05 07:04:05 multiproc_worker_utils.py:227] For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[1;36m(VllmWorkerProcess pid=369939)[0;0m ERROR 09-05 07:04:05 multiproc_worker_utils.py:227] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[1;36m(VllmWorkerProcess pid=369939)[0;0m ERROR 09-05 07:04:05 multiproc_worker_utils.py:227] , Traceback (most recent call last):
[1;36m(VllmWorkerProcess pid=369939)[0;0m ERROR 09-05 07:04:05 multiproc_worker_utils.py:227]   File "/home/lrq/proj/vllm/vllm/executor/multiproc_worker_utils.py", line 224, in _run_worker_process
[1;36m(VllmWorkerProcess pid=369939)[0;0m ERROR 09-05 07:04:05 multiproc_worker_utils.py:227]     output = executor(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=369939)[0;0m ERROR 09-05 07:04:05 multiproc_worker_utils.py:227]   File "/home/lrq/proj/vllm/vllm/worker/worker.py", line 118, in init_device
[1;36m(VllmWorkerProcess pid=369939)[0;0m ERROR 09-05 07:04:05 multiproc_worker_utils.py:227]     torch.cuda.set_device(self.device)
[1;36m(VllmWorkerProcess pid=369939)[0;0m ERROR 09-05 07:04:05 multiproc_worker_utils.py:227]   File "/home/lrq/anaconda3/envs/vllml/lib/python3.10/site-packages/torch/cuda/__init__.py", line 399, in set_device
[1;36m(VllmWorkerProcess pid=369939)[0;0m ERROR 09-05 07:04:05 multiproc_worker_utils.py:227]     torch._C._cuda_setDevice(device)
[1;36m(VllmWorkerProcess pid=369939)[0;0m ERROR 09-05 07:04:05 multiproc_worker_utils.py:227] RuntimeError: CUDA error: out of memory
[1;36m(VllmWorkerProcess pid=369939)[0;0m ERROR 09-05 07:04:05 multiproc_worker_utils.py:227] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[1;36m(VllmWorkerProcess pid=369939)[0;0m ERROR 09-05 07:04:05 multiproc_worker_utils.py:227] For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[1;36m(VllmWorkerProcess pid=369939)[0;0m ERROR 09-05 07:04:05 multiproc_worker_utils.py:227] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[1;36m(VllmWorkerProcess pid=369939)[0;0m ERROR 09-05 07:04:05 multiproc_worker_utils.py:227] 
[1;36m(VllmWorkerProcess pid=369939)[0;0m ERROR 09-05 07:04:05 multiproc_worker_utils.py:227] 
ERROR 09-05 07:14:04 multiproc_worker_utils.py:120] Worker VllmWorkerProcess pid 369940 died, exit code: -15
INFO 09-05 07:14:04 multiproc_worker_utils.py:123] Killing local vLLM worker processes
