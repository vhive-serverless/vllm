INFO 09-05 12:31:08 llm_engine.py:170] Initializing an LLM engine (v0.5.0) with config: model='facebook/opt-6.7b', speculative_config=None, tokenizer='facebook/opt-6.7b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=facebook/opt-6.7b)
INFO 09-05 12:31:08 selector.py:127] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
INFO 09-05 12:31:08 selector.py:55] Using XFormers backend.
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:10 selector.py:127] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:10 selector.py:55] Using XFormers backend.
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:11 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
INFO 09-05 12:31:12 selector.py:127] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
INFO 09-05 12:31:12 selector.py:55] Using XFormers backend.
INFO 09-05 12:31:12 weight_utils.py:218] Using model weights format ['*.bin']
INFO 09-05 12:31:24 model_runner.py:214] Loading model weights took 12.4036 GB
INFO 09-05 12:31:25 distributed_gpu_executor.py:59] # GPU blocks: 1889, # CPU blocks: 512
INFO 09-05 12:31:25 selector.py:127] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
INFO 09-05 12:31:25 selector.py:55] Using XFormers backend.
INFO 09-05 12:31:28 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:31:28 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:28 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:28 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:28 custom_all_reduce_utils.py:180] reading GPU P2P access cache from /home/lrq619/.config/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 09-05 12:31:28 custom_all_reduce_utils.py:180] reading GPU P2P access cache from /home/lrq619/.config/vllm/gpu_p2p_access_cache_for_0,1.json
WARNING 09-05 12:31:28 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:31:28 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:31:28 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:31:28 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 27.738 GB, free space: 3.539GB, frag space: 0.069GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:28 selector.py:127] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:28 selector.py:55] Using XFormers backend.
It takes: 0.75s to send model shards
After sending shards, there are 9.67GB remaining on GPU0
INFO 09-05 12:31:29 worker.py:152] send weights shards takes: 0.82s, sent out: 6.21GB, sent bw: 7.56GB/s
INFO 09-05 12:31:29 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.609 GB, free space: 9.668GB, frag space: 0.134GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.07s to init model weights
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.67s to recv shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes 0.02 to load shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:29 selector.py:127] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:29 selector.py:55] Using XFormers backend.
INFO 09-05 12:31:29 cache_engine.py:146] send kvc shards takes: 0.72s, sent out: 7.38GB, sent bw: 10.29GB/s
INFO 09-05 12:31:29 cache_engine.py:186] After deleting layer's shard, free mem: 9900.31MB
INFO 09-05 12:31:29 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:31:29 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.234 GB, free space: 17.043GB, frag space: 0.138GB
INFO 09-05 12:31:29 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5518
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:30 worker.py:205] extend gpu in worker takes: 0.51s
INFO 09-05 12:31:30 worker.py:205] extend gpu in worker takes: 0.51s
INFO 09-05 12:31:30 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.51s, after extending gpu blocks: allocated space on GPU: 28.280 GB, reserved space on GPU: 28.359 GB, free space: 2.918GB, frag space: 0.079GB
do_liquid latency: 2.86s
INFO 09-05 12:31:30 llm_engine.py:758] Finished liquid for 0 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.68s, update worker latency: 0.00s, liquid model weights latency: 0.82s, init mem latency: 0.00s, liquid kvc latency: 0.86s;, current mem info on GPU0: allocated space on GPU: 28.280 GB, reserved space on GPU: 28.359 GB, free space: 2.918GB, frag space: 0.079GB
INFO 09-05 12:31:32 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:31:32 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.234 GB, free space: 16.741GB, frag space: 0.138GB
INFO 09-05 12:31:32 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.234 GB, free space: 16.741GB, frag space: 0.138GB
INFO 09-05 12:31:33 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.365 GB, free space: 10.610GB, frag space: 0.076GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.71s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 23.09GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:33 worker.py:152] send weights shards takes: 0.75s, sent out: 6.19GB, sent bw: 8.30GB/s
INFO 09-05 12:31:33 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.594 GB, free space: 10.381GB, frag space: 0.303GB
INFO 09-05 12:31:33 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.594 GB, free space: 10.381GB, frag space: 0.303GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:34 cache_engine.py:146] send kvc shards takes: 0.79s, sent out: 7.38GB, sent bw: 9.37GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:34 cache_engine.py:186] After deleting layer's shard, free mem: 23646.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:34 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:31:34 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 27.969 GB, free space: 3.008GB, frag space: 0.299GB
do_liquid latency: 3.33s
INFO 09-05 12:31:34 llm_engine.py:758] Finished liquid for 1 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.99s, update worker latency: 0.00s, liquid model weights latency: 0.81s, init mem latency: 0.00s, liquid kvc latency: 1.18s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 27.969 GB, free space: 3.008GB, frag space: 0.299GB
INFO 09-05 12:31:35 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:35 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:31:35 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:35 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:31:35 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-05 12:31:35 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:31:35 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:31:35 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 27.969 GB, free space: 3.002GB, frag space: 0.299GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:35 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.526GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:36 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.535GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:36 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.172 GB, free space: 24.108GB, frag space: 0.454GB
It takes: 0.65s to send model shards
After sending shards, there are 9.26GB remaining on GPU0
INFO 09-05 12:31:36 worker.py:152] send weights shards takes: 0.72s, sent out: 6.19GB, sent bw: 8.57GB/s
INFO 09-05 12:31:36 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.715 GB, free space: 9.256GB, frag space: 0.239GB
INFO 09-05 12:31:37 cache_engine.py:146] send kvc shards takes: 0.72s, sent out: 7.38GB, sent bw: 10.27GB/s
INFO 09-05 12:31:37 cache_engine.py:186] After deleting layer's shard, free mem: 9478.31MB
INFO 09-05 12:31:37 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:31:37 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.340 GB, free space: 16.631GB, frag space: 0.243GB
INFO 09-05 12:31:37 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5412
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:37 worker.py:205] extend gpu in worker takes: 0.45s
INFO 09-05 12:31:37 worker.py:205] extend gpu in worker takes: 0.45s
INFO 09-05 12:31:37 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.45s, after extending gpu blocks: allocated space on GPU: 27.858 GB, reserved space on GPU: 28.090 GB, free space: 2.883GB, frag space: 0.231GB
do_liquid latency: 2.64s
INFO 09-05 12:31:37 llm_engine.py:758] Finished liquid for 2 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.60s, update worker latency: 0.00s, liquid model weights latency: 0.72s, init mem latency: 0.00s, liquid kvc latency: 0.88s;, current mem info on GPU0: allocated space on GPU: 27.858 GB, reserved space on GPU: 28.090 GB, free space: 2.883GB, frag space: 0.231GB
INFO 09-05 12:31:39 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:31:39 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.340 GB, free space: 16.631GB, frag space: 0.243GB
INFO 09-05 12:31:39 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.340 GB, free space: 16.631GB, frag space: 0.243GB
INFO 09-05 12:31:40 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.377 GB, free space: 10.594GB, frag space: 0.087GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.66s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 23.09GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:40 worker.py:152] send weights shards takes: 0.70s, sent out: 6.19GB, sent bw: 8.87GB/s
INFO 09-05 12:31:40 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.668 GB, free space: 10.303GB, frag space: 0.378GB
INFO 09-05 12:31:40 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.668 GB, free space: 10.303GB, frag space: 0.378GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:40 cache_engine.py:146] send kvc shards takes: 0.81s, sent out: 7.38GB, sent bw: 9.17GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:40 cache_engine.py:186] After deleting layer's shard, free mem: 23642.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:41 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:31:41 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.043 GB, free space: 2.930GB, frag space: 0.374GB
do_liquid latency: 3.32s
INFO 09-05 12:31:41 llm_engine.py:758] Finished liquid for 3 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.98s, update worker latency: 0.00s, liquid model weights latency: 0.76s, init mem latency: 0.00s, liquid kvc latency: 1.22s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.043 GB, free space: 2.930GB, frag space: 0.374GB
INFO 09-05 12:31:42 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:42 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:31:42 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:42 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:31:42 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:31:42 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:31:42 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:31:42 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.043 GB, free space: 2.924GB, frag space: 0.374GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:42 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.522GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:42 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.532GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:42 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.156 GB, free space: 24.119GB, frag space: 0.438GB
It takes: 0.65s to send model shards
After sending shards, there are 9.09GB remaining on GPU0
INFO 09-05 12:31:42 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.68GB/s
INFO 09-05 12:31:42 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.877 GB, free space: 9.090GB, frag space: 0.401GB
INFO 09-05 12:31:43 cache_engine.py:146] send kvc shards takes: 0.72s, sent out: 7.38GB, sent bw: 10.30GB/s
INFO 09-05 12:31:43 cache_engine.py:186] After deleting layer's shard, free mem: 9308.31MB
INFO 09-05 12:31:43 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:31:43 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.502 GB, free space: 16.465GB, frag space: 0.405GB
INFO 09-05 12:31:43 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5370
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:44 worker.py:205] extend gpu in worker takes: 0.47s
INFO 09-05 12:31:44 worker.py:205] extend gpu in worker takes: 0.47s
INFO 09-05 12:31:44 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.47s, after extending gpu blocks: allocated space on GPU: 27.718 GB, reserved space on GPU: 28.064 GB, free space: 2.903GB, frag space: 0.347GB
do_liquid latency: 2.63s
INFO 09-05 12:31:44 llm_engine.py:758] Finished liquid for 4 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.57s, update worker latency: 0.00s, liquid model weights latency: 0.72s, init mem latency: 0.00s, liquid kvc latency: 0.85s;, current mem info on GPU0: allocated space on GPU: 27.718 GB, reserved space on GPU: 28.064 GB, free space: 2.903GB, frag space: 0.347GB
INFO 09-05 12:31:45 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:31:45 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.502 GB, free space: 16.465GB, frag space: 0.405GB
INFO 09-05 12:31:45 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.502 GB, free space: 16.465GB, frag space: 0.405GB
INFO 09-05 12:31:46 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.383 GB, free space: 10.584GB, frag space: 0.093GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.68s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 23.08GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:46 worker.py:152] send weights shards takes: 0.72s, sent out: 6.19GB, sent bw: 8.66GB/s
INFO 09-05 12:31:46 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.861 GB, free space: 10.106GB, frag space: 0.571GB
INFO 09-05 12:31:46 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.861 GB, free space: 10.106GB, frag space: 0.571GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:47 cache_engine.py:146] send kvc shards takes: 0.79s, sent out: 7.38GB, sent bw: 9.36GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:47 cache_engine.py:186] After deleting layer's shard, free mem: 23638.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:47 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:31:47 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.236 GB, free space: 2.731GB, frag space: 0.567GB
do_liquid latency: 3.31s
INFO 09-05 12:31:47 llm_engine.py:758] Finished liquid for 5 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.99s, update worker latency: 0.00s, liquid model weights latency: 0.78s, init mem latency: 0.00s, liquid kvc latency: 1.21s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.236 GB, free space: 2.731GB, frag space: 0.567GB
INFO 09-05 12:31:48 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:48 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:48 pynccl.py:65] vLLM is using nccl==2.20.5
INFO 09-05 12:31:48 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:31:48 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-05 12:31:48 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:31:48 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:31:48 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.236 GB, free space: 2.727GB, frag space: 0.567GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:48 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.518GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:49 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.528GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:49 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.156 GB, free space: 24.116GB, frag space: 0.438GB
It takes: 0.65s to send model shards
After sending shards, there are 9.40GB remaining on GPU0
INFO 09-05 12:31:49 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.72GB/s
INFO 09-05 12:31:49 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.564 GB, free space: 9.399GB, frag space: 0.089GB
INFO 09-05 12:31:49 cache_engine.py:146] send kvc shards takes: 0.72s, sent out: 7.38GB, sent bw: 10.28GB/s
INFO 09-05 12:31:49 cache_engine.py:186] After deleting layer's shard, free mem: 9624.31MB
INFO 09-05 12:31:49 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:31:49 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.189 GB, free space: 16.774GB, frag space: 0.093GB
INFO 09-05 12:31:49 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5424
INFO 09-05 12:31:50 worker.py:205] extend gpu in worker takes: 0.50s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:50 worker.py:205] extend gpu in worker takes: 0.50s
INFO 09-05 12:31:50 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.50s, after extending gpu blocks: allocated space on GPU: 27.905 GB, reserved space on GPU: 27.939 GB, free space: 3.024GB, frag space: 0.034GB
do_liquid latency: 2.62s
INFO 09-05 12:31:50 llm_engine.py:758] Finished liquid for 6 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.53s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.82s;, current mem info on GPU0: allocated space on GPU: 27.905 GB, reserved space on GPU: 27.939 GB, free space: 3.024GB, frag space: 0.034GB
INFO 09-05 12:31:51 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:31:51 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.189 GB, free space: 16.774GB, frag space: 0.093GB
INFO 09-05 12:31:51 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.189 GB, free space: 16.774GB, frag space: 0.093GB
INFO 09-05 12:31:52 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.383 GB, free space: 10.580GB, frag space: 0.093GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.67s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 23.08GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:52 worker.py:152] send weights shards takes: 0.72s, sent out: 6.19GB, sent bw: 8.64GB/s
INFO 09-05 12:31:52 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.643 GB, free space: 10.321GB, frag space: 0.352GB
INFO 09-05 12:31:52 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.643 GB, free space: 10.321GB, frag space: 0.352GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:53 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.23GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:53 cache_engine.py:186] After deleting layer's shard, free mem: 23634.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:53 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:31:53 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.018 GB, free space: 2.946GB, frag space: 0.348GB
do_liquid latency: 3.24s
INFO 09-05 12:31:53 llm_engine.py:758] Finished liquid for 7 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.00s, update worker latency: 0.00s, liquid model weights latency: 0.78s, init mem latency: 0.00s, liquid kvc latency: 1.22s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.018 GB, free space: 2.946GB, frag space: 0.348GB
INFO 09-05 12:31:54 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:54 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:31:54 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:54 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:31:54 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:31:54 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:31:54 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:31:54 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.018 GB, free space: 2.942GB, frag space: 0.348GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:54 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.514GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:55 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.524GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:55 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.156 GB, free space: 24.112GB, frag space: 0.438GB
It takes: 0.65s to send model shards
After sending shards, there are 9.11GB remaining on GPU0
INFO 09-05 12:31:55 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.69GB/s
INFO 09-05 12:31:55 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.852 GB, free space: 9.108GB, frag space: 0.376GB
INFO 09-05 12:31:55 cache_engine.py:146] send kvc shards takes: 0.71s, sent out: 7.38GB, sent bw: 10.32GB/s
INFO 09-05 12:31:55 cache_engine.py:186] After deleting layer's shard, free mem: 9326.31MB
INFO 09-05 12:31:55 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:31:55 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.477 GB, free space: 16.483GB, frag space: 0.380GB
INFO 09-05 12:31:55 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5374
INFO 09-05 12:31:56 worker.py:205] extend gpu in worker takes: 0.48s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:56 worker.py:205] extend gpu in worker takes: 0.48s
INFO 09-05 12:31:56 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.49s, after extending gpu blocks: allocated space on GPU: 27.718 GB, reserved space on GPU: 28.039 GB, free space: 2.920GB, frag space: 0.321GB
do_liquid latency: 2.62s
INFO 09-05 12:31:56 llm_engine.py:758] Finished liquid for 8 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.54s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.82s;, current mem info on GPU0: allocated space on GPU: 27.718 GB, reserved space on GPU: 28.039 GB, free space: 2.920GB, frag space: 0.321GB
INFO 09-05 12:31:57 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:31:57 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.477 GB, free space: 16.483GB, frag space: 0.380GB
INFO 09-05 12:31:57 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.477 GB, free space: 16.483GB, frag space: 0.380GB
INFO 09-05 12:31:58 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.389 GB, free space: 10.571GB, frag space: 0.099GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.65s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 23.08GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:58 worker.py:152] send weights shards takes: 0.70s, sent out: 6.19GB, sent bw: 8.90GB/s
INFO 09-05 12:31:58 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.680 GB, free space: 10.280GB, frag space: 0.389GB
INFO 09-05 12:31:58 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.680 GB, free space: 10.280GB, frag space: 0.389GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:59 cache_engine.py:146] send kvc shards takes: 0.79s, sent out: 7.38GB, sent bw: 9.31GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:59 cache_engine.py:186] After deleting layer's shard, free mem: 23630.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:31:59 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:31:59 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.055 GB, free space: 2.905GB, frag space: 0.385GB
do_liquid latency: 3.22s
INFO 09-05 12:31:59 llm_engine.py:758] Finished liquid for 9 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.96s, update worker latency: 0.00s, liquid model weights latency: 0.75s, init mem latency: 0.00s, liquid kvc latency: 1.20s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.055 GB, free space: 2.905GB, frag space: 0.385GB
INFO 09-05 12:32:00 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:32:00 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:00 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:00 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:32:00 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:32:00 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:32:00 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:32:00 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.055 GB, free space: 2.901GB, frag space: 0.385GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:00 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.510GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:01 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.520GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:01 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.141 GB, free space: 24.123GB, frag space: 0.423GB
It takes: 0.65s to send model shards
After sending shards, there are 8.73GB remaining on GPU0
INFO 09-05 12:32:01 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.72GB/s
INFO 09-05 12:32:01 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 22.227 GB, free space: 8.729GB, frag space: 0.751GB
INFO 09-05 12:32:01 cache_engine.py:146] send kvc shards takes: 0.72s, sent out: 7.38GB, sent bw: 10.31GB/s
INFO 09-05 12:32:01 cache_engine.py:186] After deleting layer's shard, free mem: 8938.31MB
INFO 09-05 12:32:02 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:32:02 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.852 GB, free space: 16.104GB, frag space: 0.755GB
INFO 09-05 12:32:02 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5277
INFO 09-05 12:32:02 worker.py:205] extend gpu in worker takes: 0.46s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:02 worker.py:205] extend gpu in worker takes: 0.46s
INFO 09-05 12:32:02 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.47s, after extending gpu blocks: allocated space on GPU: 27.343 GB, reserved space on GPU: 28.039 GB, free space: 2.916GB, frag space: 0.696GB
do_liquid latency: 2.62s
INFO 09-05 12:32:02 llm_engine.py:758] Finished liquid for 10 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.56s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.85s;, current mem info on GPU0: allocated space on GPU: 27.343 GB, reserved space on GPU: 28.039 GB, free space: 2.916GB, frag space: 0.696GB
INFO 09-05 12:32:04 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:32:04 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.852 GB, free space: 16.104GB, frag space: 0.755GB
INFO 09-05 12:32:04 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.852 GB, free space: 16.104GB, frag space: 0.755GB
INFO 09-05 12:32:04 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.389 GB, free space: 10.567GB, frag space: 0.099GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.65s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 23.07GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:04 worker.py:152] send weights shards takes: 0.69s, sent out: 6.19GB, sent bw: 9.00GB/s
INFO 09-05 12:32:04 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.773 GB, free space: 10.182GB, frag space: 0.483GB
INFO 09-05 12:32:04 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.773 GB, free space: 10.182GB, frag space: 0.483GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:05 cache_engine.py:146] send kvc shards takes: 0.79s, sent out: 7.38GB, sent bw: 9.29GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:05 cache_engine.py:186] After deleting layer's shard, free mem: 23626.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:05 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:32:06 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.148 GB, free space: 2.807GB, frag space: 0.479GB
do_liquid latency: 3.31s
INFO 09-05 12:32:06 llm_engine.py:758] Finished liquid for 11 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.95s, update worker latency: 0.00s, liquid model weights latency: 0.76s, init mem latency: 0.00s, liquid kvc latency: 1.19s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.148 GB, free space: 2.807GB, frag space: 0.479GB
INFO 09-05 12:32:06 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:06 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:32:06 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:06 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:32:06 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-05 12:32:06 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:32:06 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:32:06 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.148 GB, free space: 2.803GB, frag space: 0.479GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:06 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.506GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:07 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.516GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:07 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.156 GB, free space: 24.104GB, frag space: 0.438GB
It takes: 0.64s to send model shards
After sending shards, there are 9.14GB remaining on GPU0
INFO 09-05 12:32:07 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.78GB/s
INFO 09-05 12:32:07 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.814 GB, free space: 9.137GB, frag space: 0.339GB
INFO 09-05 12:32:08 cache_engine.py:146] send kvc shards takes: 0.71s, sent out: 7.38GB, sent bw: 10.34GB/s
INFO 09-05 12:32:08 cache_engine.py:186] After deleting layer's shard, free mem: 9356.31MB
INFO 09-05 12:32:08 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:32:08 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.439 GB, free space: 16.512GB, frag space: 0.343GB
INFO 09-05 12:32:08 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5382
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:08 worker.py:205] extend gpu in worker takes: 0.47s
INFO 09-05 12:32:08 worker.py:205] extend gpu in worker takes: 0.47s
INFO 09-05 12:32:08 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.47s, after extending gpu blocks: allocated space on GPU: 27.741 GB, reserved space on GPU: 28.064 GB, free space: 2.887GB, frag space: 0.323GB
do_liquid latency: 2.61s
INFO 09-05 12:32:08 llm_engine.py:758] Finished liquid for 12 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.56s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.85s;, current mem info on GPU0: allocated space on GPU: 27.741 GB, reserved space on GPU: 28.064 GB, free space: 2.887GB, frag space: 0.323GB
INFO 09-05 12:32:10 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:32:10 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.439 GB, free space: 16.512GB, frag space: 0.343GB
INFO 09-05 12:32:10 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.439 GB, free space: 16.512GB, frag space: 0.343GB
INFO 09-05 12:32:10 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.383 GB, free space: 10.569GB, frag space: 0.093GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.65s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 23.07GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:10 worker.py:152] send weights shards takes: 0.69s, sent out: 6.19GB, sent bw: 8.96GB/s
INFO 09-05 12:32:10 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.799 GB, free space: 10.153GB, frag space: 0.508GB
INFO 09-05 12:32:10 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.799 GB, free space: 10.153GB, frag space: 0.508GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:11 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.26GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:11 cache_engine.py:186] After deleting layer's shard, free mem: 23622.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:11 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:32:12 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.174 GB, free space: 2.778GB, frag space: 0.504GB
do_liquid latency: 3.24s
INFO 09-05 12:32:12 llm_engine.py:758] Finished liquid for 13 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.99s, update worker latency: 0.00s, liquid model weights latency: 0.76s, init mem latency: 0.00s, liquid kvc latency: 1.23s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.174 GB, free space: 2.778GB, frag space: 0.504GB
INFO 09-05 12:32:12 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:12 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:32:12 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:12 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:32:12 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:32:12 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:32:12 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:32:12 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.174 GB, free space: 2.772GB, frag space: 0.504GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:12 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.502GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:13 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.512GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:13 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.141 GB, free space: 24.116GB, frag space: 0.423GB
It takes: 0.64s to send model shards
After sending shards, there are 8.94GB remaining on GPU0
INFO 09-05 12:32:13 worker.py:152] send weights shards takes: 0.70s, sent out: 6.19GB, sent bw: 8.82GB/s
INFO 09-05 12:32:13 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 22.002 GB, free space: 8.944GB, frag space: 0.526GB
INFO 09-05 12:32:14 cache_engine.py:146] send kvc shards takes: 0.72s, sent out: 7.38GB, sent bw: 10.31GB/s
INFO 09-05 12:32:14 cache_engine.py:186] After deleting layer's shard, free mem: 9158.31MB
INFO 09-05 12:32:14 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:32:14 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.627 GB, free space: 16.319GB, frag space: 0.530GB
INFO 09-05 12:32:14 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5332
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:14 worker.py:205] extend gpu in worker takes: 0.48s
INFO 09-05 12:32:14 worker.py:205] extend gpu in worker takes: 0.48s
INFO 09-05 12:32:14 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.48s, after extending gpu blocks: allocated space on GPU: 27.546 GB, reserved space on GPU: 28.064 GB, free space: 2.881GB, frag space: 0.519GB
do_liquid latency: 2.62s
INFO 09-05 12:32:14 llm_engine.py:758] Finished liquid for 14 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.54s, update worker latency: 0.00s, liquid model weights latency: 0.70s, init mem latency: 0.00s, liquid kvc latency: 0.84s;, current mem info on GPU0: allocated space on GPU: 27.546 GB, reserved space on GPU: 28.064 GB, free space: 2.881GB, frag space: 0.519GB
INFO 09-05 12:32:16 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:32:16 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.627 GB, free space: 16.319GB, frag space: 0.530GB
INFO 09-05 12:32:16 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.627 GB, free space: 16.319GB, frag space: 0.530GB
INFO 09-05 12:32:16 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.383 GB, free space: 10.563GB, frag space: 0.093GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.67s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 23.06GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:16 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.75GB/s
INFO 09-05 12:32:16 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.830 GB, free space: 10.116GB, frag space: 0.540GB
INFO 09-05 12:32:16 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.830 GB, free space: 10.116GB, frag space: 0.540GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:17 cache_engine.py:146] send kvc shards takes: 0.79s, sent out: 7.38GB, sent bw: 9.30GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:17 cache_engine.py:186] After deleting layer's shard, free mem: 23616.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:17 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:32:18 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.205 GB, free space: 2.741GB, frag space: 0.536GB
do_liquid latency: 3.24s
INFO 09-05 12:32:18 llm_engine.py:758] Finished liquid for 15 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.01s, update worker latency: 0.00s, liquid model weights latency: 0.77s, init mem latency: 0.00s, liquid kvc latency: 1.23s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.205 GB, free space: 2.741GB, frag space: 0.536GB
INFO 09-05 12:32:18 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:18 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:32:18 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:18 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:32:18 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:32:18 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:32:18 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:32:18 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.205 GB, free space: 2.737GB, frag space: 0.536GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:18 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.496GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:19 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.506GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:19 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.156 GB, free space: 24.094GB, frag space: 0.438GB
It takes: 0.64s to send model shards
After sending shards, there are 9.03GB remaining on GPU0
INFO 09-05 12:32:19 worker.py:152] send weights shards takes: 0.70s, sent out: 6.19GB, sent bw: 8.79GB/s
INFO 09-05 12:32:19 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.908 GB, free space: 9.034GB, frag space: 0.433GB
INFO 09-05 12:32:20 cache_engine.py:146] send kvc shards takes: 0.72s, sent out: 7.38GB, sent bw: 10.31GB/s
INFO 09-05 12:32:20 cache_engine.py:186] After deleting layer's shard, free mem: 9250.31MB
INFO 09-05 12:32:20 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:32:20 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.533 GB, free space: 16.409GB, frag space: 0.436GB
INFO 09-05 12:32:20 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5355
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:20 worker.py:205] extend gpu in worker takes: 0.47s
INFO 09-05 12:32:20 worker.py:205] extend gpu in worker takes: 0.47s
INFO 09-05 12:32:20 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.47s, after extending gpu blocks: allocated space on GPU: 27.655 GB, reserved space on GPU: 28.033 GB, free space: 2.909GB, frag space: 0.378GB
do_liquid latency: 2.62s
INFO 09-05 12:32:20 llm_engine.py:758] Finished liquid for 16 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.55s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.84s;, current mem info on GPU0: allocated space on GPU: 27.655 GB, reserved space on GPU: 28.033 GB, free space: 2.909GB, frag space: 0.378GB
INFO 09-05 12:32:22 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:32:22 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.533 GB, free space: 16.407GB, frag space: 0.436GB
INFO 09-05 12:32:22 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.533 GB, free space: 16.407GB, frag space: 0.436GB
INFO 09-05 12:32:22 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.383 GB, free space: 10.557GB, frag space: 0.093GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.66s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 23.06GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:22 worker.py:152] send weights shards takes: 0.70s, sent out: 6.19GB, sent bw: 8.88GB/s
INFO 09-05 12:32:23 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.830 GB, free space: 10.110GB, frag space: 0.540GB
INFO 09-05 12:32:23 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.830 GB, free space: 10.110GB, frag space: 0.540GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:23 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.28GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:23 cache_engine.py:186] After deleting layer's shard, free mem: 23610.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:23 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:32:24 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.205 GB, free space: 2.737GB, frag space: 0.536GB
do_liquid latency: 3.24s
INFO 09-05 12:32:24 llm_engine.py:758] Finished liquid for 17 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.00s, update worker latency: 0.00s, liquid model weights latency: 0.76s, init mem latency: 0.00s, liquid kvc latency: 1.24s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.205 GB, free space: 2.737GB, frag space: 0.536GB
INFO 09-05 12:32:24 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:24 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:32:24 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:24 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:32:24 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:32:24 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:32:24 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:32:24 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.205 GB, free space: 2.731GB, frag space: 0.536GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:24 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.491GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:25 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.500GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:25 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.156 GB, free space: 24.088GB, frag space: 0.438GB
It takes: 0.65s to send model shards
After sending shards, there are 8.96GB remaining on GPU0
INFO 09-05 12:32:25 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.75GB/s
INFO 09-05 12:32:25 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.977 GB, free space: 8.959GB, frag space: 0.501GB
INFO 09-05 12:32:26 cache_engine.py:146] send kvc shards takes: 0.72s, sent out: 7.38GB, sent bw: 10.29GB/s
INFO 09-05 12:32:26 cache_engine.py:186] After deleting layer's shard, free mem: 9174.31MB
INFO 09-05 12:32:26 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:32:26 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.602 GB, free space: 16.334GB, frag space: 0.505GB
INFO 09-05 12:32:26 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5336
INFO 09-05 12:32:26 worker.py:205] extend gpu in worker takes: 0.47s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:26 worker.py:205] extend gpu in worker takes: 0.47s
INFO 09-05 12:32:26 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.47s, after extending gpu blocks: allocated space on GPU: 27.593 GB, reserved space on GPU: 28.039 GB, free space: 2.899GB, frag space: 0.446GB
do_liquid latency: 2.62s
INFO 09-05 12:32:26 llm_engine.py:758] Finished liquid for 18 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.56s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.85s;, current mem info on GPU0: allocated space on GPU: 27.593 GB, reserved space on GPU: 28.039 GB, free space: 2.899GB, frag space: 0.446GB
INFO 09-05 12:32:28 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:32:28 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.602 GB, free space: 16.334GB, frag space: 0.505GB
INFO 09-05 12:32:28 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.602 GB, free space: 16.334GB, frag space: 0.505GB
INFO 09-05 12:32:28 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.389 GB, free space: 10.547GB, frag space: 0.099GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.66s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 23.05GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:29 worker.py:152] send weights shards takes: 0.70s, sent out: 6.19GB, sent bw: 8.81GB/s
INFO 09-05 12:32:29 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.867 GB, free space: 10.069GB, frag space: 0.577GB
INFO 09-05 12:32:29 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.867 GB, free space: 10.069GB, frag space: 0.577GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:29 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.22GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:29 cache_engine.py:186] After deleting layer's shard, free mem: 23606.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:29 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:32:30 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.242 GB, free space: 2.696GB, frag space: 0.573GB
do_liquid latency: 3.26s
INFO 09-05 12:32:30 llm_engine.py:758] Finished liquid for 19 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.02s, update worker latency: 0.00s, liquid model weights latency: 0.78s, init mem latency: 0.00s, liquid kvc latency: 1.24s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.242 GB, free space: 2.696GB, frag space: 0.573GB
INFO 09-05 12:32:30 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:30 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:32:30 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:30 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:32:31 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:32:31 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:32:31 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:32:31 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.242 GB, free space: 2.690GB, frag space: 0.573GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:31 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.487GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:31 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.496GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:31 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.156 GB, free space: 24.084GB, frag space: 0.438GB
It takes: 0.65s to send model shards
After sending shards, there are 9.11GB remaining on GPU0
INFO 09-05 12:32:31 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.75GB/s
INFO 09-05 12:32:31 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.820 GB, free space: 9.112GB, frag space: 0.345GB
INFO 09-05 12:32:32 cache_engine.py:146] send kvc shards takes: 0.72s, sent out: 7.38GB, sent bw: 10.30GB/s
INFO 09-05 12:32:32 cache_engine.py:186] After deleting layer's shard, free mem: 9330.31MB
INFO 09-05 12:32:32 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:32:32 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.445 GB, free space: 16.487GB, frag space: 0.349GB
INFO 09-05 12:32:32 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5375
INFO 09-05 12:32:33 worker.py:205] extend gpu in worker takes: 0.48s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:33 worker.py:205] extend gpu in worker takes: 0.48s
INFO 09-05 12:32:33 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.48s, after extending gpu blocks: allocated space on GPU: 27.718 GB, reserved space on GPU: 28.008 GB, free space: 2.924GB, frag space: 0.290GB
do_liquid latency: 2.61s
INFO 09-05 12:32:33 llm_engine.py:758] Finished liquid for 20 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.54s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.84s;, current mem info on GPU0: allocated space on GPU: 27.718 GB, reserved space on GPU: 28.008 GB, free space: 2.924GB, frag space: 0.290GB
INFO 09-05 12:32:34 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:32:34 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.445 GB, free space: 16.487GB, frag space: 0.349GB
INFO 09-05 12:32:34 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.445 GB, free space: 16.487GB, frag space: 0.349GB
INFO 09-05 12:32:35 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.389 GB, free space: 10.543GB, frag space: 0.099GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.67s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 23.05GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:35 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.71GB/s
INFO 09-05 12:32:35 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.867 GB, free space: 10.065GB, frag space: 0.577GB
INFO 09-05 12:32:35 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.867 GB, free space: 10.065GB, frag space: 0.577GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:36 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.21GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:36 cache_engine.py:186] After deleting layer's shard, free mem: 23602.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:36 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:32:36 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.242 GB, free space: 2.690GB, frag space: 0.573GB
do_liquid latency: 3.26s
INFO 09-05 12:32:36 llm_engine.py:758] Finished liquid for 21 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.02s, update worker latency: 0.00s, liquid model weights latency: 0.78s, init mem latency: 0.00s, liquid kvc latency: 1.24s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.242 GB, free space: 2.690GB, frag space: 0.573GB
INFO 09-05 12:32:37 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:32:37 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:37 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:37 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:32:37 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:32:37 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:32:37 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:32:37 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.242 GB, free space: 2.686GB, frag space: 0.573GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:37 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.483GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:37 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.492GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:37 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.156 GB, free space: 24.080GB, frag space: 0.438GB
It takes: 0.65s to send model shards
After sending shards, there are 8.89GB remaining on GPU0
INFO 09-05 12:32:37 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.77GB/s
INFO 09-05 12:32:37 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 22.039 GB, free space: 8.889GB, frag space: 0.563GB
INFO 09-05 12:32:38 cache_engine.py:146] send kvc shards takes: 0.71s, sent out: 7.38GB, sent bw: 10.34GB/s
INFO 09-05 12:32:38 cache_engine.py:186] After deleting layer's shard, free mem: 9102.31MB
INFO 09-05 12:32:38 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:32:38 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.664 GB, free space: 16.264GB, frag space: 0.567GB
INFO 09-05 12:32:38 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5318
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:39 worker.py:205] extend gpu in worker takes: 0.45s
INFO 09-05 12:32:39 worker.py:205] extend gpu in worker takes: 0.46s
INFO 09-05 12:32:39 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.46s, after extending gpu blocks: allocated space on GPU: 27.491 GB, reserved space on GPU: 28.039 GB, free space: 2.889GB, frag space: 0.548GB
do_liquid latency: 2.60s
INFO 09-05 12:32:39 llm_engine.py:758] Finished liquid for 22 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.55s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.85s;, current mem info on GPU0: allocated space on GPU: 27.491 GB, reserved space on GPU: 28.039 GB, free space: 2.889GB, frag space: 0.548GB
INFO 09-05 12:32:40 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:32:40 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.664 GB, free space: 16.264GB, frag space: 0.567GB
INFO 09-05 12:32:40 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.664 GB, free space: 16.264GB, frag space: 0.567GB
INFO 09-05 12:32:41 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.389 GB, free space: 10.539GB, frag space: 0.099GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.65s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 23.05GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:41 worker.py:152] send weights shards takes: 0.68s, sent out: 6.19GB, sent bw: 9.08GB/s
INFO 09-05 12:32:41 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.898 GB, free space: 10.030GB, frag space: 0.608GB
INFO 09-05 12:32:41 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.898 GB, free space: 10.030GB, frag space: 0.608GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:42 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.25GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:42 cache_engine.py:186] After deleting layer's shard, free mem: 23598.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:42 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:32:42 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.273 GB, free space: 2.655GB, frag space: 0.604GB
do_liquid latency: 3.27s
INFO 09-05 12:32:42 llm_engine.py:758] Finished liquid for 23 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.00s, update worker latency: 0.00s, liquid model weights latency: 0.76s, init mem latency: 0.00s, liquid kvc latency: 1.24s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.273 GB, free space: 2.655GB, frag space: 0.604GB
INFO 09-05 12:32:43 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:43 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:32:43 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:43 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:32:43 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-05 12:32:43 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:32:43 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:32:43 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.273 GB, free space: 2.651GB, frag space: 0.604GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:43 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.479GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:43 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.489GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:43 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.172 GB, free space: 24.061GB, frag space: 0.454GB
It takes: 0.64s to send model shards
After sending shards, there are 8.79GB remaining on GPU0
INFO 09-05 12:32:43 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.78GB/s
INFO 09-05 12:32:43 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 22.133 GB, free space: 8.791GB, frag space: 0.657GB
INFO 09-05 12:32:44 cache_engine.py:146] send kvc shards takes: 0.71s, sent out: 7.38GB, sent bw: 10.36GB/s
INFO 09-05 12:32:44 cache_engine.py:186] After deleting layer's shard, free mem: 9002.31MB
INFO 09-05 12:32:44 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:32:44 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.758 GB, free space: 16.166GB, frag space: 0.661GB
INFO 09-05 12:32:44 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5293
INFO 09-05 12:32:45 worker.py:205] extend gpu in worker takes: 0.49s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:45 worker.py:205] extend gpu in worker takes: 0.49s
INFO 09-05 12:32:45 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.49s, after extending gpu blocks: allocated space on GPU: 27.405 GB, reserved space on GPU: 28.008 GB, free space: 2.916GB, frag space: 0.603GB
do_liquid latency: 2.61s
INFO 09-05 12:32:45 llm_engine.py:758] Finished liquid for 24 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.53s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.83s;, current mem info on GPU0: allocated space on GPU: 27.405 GB, reserved space on GPU: 28.008 GB, free space: 2.916GB, frag space: 0.603GB
INFO 09-05 12:32:46 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:32:46 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.758 GB, free space: 16.166GB, frag space: 0.661GB
INFO 09-05 12:32:46 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.758 GB, free space: 16.166GB, frag space: 0.661GB
INFO 09-05 12:32:47 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.389 GB, free space: 10.535GB, frag space: 0.099GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.65s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 23.04GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:47 worker.py:152] send weights shards takes: 0.69s, sent out: 6.19GB, sent bw: 9.00GB/s
INFO 09-05 12:32:47 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.930 GB, free space: 9.994GB, frag space: 0.639GB
INFO 09-05 12:32:47 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.930 GB, free space: 9.994GB, frag space: 0.639GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:48 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.25GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:48 cache_engine.py:186] After deleting layer's shard, free mem: 23594.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:48 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:32:48 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.305 GB, free space: 2.619GB, frag space: 0.635GB
do_liquid latency: 3.24s
INFO 09-05 12:32:48 llm_engine.py:758] Finished liquid for 25 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.99s, update worker latency: 0.00s, liquid model weights latency: 0.75s, init mem latency: 0.00s, liquid kvc latency: 1.23s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.305 GB, free space: 2.619GB, frag space: 0.635GB
INFO 09-05 12:32:49 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:49 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:32:49 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:49 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:32:49 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:32:49 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:32:49 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:32:49 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.305 GB, free space: 2.616GB, frag space: 0.635GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:49 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.475GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:49 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.485GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:50 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.172 GB, free space: 24.057GB, frag space: 0.454GB
It takes: 0.65s to send model shards
After sending shards, there are 8.88GB remaining on GPU0
INFO 09-05 12:32:50 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.72GB/s
INFO 09-05 12:32:50 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 22.039 GB, free space: 8.881GB, frag space: 0.563GB
INFO 09-05 12:32:50 cache_engine.py:146] send kvc shards takes: 0.71s, sent out: 7.38GB, sent bw: 10.37GB/s
INFO 09-05 12:32:50 cache_engine.py:186] After deleting layer's shard, free mem: 9094.31MB
INFO 09-05 12:32:50 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:32:50 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.664 GB, free space: 16.256GB, frag space: 0.567GB
INFO 09-05 12:32:50 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5316
INFO 09-05 12:32:51 worker.py:205] extend gpu in worker takes: 0.47s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:51 worker.py:205] extend gpu in worker takes: 0.48s
INFO 09-05 12:32:51 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.48s, after extending gpu blocks: allocated space on GPU: 27.483 GB, reserved space on GPU: 28.039 GB, free space: 2.881GB, frag space: 0.556GB
do_liquid latency: 2.61s
INFO 09-05 12:32:51 llm_engine.py:758] Finished liquid for 26 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.54s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.83s;, current mem info on GPU0: allocated space on GPU: 27.483 GB, reserved space on GPU: 28.039 GB, free space: 2.881GB, frag space: 0.556GB
INFO 09-05 12:32:52 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:32:52 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.664 GB, free space: 16.256GB, frag space: 0.567GB
INFO 09-05 12:32:52 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.664 GB, free space: 16.256GB, frag space: 0.567GB
INFO 09-05 12:32:53 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.389 GB, free space: 10.532GB, frag space: 0.099GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.63s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 23.04GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:53 worker.py:152] send weights shards takes: 0.67s, sent out: 6.19GB, sent bw: 9.22GB/s
INFO 09-05 12:32:53 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.867 GB, free space: 10.053GB, frag space: 0.577GB
INFO 09-05 12:32:53 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.867 GB, free space: 10.053GB, frag space: 0.577GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:54 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.19GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:54 cache_engine.py:186] After deleting layer's shard, free mem: 23590.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:54 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:32:54 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.242 GB, free space: 2.678GB, frag space: 0.573GB
do_liquid latency: 3.23s
INFO 09-05 12:32:54 llm_engine.py:758] Finished liquid for 27 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.96s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 1.23s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.242 GB, free space: 2.678GB, frag space: 0.573GB
INFO 09-05 12:32:55 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:55 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:32:55 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:55 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:32:55 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:32:55 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:32:55 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:32:55 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.242 GB, free space: 2.672GB, frag space: 0.573GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:55 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.471GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:56 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.481GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:56 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.188 GB, free space: 24.037GB, frag space: 0.470GB
It takes: 0.64s to send model shards
After sending shards, there are 9.13GB remaining on GPU0
INFO 09-05 12:32:56 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.78GB/s
INFO 09-05 12:32:56 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.789 GB, free space: 9.125GB, frag space: 0.313GB
INFO 09-05 12:32:56 cache_engine.py:146] send kvc shards takes: 0.71s, sent out: 7.38GB, sent bw: 10.35GB/s
INFO 09-05 12:32:56 cache_engine.py:186] After deleting layer's shard, free mem: 9344.31MB
INFO 09-05 12:32:56 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:32:56 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.414 GB, free space: 16.500GB, frag space: 0.317GB
INFO 09-05 12:32:56 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5379
INFO 09-05 12:32:57 worker.py:205] extend gpu in worker takes: 0.49s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:57 worker.py:205] extend gpu in worker takes: 0.48s
INFO 09-05 12:32:57 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.49s, after extending gpu blocks: allocated space on GPU: 27.730 GB, reserved space on GPU: 28.039 GB, free space: 2.875GB, frag space: 0.310GB
do_liquid latency: 2.61s
INFO 09-05 12:32:57 llm_engine.py:758] Finished liquid for 28 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.53s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.82s;, current mem info on GPU0: allocated space on GPU: 27.730 GB, reserved space on GPU: 28.039 GB, free space: 2.875GB, frag space: 0.310GB
INFO 09-05 12:32:58 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:32:58 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.414 GB, free space: 16.500GB, frag space: 0.317GB
INFO 09-05 12:32:58 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.414 GB, free space: 16.500GB, frag space: 0.317GB
INFO 09-05 12:32:59 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.389 GB, free space: 10.526GB, frag space: 0.099GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.67s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 23.03GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:32:59 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.77GB/s
INFO 09-05 12:32:59 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.836 GB, free space: 10.078GB, frag space: 0.546GB
INFO 09-05 12:32:59 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.836 GB, free space: 10.078GB, frag space: 0.546GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:00 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.18GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:00 cache_engine.py:186] After deleting layer's shard, free mem: 23586.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:00 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:33:00 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.211 GB, free space: 2.703GB, frag space: 0.542GB
do_liquid latency: 3.26s
INFO 09-05 12:33:00 llm_engine.py:758] Finished liquid for 29 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.02s, update worker latency: 0.00s, liquid model weights latency: 0.77s, init mem latency: 0.00s, liquid kvc latency: 1.25s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.211 GB, free space: 2.703GB, frag space: 0.542GB
INFO 09-05 12:33:01 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:01 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:33:01 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:01 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:33:01 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:33:01 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:33:01 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:33:01 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.211 GB, free space: 2.700GB, frag space: 0.542GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:01 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.467GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:02 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.477GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:02 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.156 GB, free space: 24.065GB, frag space: 0.438GB
It takes: 0.65s to send model shards
After sending shards, there are 9.18GB remaining on GPU0
INFO 09-05 12:33:02 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.70GB/s
INFO 09-05 12:33:02 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.727 GB, free space: 9.184GB, frag space: 0.251GB
INFO 09-05 12:33:02 cache_engine.py:146] send kvc shards takes: 0.71s, sent out: 7.38GB, sent bw: 10.33GB/s
INFO 09-05 12:33:02 cache_engine.py:186] After deleting layer's shard, free mem: 9404.31MB
INFO 09-05 12:33:02 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:33:02 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.352 GB, free space: 16.559GB, frag space: 0.255GB
INFO 09-05 12:33:02 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5394
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:03 worker.py:205] extend gpu in worker takes: 0.48s
INFO 09-05 12:33:03 worker.py:205] extend gpu in worker takes: 0.49s
INFO 09-05 12:33:03 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.49s, after extending gpu blocks: allocated space on GPU: 27.788 GB, reserved space on GPU: 28.039 GB, free space: 2.871GB, frag space: 0.251GB
do_liquid latency: 2.61s
INFO 09-05 12:33:03 llm_engine.py:758] Finished liquid for 30 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.54s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.82s;, current mem info on GPU0: allocated space on GPU: 27.788 GB, reserved space on GPU: 28.039 GB, free space: 2.871GB, frag space: 0.251GB
INFO 09-05 12:33:04 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:33:04 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.352 GB, free space: 16.559GB, frag space: 0.255GB
INFO 09-05 12:33:04 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.352 GB, free space: 16.559GB, frag space: 0.255GB
INFO 09-05 12:33:05 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.389 GB, free space: 10.522GB, frag space: 0.099GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.66s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 23.03GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:05 worker.py:152] send weights shards takes: 0.70s, sent out: 6.19GB, sent bw: 8.86GB/s
INFO 09-05 12:33:05 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.836 GB, free space: 10.075GB, frag space: 0.546GB
INFO 09-05 12:33:05 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.836 GB, free space: 10.075GB, frag space: 0.546GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:06 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.21GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:06 cache_engine.py:186] After deleting layer's shard, free mem: 23582.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:06 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:33:06 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.211 GB, free space: 2.700GB, frag space: 0.542GB
do_liquid latency: 3.25s
INFO 09-05 12:33:06 llm_engine.py:758] Finished liquid for 31 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.99s, update worker latency: 0.00s, liquid model weights latency: 0.77s, init mem latency: 0.00s, liquid kvc latency: 1.23s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.211 GB, free space: 2.700GB, frag space: 0.542GB
INFO 09-05 12:33:07 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:07 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:33:07 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:07 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:33:07 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:33:07 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:33:07 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:33:07 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.211 GB, free space: 2.696GB, frag space: 0.542GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:07 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.463GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:08 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.473GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:08 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.172 GB, free space: 24.045GB, frag space: 0.454GB
It takes: 0.65s to send model shards
After sending shards, there are 9.12GB remaining on GPU0
INFO 09-05 12:33:08 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.71GB/s
INFO 09-05 12:33:08 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.789 GB, free space: 9.117GB, frag space: 0.313GB
INFO 09-05 12:33:08 cache_engine.py:146] send kvc shards takes: 0.71s, sent out: 7.38GB, sent bw: 10.35GB/s
INFO 09-05 12:33:08 cache_engine.py:186] After deleting layer's shard, free mem: 9336.31MB
INFO 09-05 12:33:09 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:33:09 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.414 GB, free space: 16.492GB, frag space: 0.317GB
INFO 09-05 12:33:09 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5377
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:09 worker.py:205] extend gpu in worker takes: 0.47s
INFO 09-05 12:33:09 worker.py:205] extend gpu in worker takes: 0.47s
INFO 09-05 12:33:09 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.47s, after extending gpu blocks: allocated space on GPU: 27.722 GB, reserved space on GPU: 28.039 GB, free space: 2.867GB, frag space: 0.317GB
do_liquid latency: 2.61s
INFO 09-05 12:33:09 llm_engine.py:758] Finished liquid for 32 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.55s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.83s;, current mem info on GPU0: allocated space on GPU: 27.722 GB, reserved space on GPU: 28.039 GB, free space: 2.867GB, frag space: 0.317GB
INFO 09-05 12:33:10 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:33:10 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.414 GB, free space: 16.491GB, frag space: 0.317GB
INFO 09-05 12:33:10 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.414 GB, free space: 16.491GB, frag space: 0.317GB
INFO 09-05 12:33:11 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.389 GB, free space: 10.516GB, frag space: 0.099GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.67s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 23.02GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:11 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.78GB/s
INFO 09-05 12:33:11 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.836 GB, free space: 10.069GB, frag space: 0.546GB
INFO 09-05 12:33:11 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.836 GB, free space: 10.069GB, frag space: 0.546GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:12 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.19GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:12 cache_engine.py:186] After deleting layer's shard, free mem: 23576.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:12 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:33:12 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.211 GB, free space: 2.696GB, frag space: 0.542GB
do_liquid latency: 3.24s
INFO 09-05 12:33:12 llm_engine.py:758] Finished liquid for 33 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.99s, update worker latency: 0.00s, liquid model weights latency: 0.77s, init mem latency: 0.00s, liquid kvc latency: 1.22s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.211 GB, free space: 2.696GB, frag space: 0.542GB
INFO 09-05 12:33:13 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:13 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:33:13 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:13 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:33:13 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:33:13 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:33:13 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:33:13 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.211 GB, free space: 2.690GB, frag space: 0.542GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:13 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.457GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:14 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.467GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:14 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.188 GB, free space: 24.024GB, frag space: 0.470GB
It takes: 0.65s to send model shards
After sending shards, there are 9.07GB remaining on GPU0
INFO 09-05 12:33:14 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.76GB/s
INFO 09-05 12:33:14 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.826 GB, free space: 9.075GB, frag space: 0.351GB
INFO 09-05 12:33:14 cache_engine.py:146] send kvc shards takes: 0.71s, sent out: 7.38GB, sent bw: 10.33GB/s
INFO 09-05 12:33:14 cache_engine.py:186] After deleting layer's shard, free mem: 9292.31MB
INFO 09-05 12:33:15 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:33:15 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.451 GB, free space: 16.450GB, frag space: 0.354GB
INFO 09-05 12:33:15 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5366
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:15 worker.py:205] extend gpu in worker takes: 0.48s
INFO 09-05 12:33:15 worker.py:205] extend gpu in worker takes: 0.48s
INFO 09-05 12:33:15 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.48s, after extending gpu blocks: allocated space on GPU: 27.679 GB, reserved space on GPU: 28.014 GB, free space: 2.889GB, frag space: 0.335GB
do_liquid latency: 2.62s
INFO 09-05 12:33:15 llm_engine.py:758] Finished liquid for 34 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.54s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.83s;, current mem info on GPU0: allocated space on GPU: 27.679 GB, reserved space on GPU: 28.014 GB, free space: 2.889GB, frag space: 0.335GB
INFO 09-05 12:33:16 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:33:16 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.451 GB, free space: 16.450GB, frag space: 0.354GB
INFO 09-05 12:33:16 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.451 GB, free space: 16.450GB, frag space: 0.354GB
INFO 09-05 12:33:17 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.395 GB, free space: 10.506GB, frag space: 0.105GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.68s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 23.02GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:17 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.67GB/s
INFO 09-05 12:33:17 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.936 GB, free space: 9.965GB, frag space: 0.645GB
INFO 09-05 12:33:17 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.936 GB, free space: 9.965GB, frag space: 0.645GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:18 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.25GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:18 cache_engine.py:186] After deleting layer's shard, free mem: 23572.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:18 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:33:18 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.311 GB, free space: 2.592GB, frag space: 0.641GB
do_liquid latency: 3.25s
INFO 09-05 12:33:18 llm_engine.py:758] Finished liquid for 35 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.01s, update worker latency: 0.00s, liquid model weights latency: 0.78s, init mem latency: 0.00s, liquid kvc latency: 1.23s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.311 GB, free space: 2.592GB, frag space: 0.641GB
INFO 09-05 12:33:19 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:19 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:33:19 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:19 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:33:19 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:33:19 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:33:19 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:33:19 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.311 GB, free space: 2.586GB, frag space: 0.641GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:19 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.453GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:20 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.463GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:20 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.172 GB, free space: 24.035GB, frag space: 0.454GB
It takes: 0.65s to send model shards
After sending shards, there are 9.07GB remaining on GPU0
INFO 09-05 12:33:20 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.74GB/s
INFO 09-05 12:33:20 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.826 GB, free space: 9.071GB, frag space: 0.351GB
INFO 09-05 12:33:21 cache_engine.py:146] send kvc shards takes: 0.71s, sent out: 7.38GB, sent bw: 10.33GB/s
INFO 09-05 12:33:21 cache_engine.py:186] After deleting layer's shard, free mem: 9288.31MB
INFO 09-05 12:33:21 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:33:21 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.451 GB, free space: 16.446GB, frag space: 0.354GB
INFO 09-05 12:33:21 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5365
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:21 worker.py:205] extend gpu in worker takes: 0.49s
INFO 09-05 12:33:21 worker.py:205] extend gpu in worker takes: 0.49s
INFO 09-05 12:33:21 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.49s, after extending gpu blocks: allocated space on GPU: 27.675 GB, reserved space on GPU: 28.014 GB, free space: 2.883GB, frag space: 0.339GB
do_liquid latency: 2.62s
INFO 09-05 12:33:21 llm_engine.py:758] Finished liquid for 36 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.54s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.83s;, current mem info on GPU0: allocated space on GPU: 27.675 GB, reserved space on GPU: 28.014 GB, free space: 2.883GB, frag space: 0.339GB
INFO 09-05 12:33:23 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:33:23 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.451 GB, free space: 16.446GB, frag space: 0.354GB
INFO 09-05 12:33:23 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.451 GB, free space: 16.446GB, frag space: 0.354GB
INFO 09-05 12:33:23 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.395 GB, free space: 10.502GB, frag space: 0.105GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.65s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 23.02GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:23 worker.py:152] send weights shards takes: 0.69s, sent out: 6.19GB, sent bw: 8.96GB/s
INFO 09-05 12:33:23 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.842 GB, free space: 10.055GB, frag space: 0.551GB
INFO 09-05 12:33:23 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.842 GB, free space: 10.055GB, frag space: 0.551GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:24 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.25GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:24 cache_engine.py:186] After deleting layer's shard, free mem: 23568.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:24 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:33:25 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.217 GB, free space: 2.680GB, frag space: 0.547GB
do_liquid latency: 3.36s
INFO 09-05 12:33:25 llm_engine.py:758] Finished liquid for 37 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.99s, update worker latency: 0.00s, liquid model weights latency: 0.76s, init mem latency: 0.00s, liquid kvc latency: 1.23s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.217 GB, free space: 2.680GB, frag space: 0.547GB
INFO 09-05 12:33:25 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:33:25 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:25 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:25 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:33:25 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:33:25 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:33:25 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:33:25 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.217 GB, free space: 2.676GB, frag space: 0.547GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:25 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.450GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:26 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.459GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:26 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.188 GB, free space: 24.016GB, frag space: 0.470GB
It takes: 0.64s to send model shards
After sending shards, there are 8.98GB remaining on GPU0
INFO 09-05 12:33:26 worker.py:152] send weights shards takes: 0.70s, sent out: 6.19GB, sent bw: 8.79GB/s
INFO 09-05 12:33:26 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.914 GB, free space: 8.979GB, frag space: 0.438GB
INFO 09-05 12:33:27 cache_engine.py:146] send kvc shards takes: 0.72s, sent out: 7.38GB, sent bw: 10.31GB/s
INFO 09-05 12:33:27 cache_engine.py:186] After deleting layer's shard, free mem: 9194.31MB
INFO 09-05 12:33:27 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:33:27 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.539 GB, free space: 16.354GB, frag space: 0.442GB
INFO 09-05 12:33:27 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5341
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:27 worker.py:205] extend gpu in worker takes: 0.48s
INFO 09-05 12:33:27 worker.py:205] extend gpu in worker takes: 0.48s
INFO 09-05 12:33:27 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.48s, after extending gpu blocks: allocated space on GPU: 27.593 GB, reserved space on GPU: 27.977 GB, free space: 2.916GB, frag space: 0.384GB
do_liquid latency: 2.62s
INFO 09-05 12:33:27 llm_engine.py:758] Finished liquid for 38 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.54s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.84s;, current mem info on GPU0: allocated space on GPU: 27.593 GB, reserved space on GPU: 27.977 GB, free space: 2.916GB, frag space: 0.384GB
INFO 09-05 12:33:29 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:33:29 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.539 GB, free space: 16.354GB, frag space: 0.442GB
INFO 09-05 12:33:29 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.539 GB, free space: 16.354GB, frag space: 0.442GB
INFO 09-05 12:33:29 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.389 GB, free space: 10.504GB, frag space: 0.099GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.66s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 23.01GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:29 worker.py:152] send weights shards takes: 0.70s, sent out: 6.19GB, sent bw: 8.90GB/s
INFO 09-05 12:33:30 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.898 GB, free space: 9.994GB, frag space: 0.608GB
INFO 09-05 12:33:30 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.898 GB, free space: 9.994GB, frag space: 0.608GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:30 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.17GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:30 cache_engine.py:186] After deleting layer's shard, free mem: 23564.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:30 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:33:31 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.273 GB, free space: 2.619GB, frag space: 0.604GB
do_liquid latency: 3.26s
INFO 09-05 12:33:31 llm_engine.py:758] Finished liquid for 39 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.01s, update worker latency: 0.00s, liquid model weights latency: 0.76s, init mem latency: 0.00s, liquid kvc latency: 1.25s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.273 GB, free space: 2.619GB, frag space: 0.604GB
INFO 09-05 12:33:31 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:31 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:33:31 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:31 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:33:31 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:33:31 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:33:31 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:33:31 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.273 GB, free space: 2.616GB, frag space: 0.604GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:31 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.446GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:32 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.455GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:32 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.156 GB, free space: 24.043GB, frag space: 0.438GB
It takes: 0.65s to send model shards
After sending shards, there are 8.91GB remaining on GPU0
INFO 09-05 12:33:32 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.75GB/s
INFO 09-05 12:33:32 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.977 GB, free space: 8.912GB, frag space: 0.501GB
INFO 09-05 12:33:33 cache_engine.py:146] send kvc shards takes: 0.72s, sent out: 7.38GB, sent bw: 10.32GB/s
INFO 09-05 12:33:33 cache_engine.py:186] After deleting layer's shard, free mem: 9126.31MB
INFO 09-05 12:33:33 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:33:33 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.602 GB, free space: 16.287GB, frag space: 0.505GB
INFO 09-05 12:33:33 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5324
INFO 09-05 12:33:33 worker.py:205] extend gpu in worker takes: 0.49s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:33 worker.py:205] extend gpu in worker takes: 0.49s
INFO 09-05 12:33:33 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.49s, after extending gpu blocks: allocated space on GPU: 27.530 GB, reserved space on GPU: 27.977 GB, free space: 2.912GB, frag space: 0.446GB
do_liquid latency: 2.61s
INFO 09-05 12:33:33 llm_engine.py:758] Finished liquid for 40 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.52s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.81s;, current mem info on GPU0: allocated space on GPU: 27.530 GB, reserved space on GPU: 27.977 GB, free space: 2.912GB, frag space: 0.446GB
INFO 09-05 12:33:35 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:33:35 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.602 GB, free space: 16.287GB, frag space: 0.505GB
INFO 09-05 12:33:35 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.602 GB, free space: 16.287GB, frag space: 0.505GB
INFO 09-05 12:33:36 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.389 GB, free space: 10.500GB, frag space: 0.099GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.65s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 23.01GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:36 worker.py:152] send weights shards takes: 0.69s, sent out: 6.19GB, sent bw: 8.93GB/s
INFO 09-05 12:33:36 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.805 GB, free space: 10.084GB, frag space: 0.514GB
INFO 09-05 12:33:36 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.805 GB, free space: 10.084GB, frag space: 0.514GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:37 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.20GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:37 cache_engine.py:186] After deleting layer's shard, free mem: 23560.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:37 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:33:37 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.180 GB, free space: 2.709GB, frag space: 0.510GB
do_liquid latency: 3.29s
INFO 09-05 12:33:37 llm_engine.py:758] Finished liquid for 41 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.97s, update worker latency: 0.00s, liquid model weights latency: 0.75s, init mem latency: 0.00s, liquid kvc latency: 1.21s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.180 GB, free space: 2.709GB, frag space: 0.510GB
INFO 09-05 12:33:38 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:38 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:38 pynccl.py:65] vLLM is using nccl==2.20.5
INFO 09-05 12:33:38 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:33:38 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-05 12:33:38 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:33:38 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:33:38 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.180 GB, free space: 2.703GB, frag space: 0.510GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:38 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.442GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:38 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.451GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:38 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.156 GB, free space: 24.039GB, frag space: 0.438GB
It takes: 0.65s to send model shards
After sending shards, there are 8.94GB remaining on GPU0
INFO 09-05 12:33:38 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.67GB/s
INFO 09-05 12:33:38 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.945 GB, free space: 8.938GB, frag space: 0.470GB
INFO 09-05 12:33:39 cache_engine.py:146] send kvc shards takes: 0.71s, sent out: 7.38GB, sent bw: 10.35GB/s
INFO 09-05 12:33:39 cache_engine.py:186] After deleting layer's shard, free mem: 9152.31MB
INFO 09-05 12:33:39 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:33:39 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.570 GB, free space: 16.313GB, frag space: 0.474GB
INFO 09-05 12:33:39 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5331
INFO 09-05 12:33:40 worker.py:205] extend gpu in worker takes: 0.45s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:40 worker.py:205] extend gpu in worker takes: 0.45s
INFO 09-05 12:33:40 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.45s, after extending gpu blocks: allocated space on GPU: 27.542 GB, reserved space on GPU: 28.008 GB, free space: 2.875GB, frag space: 0.466GB
do_liquid latency: 2.62s
INFO 09-05 12:33:40 llm_engine.py:758] Finished liquid for 42 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.57s, update worker latency: 0.00s, liquid model weights latency: 0.72s, init mem latency: 0.00s, liquid kvc latency: 0.85s;, current mem info on GPU0: allocated space on GPU: 27.542 GB, reserved space on GPU: 28.008 GB, free space: 2.875GB, frag space: 0.466GB
INFO 09-05 12:33:41 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:33:41 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.570 GB, free space: 16.313GB, frag space: 0.474GB
INFO 09-05 12:33:41 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.570 GB, free space: 16.313GB, frag space: 0.474GB
INFO 09-05 12:33:42 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.389 GB, free space: 10.494GB, frag space: 0.099GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.68s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 23.00GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:42 worker.py:152] send weights shards takes: 0.72s, sent out: 6.19GB, sent bw: 8.59GB/s
INFO 09-05 12:33:42 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.867 GB, free space: 10.016GB, frag space: 0.577GB
INFO 09-05 12:33:42 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.867 GB, free space: 10.016GB, frag space: 0.577GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:43 cache_engine.py:146] send kvc shards takes: 0.79s, sent out: 7.38GB, sent bw: 9.30GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:43 cache_engine.py:186] After deleting layer's shard, free mem: 23554.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:43 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:33:43 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.242 GB, free space: 2.641GB, frag space: 0.573GB
do_liquid latency: 3.22s
INFO 09-05 12:33:43 llm_engine.py:758] Finished liquid for 43 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.99s, update worker latency: 0.00s, liquid model weights latency: 0.78s, init mem latency: 0.00s, liquid kvc latency: 1.21s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.242 GB, free space: 2.641GB, frag space: 0.573GB
INFO 09-05 12:33:44 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:33:44 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:44 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:44 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:33:44 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:33:44 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:33:44 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:33:44 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.242 GB, free space: 2.637GB, frag space: 0.573GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:44 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.436GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:44 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.446GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:44 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.172 GB, free space: 24.018GB, frag space: 0.454GB
It takes: 0.65s to send model shards
After sending shards, there are 9.00GB remaining on GPU0
INFO 09-05 12:33:44 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.69GB/s
INFO 09-05 12:33:44 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.883 GB, free space: 8.996GB, frag space: 0.407GB
INFO 09-05 12:33:45 cache_engine.py:146] send kvc shards takes: 0.72s, sent out: 7.38GB, sent bw: 10.31GB/s
INFO 09-05 12:33:45 cache_engine.py:186] After deleting layer's shard, free mem: 9212.31MB
INFO 09-05 12:33:45 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:33:45 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.508 GB, free space: 16.371GB, frag space: 0.411GB
INFO 09-05 12:33:45 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5346
INFO 09-05 12:33:46 worker.py:205] extend gpu in worker takes: 0.45s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:46 worker.py:205] extend gpu in worker takes: 0.45s
INFO 09-05 12:33:46 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.45s, after extending gpu blocks: allocated space on GPU: 27.601 GB, reserved space on GPU: 28.008 GB, free space: 2.871GB, frag space: 0.407GB
do_liquid latency: 2.63s
INFO 09-05 12:33:46 llm_engine.py:758] Finished liquid for 44 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.58s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.87s;, current mem info on GPU0: allocated space on GPU: 27.601 GB, reserved space on GPU: 28.008 GB, free space: 2.871GB, frag space: 0.407GB
INFO 09-05 12:33:47 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:33:47 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.508 GB, free space: 16.371GB, frag space: 0.411GB
INFO 09-05 12:33:47 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.508 GB, free space: 16.371GB, frag space: 0.411GB
INFO 09-05 12:33:48 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.389 GB, free space: 10.491GB, frag space: 0.099GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.67s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 23.00GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:48 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.70GB/s
INFO 09-05 12:33:48 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.836 GB, free space: 10.043GB, frag space: 0.546GB
INFO 09-05 12:33:48 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.836 GB, free space: 10.043GB, frag space: 0.546GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:49 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.23GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:49 cache_engine.py:186] After deleting layer's shard, free mem: 23550.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:49 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:33:49 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.211 GB, free space: 2.668GB, frag space: 0.542GB
do_liquid latency: 3.26s
INFO 09-05 12:33:49 llm_engine.py:758] Finished liquid for 45 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.01s, update worker latency: 0.00s, liquid model weights latency: 0.77s, init mem latency: 0.00s, liquid kvc latency: 1.23s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.211 GB, free space: 2.668GB, frag space: 0.542GB
INFO 09-05 12:33:50 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:50 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:33:50 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:50 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:33:50 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:33:50 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:33:50 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:33:50 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.211 GB, free space: 2.664GB, frag space: 0.542GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:50 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.432GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:50 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.442GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:50 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.172 GB, free space: 24.014GB, frag space: 0.454GB
It takes: 0.65s to send model shards
After sending shards, there are 8.99GB remaining on GPU0
INFO 09-05 12:33:50 worker.py:152] send weights shards takes: 0.72s, sent out: 6.19GB, sent bw: 8.65GB/s
INFO 09-05 12:33:50 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.883 GB, free space: 8.992GB, frag space: 0.407GB
INFO 09-05 12:33:51 cache_engine.py:146] send kvc shards takes: 0.72s, sent out: 7.38GB, sent bw: 10.26GB/s
INFO 09-05 12:33:51 cache_engine.py:186] After deleting layer's shard, free mem: 9208.31MB
INFO 09-05 12:33:51 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:33:51 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.508 GB, free space: 16.367GB, frag space: 0.411GB
INFO 09-05 12:33:51 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5345
INFO 09-05 12:33:52 worker.py:205] extend gpu in worker takes: 0.48s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:52 worker.py:205] extend gpu in worker takes: 0.48s
INFO 09-05 12:33:52 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.48s, after extending gpu blocks: allocated space on GPU: 27.597 GB, reserved space on GPU: 28.008 GB, free space: 2.867GB, frag space: 0.411GB
do_liquid latency: 2.63s
INFO 09-05 12:33:52 llm_engine.py:758] Finished liquid for 46 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.56s, update worker latency: 0.00s, liquid model weights latency: 0.72s, init mem latency: 0.00s, liquid kvc latency: 0.84s;, current mem info on GPU0: allocated space on GPU: 27.597 GB, reserved space on GPU: 28.008 GB, free space: 2.867GB, frag space: 0.411GB
INFO 09-05 12:33:53 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:33:53 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.508 GB, free space: 16.367GB, frag space: 0.411GB
INFO 09-05 12:33:53 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.508 GB, free space: 16.367GB, frag space: 0.411GB
INFO 09-05 12:33:54 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.389 GB, free space: 10.487GB, frag space: 0.099GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.64s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 22.99GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:54 worker.py:152] send weights shards takes: 0.68s, sent out: 6.19GB, sent bw: 9.12GB/s
INFO 09-05 12:33:54 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.836 GB, free space: 10.039GB, frag space: 0.546GB
INFO 09-05 12:33:54 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.836 GB, free space: 10.039GB, frag space: 0.546GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:55 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.22GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:55 cache_engine.py:186] After deleting layer's shard, free mem: 23546.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:55 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:33:55 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.211 GB, free space: 2.664GB, frag space: 0.542GB
do_liquid latency: 3.26s
INFO 09-05 12:33:55 llm_engine.py:758] Finished liquid for 47 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.99s, update worker latency: 0.00s, liquid model weights latency: 0.75s, init mem latency: 0.00s, liquid kvc latency: 1.24s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.211 GB, free space: 2.664GB, frag space: 0.542GB
INFO 09-05 12:33:56 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:56 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:56 pynccl.py:65] vLLM is using nccl==2.20.5
INFO 09-05 12:33:56 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:33:56 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:33:56 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:33:56 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:33:56 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.211 GB, free space: 2.660GB, frag space: 0.542GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:56 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.428GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:57 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.438GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:57 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.188 GB, free space: 23.994GB, frag space: 0.470GB
It takes: 0.65s to send model shards
After sending shards, there are 8.99GB remaining on GPU0
INFO 09-05 12:33:57 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.74GB/s
INFO 09-05 12:33:57 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.883 GB, free space: 8.989GB, frag space: 0.407GB
INFO 09-05 12:33:57 cache_engine.py:146] send kvc shards takes: 0.72s, sent out: 7.38GB, sent bw: 10.30GB/s
INFO 09-05 12:33:57 cache_engine.py:186] After deleting layer's shard, free mem: 9204.31MB
INFO 09-05 12:33:57 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:33:57 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.508 GB, free space: 16.364GB, frag space: 0.411GB
INFO 09-05 12:33:57 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5344
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:33:58 worker.py:205] extend gpu in worker takes: 0.47s
INFO 09-05 12:33:58 worker.py:205] extend gpu in worker takes: 0.47s
INFO 09-05 12:33:58 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.48s, after extending gpu blocks: allocated space on GPU: 27.593 GB, reserved space on GPU: 27.945 GB, free space: 2.926GB, frag space: 0.353GB
do_liquid latency: 2.61s
INFO 09-05 12:33:58 llm_engine.py:758] Finished liquid for 48 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.54s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.83s;, current mem info on GPU0: allocated space on GPU: 27.593 GB, reserved space on GPU: 27.945 GB, free space: 2.926GB, frag space: 0.353GB
INFO 09-05 12:33:59 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:33:59 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.508 GB, free space: 16.360GB, frag space: 0.411GB
INFO 09-05 12:33:59 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.508 GB, free space: 16.360GB, frag space: 0.411GB
INFO 09-05 12:34:00 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.389 GB, free space: 10.479GB, frag space: 0.099GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.67s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 22.99GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:00 worker.py:152] send weights shards takes: 0.70s, sent out: 6.19GB, sent bw: 8.82GB/s
INFO 09-05 12:34:00 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.836 GB, free space: 10.032GB, frag space: 0.546GB
INFO 09-05 12:34:00 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.836 GB, free space: 10.032GB, frag space: 0.546GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:01 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.21GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:01 cache_engine.py:186] After deleting layer's shard, free mem: 23538.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:01 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:34:01 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.211 GB, free space: 2.659GB, frag space: 0.542GB
do_liquid latency: 3.27s
INFO 09-05 12:34:01 llm_engine.py:758] Finished liquid for 49 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.02s, update worker latency: 0.00s, liquid model weights latency: 0.77s, init mem latency: 0.00s, liquid kvc latency: 1.25s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.211 GB, free space: 2.659GB, frag space: 0.542GB
INFO 09-05 12:34:02 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:02 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:34:02 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:02 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:34:02 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-05 12:34:02 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:34:02 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:34:02 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.211 GB, free space: 2.653GB, frag space: 0.542GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:02 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.420GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:03 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.430GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:03 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.172 GB, free space: 24.002GB, frag space: 0.454GB
It takes: 0.65s to send model shards
After sending shards, there are 8.95GB remaining on GPU0
INFO 09-05 12:34:03 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.68GB/s
INFO 09-05 12:34:03 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.914 GB, free space: 8.950GB, frag space: 0.438GB
INFO 09-05 12:34:03 cache_engine.py:146] send kvc shards takes: 0.72s, sent out: 7.38GB, sent bw: 10.32GB/s
INFO 09-05 12:34:03 cache_engine.py:186] After deleting layer's shard, free mem: 9164.31MB
INFO 09-05 12:34:04 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:34:04 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.539 GB, free space: 16.325GB, frag space: 0.442GB
INFO 09-05 12:34:04 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5334
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:04 worker.py:205] extend gpu in worker takes: 0.45s
INFO 09-05 12:34:04 worker.py:205] extend gpu in worker takes: 0.45s
INFO 09-05 12:34:04 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.45s, after extending gpu blocks: allocated space on GPU: 27.554 GB, reserved space on GPU: 27.977 GB, free space: 2.889GB, frag space: 0.423GB
do_liquid latency: 2.61s
INFO 09-05 12:34:04 llm_engine.py:758] Finished liquid for 50 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.57s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.85s;, current mem info on GPU0: allocated space on GPU: 27.554 GB, reserved space on GPU: 27.977 GB, free space: 2.889GB, frag space: 0.423GB
INFO 09-05 12:34:05 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:34:05 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.539 GB, free space: 16.325GB, frag space: 0.442GB
INFO 09-05 12:34:05 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.539 GB, free space: 16.325GB, frag space: 0.442GB
INFO 09-05 12:34:06 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.389 GB, free space: 10.475GB, frag space: 0.099GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.65s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 22.98GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:06 worker.py:152] send weights shards takes: 0.69s, sent out: 6.19GB, sent bw: 9.00GB/s
INFO 09-05 12:34:06 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.805 GB, free space: 10.059GB, frag space: 0.514GB
INFO 09-05 12:34:06 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.805 GB, free space: 10.059GB, frag space: 0.514GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:07 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.22GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:07 cache_engine.py:186] After deleting layer's shard, free mem: 23534.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:07 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:34:07 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.180 GB, free space: 2.686GB, frag space: 0.510GB
do_liquid latency: 3.26s
INFO 09-05 12:34:07 llm_engine.py:758] Finished liquid for 51 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.99s, update worker latency: 0.00s, liquid model weights latency: 0.75s, init mem latency: 0.00s, liquid kvc latency: 1.24s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.180 GB, free space: 2.686GB, frag space: 0.510GB
INFO 09-05 12:34:08 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:34:08 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:08 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:08 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:34:08 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:34:08 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:34:08 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:34:08 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.180 GB, free space: 2.680GB, frag space: 0.510GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:08 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.416GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:09 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.426GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:09 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.156 GB, free space: 24.014GB, frag space: 0.438GB
It takes: 0.65s to send model shards
After sending shards, there are 8.76GB remaining on GPU0
INFO 09-05 12:34:09 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.74GB/s
INFO 09-05 12:34:09 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 22.102 GB, free space: 8.758GB, frag space: 0.626GB
INFO 09-05 12:34:09 cache_engine.py:146] send kvc shards takes: 0.71s, sent out: 7.38GB, sent bw: 10.32GB/s
INFO 09-05 12:34:09 cache_engine.py:186] After deleting layer's shard, free mem: 8968.31MB
INFO 09-05 12:34:10 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:34:10 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.727 GB, free space: 16.133GB, frag space: 0.630GB
INFO 09-05 12:34:10 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5285
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:10 worker.py:205] extend gpu in worker takes: 0.48s
INFO 09-05 12:34:10 worker.py:205] extend gpu in worker takes: 0.48s
INFO 09-05 12:34:10 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.48s, after extending gpu blocks: allocated space on GPU: 27.362 GB, reserved space on GPU: 27.977 GB, free space: 2.883GB, frag space: 0.614GB
do_liquid latency: 2.61s
INFO 09-05 12:34:10 llm_engine.py:758] Finished liquid for 52 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.53s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.82s;, current mem info on GPU0: allocated space on GPU: 27.362 GB, reserved space on GPU: 27.977 GB, free space: 2.883GB, frag space: 0.614GB
INFO 09-05 12:34:11 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:34:11 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.727 GB, free space: 16.133GB, frag space: 0.630GB
INFO 09-05 12:34:11 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.727 GB, free space: 16.133GB, frag space: 0.630GB
INFO 09-05 12:34:12 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.389 GB, free space: 10.471GB, frag space: 0.099GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.66s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 22.98GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:12 worker.py:152] send weights shards takes: 0.69s, sent out: 6.19GB, sent bw: 8.93GB/s
INFO 09-05 12:34:12 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.867 GB, free space: 9.992GB, frag space: 0.577GB
INFO 09-05 12:34:12 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.867 GB, free space: 9.992GB, frag space: 0.577GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:13 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.22GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:13 cache_engine.py:186] After deleting layer's shard, free mem: 23530.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:13 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:34:13 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.242 GB, free space: 2.617GB, frag space: 0.573GB
do_liquid latency: 3.23s
INFO 09-05 12:34:13 llm_engine.py:758] Finished liquid for 53 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.98s, update worker latency: 0.00s, liquid model weights latency: 0.76s, init mem latency: 0.00s, liquid kvc latency: 1.22s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.242 GB, free space: 2.617GB, frag space: 0.573GB
INFO 09-05 12:34:14 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:34:14 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:14 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:14 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:34:14 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-05 12:34:14 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:34:14 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:34:14 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.242 GB, free space: 2.614GB, frag space: 0.573GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:14 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.412GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:15 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.422GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:15 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.156 GB, free space: 24.010GB, frag space: 0.438GB
It takes: 0.65s to send model shards
After sending shards, there are 8.82GB remaining on GPU0
INFO 09-05 12:34:15 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.76GB/s
INFO 09-05 12:34:15 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 22.039 GB, free space: 8.817GB, frag space: 0.563GB
INFO 09-05 12:34:16 cache_engine.py:146] send kvc shards takes: 0.72s, sent out: 7.38GB, sent bw: 10.30GB/s
INFO 09-05 12:34:16 cache_engine.py:186] After deleting layer's shard, free mem: 9028.31MB
INFO 09-05 12:34:16 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:34:16 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.664 GB, free space: 16.192GB, frag space: 0.567GB
INFO 09-05 12:34:16 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5300
INFO 09-05 12:34:16 worker.py:205] extend gpu in worker takes: 0.50s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:16 worker.py:205] extend gpu in worker takes: 0.50s
INFO 09-05 12:34:16 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.50s, after extending gpu blocks: allocated space on GPU: 27.421 GB, reserved space on GPU: 27.977 GB, free space: 2.879GB, frag space: 0.556GB
do_liquid latency: 2.62s
INFO 09-05 12:34:16 llm_engine.py:758] Finished liquid for 54 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.53s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.82s;, current mem info on GPU0: allocated space on GPU: 27.421 GB, reserved space on GPU: 27.977 GB, free space: 2.879GB, frag space: 0.556GB
INFO 09-05 12:34:17 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:34:17 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.664 GB, free space: 16.192GB, frag space: 0.567GB
INFO 09-05 12:34:17 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.664 GB, free space: 16.192GB, frag space: 0.567GB
INFO 09-05 12:34:18 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.389 GB, free space: 10.467GB, frag space: 0.099GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.67s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 22.97GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:18 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.72GB/s
INFO 09-05 12:34:18 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.867 GB, free space: 9.989GB, frag space: 0.577GB
INFO 09-05 12:34:18 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.867 GB, free space: 9.989GB, frag space: 0.577GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:19 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.20GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:19 cache_engine.py:186] After deleting layer's shard, free mem: 23526.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:19 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:34:20 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.242 GB, free space: 2.614GB, frag space: 0.573GB
do_liquid latency: 3.27s
INFO 09-05 12:34:20 llm_engine.py:758] Finished liquid for 55 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.03s, update worker latency: 0.00s, liquid model weights latency: 0.77s, init mem latency: 0.00s, liquid kvc latency: 1.25s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.242 GB, free space: 2.614GB, frag space: 0.573GB
INFO 09-05 12:34:20 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:20 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:34:20 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:20 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:34:20 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:34:20 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:34:20 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:34:20 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.242 GB, free space: 2.608GB, frag space: 0.573GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:20 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.409GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:21 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.418GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:21 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.188 GB, free space: 23.975GB, frag space: 0.470GB
It takes: 0.64s to send model shards
After sending shards, there are 8.81GB remaining on GPU0
INFO 09-05 12:34:21 worker.py:152] send weights shards takes: 0.70s, sent out: 6.19GB, sent bw: 8.80GB/s
INFO 09-05 12:34:21 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 22.039 GB, free space: 8.811GB, frag space: 0.563GB
INFO 09-05 12:34:22 cache_engine.py:146] send kvc shards takes: 0.71s, sent out: 7.38GB, sent bw: 10.35GB/s
INFO 09-05 12:34:22 cache_engine.py:186] After deleting layer's shard, free mem: 9022.31MB
INFO 09-05 12:34:22 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:34:22 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.664 GB, free space: 16.186GB, frag space: 0.567GB
INFO 09-05 12:34:22 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5298
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:22 worker.py:205] extend gpu in worker takes: 0.49s
INFO 09-05 12:34:22 worker.py:205] extend gpu in worker takes: 0.49s
INFO 09-05 12:34:22 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.49s, after extending gpu blocks: allocated space on GPU: 27.413 GB, reserved space on GPU: 27.977 GB, free space: 2.873GB, frag space: 0.563GB
do_liquid latency: 2.60s
INFO 09-05 12:34:22 llm_engine.py:758] Finished liquid for 56 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.52s, update worker latency: 0.00s, liquid model weights latency: 0.70s, init mem latency: 0.00s, liquid kvc latency: 0.81s;, current mem info on GPU0: allocated space on GPU: 27.413 GB, reserved space on GPU: 27.977 GB, free space: 2.873GB, frag space: 0.563GB
INFO 09-05 12:34:24 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:34:24 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.664 GB, free space: 16.186GB, frag space: 0.567GB
INFO 09-05 12:34:24 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.664 GB, free space: 16.186GB, frag space: 0.567GB
INFO 09-05 12:34:24 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.389 GB, free space: 10.461GB, frag space: 0.099GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.64s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 22.97GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:24 worker.py:152] send weights shards takes: 0.67s, sent out: 6.19GB, sent bw: 9.19GB/s
INFO 09-05 12:34:24 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.867 GB, free space: 9.983GB, frag space: 0.577GB
INFO 09-05 12:34:24 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.867 GB, free space: 9.983GB, frag space: 0.577GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:25 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.19GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:25 cache_engine.py:186] After deleting layer's shard, free mem: 23522.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:25 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:34:26 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.242 GB, free space: 2.608GB, frag space: 0.573GB
do_liquid latency: 3.25s
INFO 09-05 12:34:26 llm_engine.py:758] Finished liquid for 57 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.98s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 1.24s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.242 GB, free space: 2.608GB, frag space: 0.573GB
INFO 09-05 12:34:26 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:26 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:34:26 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:26 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:34:26 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:34:26 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:34:26 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:34:26 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.242 GB, free space: 2.604GB, frag space: 0.573GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:26 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.405GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:27 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.414GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:27 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.188 GB, free space: 23.971GB, frag space: 0.470GB
It takes: 0.65s to send model shards
After sending shards, there are 8.78GB remaining on GPU0
INFO 09-05 12:34:27 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.77GB/s
INFO 09-05 12:34:27 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 22.070 GB, free space: 8.776GB, frag space: 0.595GB
INFO 09-05 12:34:28 cache_engine.py:146] send kvc shards takes: 0.71s, sent out: 7.38GB, sent bw: 10.36GB/s
INFO 09-05 12:34:28 cache_engine.py:186] After deleting layer's shard, free mem: 8986.31MB
INFO 09-05 12:34:28 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:34:28 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.695 GB, free space: 16.151GB, frag space: 0.599GB
INFO 09-05 12:34:28 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5289
INFO 09-05 12:34:28 worker.py:205] extend gpu in worker takes: 0.49s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:28 worker.py:205] extend gpu in worker takes: 0.50s
INFO 09-05 12:34:28 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.50s, after extending gpu blocks: allocated space on GPU: 27.405 GB, reserved space on GPU: 27.945 GB, free space: 2.901GB, frag space: 0.540GB
do_liquid latency: 2.61s
INFO 09-05 12:34:28 llm_engine.py:758] Finished liquid for 58 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.52s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.81s;, current mem info on GPU0: allocated space on GPU: 27.405 GB, reserved space on GPU: 27.945 GB, free space: 2.901GB, frag space: 0.540GB
INFO 09-05 12:34:30 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:34:30 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.695 GB, free space: 16.151GB, frag space: 0.599GB
INFO 09-05 12:34:30 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.695 GB, free space: 16.151GB, frag space: 0.599GB
INFO 09-05 12:34:30 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.389 GB, free space: 10.457GB, frag space: 0.099GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.64s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 22.97GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:30 worker.py:152] send weights shards takes: 0.67s, sent out: 6.19GB, sent bw: 9.22GB/s
INFO 09-05 12:34:31 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.805 GB, free space: 10.041GB, frag space: 0.514GB
INFO 09-05 12:34:31 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.805 GB, free space: 10.041GB, frag space: 0.514GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:31 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.17GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:31 cache_engine.py:186] After deleting layer's shard, free mem: 23518.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:31 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:34:32 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.180 GB, free space: 2.666GB, frag space: 0.510GB
do_liquid latency: 3.22s
INFO 09-05 12:34:32 llm_engine.py:758] Finished liquid for 59 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.94s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 1.21s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.180 GB, free space: 2.666GB, frag space: 0.510GB
INFO 09-05 12:34:32 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:32 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:34:32 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:32 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:34:32 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-05 12:34:32 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:34:32 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:34:32 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.180 GB, free space: 2.662GB, frag space: 0.510GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:32 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.401GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:33 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.410GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:33 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.172 GB, free space: 23.983GB, frag space: 0.454GB
It takes: 0.64s to send model shards
After sending shards, there are 8.68GB remaining on GPU0
INFO 09-05 12:34:33 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.77GB/s
INFO 09-05 12:34:33 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 22.164 GB, free space: 8.678GB, frag space: 0.688GB
INFO 09-05 12:34:34 cache_engine.py:146] send kvc shards takes: 0.71s, sent out: 7.38GB, sent bw: 10.35GB/s
INFO 09-05 12:34:34 cache_engine.py:186] After deleting layer's shard, free mem: 8886.31MB
INFO 09-05 12:34:34 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:34:34 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.789 GB, free space: 16.053GB, frag space: 0.692GB
INFO 09-05 12:34:34 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5264
INFO 09-05 12:34:34 worker.py:205] extend gpu in worker takes: 0.47s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:34 worker.py:205] extend gpu in worker takes: 0.47s
INFO 09-05 12:34:34 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.47s, after extending gpu blocks: allocated space on GPU: 27.280 GB, reserved space on GPU: 27.914 GB, free space: 2.928GB, frag space: 0.634GB
do_liquid latency: 2.60s
INFO 09-05 12:34:34 llm_engine.py:758] Finished liquid for 60 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.54s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.83s;, current mem info on GPU0: allocated space on GPU: 27.280 GB, reserved space on GPU: 27.914 GB, free space: 2.928GB, frag space: 0.634GB
INFO 09-05 12:34:36 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:34:36 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.789 GB, free space: 16.053GB, frag space: 0.692GB
INFO 09-05 12:34:36 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.789 GB, free space: 16.053GB, frag space: 0.692GB
INFO 09-05 12:34:37 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.389 GB, free space: 10.453GB, frag space: 0.099GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.64s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 22.96GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:37 worker.py:152] send weights shards takes: 0.68s, sent out: 6.19GB, sent bw: 9.14GB/s
INFO 09-05 12:34:37 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.773 GB, free space: 10.069GB, frag space: 0.483GB
INFO 09-05 12:34:37 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.773 GB, free space: 10.069GB, frag space: 0.483GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:37 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.23GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:37 cache_engine.py:186] After deleting layer's shard, free mem: 23514.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:37 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:34:38 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.148 GB, free space: 2.694GB, frag space: 0.479GB
do_liquid latency: 3.34s
INFO 09-05 12:34:38 llm_engine.py:758] Finished liquid for 61 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.97s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 1.22s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.148 GB, free space: 2.694GB, frag space: 0.479GB
INFO 09-05 12:34:38 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:38 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:34:38 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:38 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:34:39 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:34:39 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:34:39 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:34:39 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.148 GB, free space: 2.690GB, frag space: 0.479GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:39 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.397GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:39 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.407GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:39 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.156 GB, free space: 23.994GB, frag space: 0.438GB
It takes: 0.65s to send model shards
After sending shards, there are 8.80GB remaining on GPU0
INFO 09-05 12:34:39 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.73GB/s
INFO 09-05 12:34:39 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 22.039 GB, free space: 8.799GB, frag space: 0.563GB
INFO 09-05 12:34:40 cache_engine.py:146] send kvc shards takes: 0.72s, sent out: 7.38GB, sent bw: 10.32GB/s
INFO 09-05 12:34:40 cache_engine.py:186] After deleting layer's shard, free mem: 9010.31MB
INFO 09-05 12:34:40 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:34:40 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.664 GB, free space: 16.174GB, frag space: 0.567GB
INFO 09-05 12:34:40 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5295
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:41 worker.py:205] extend gpu in worker takes: 0.49s
INFO 09-05 12:34:41 worker.py:205] extend gpu in worker takes: 0.49s
INFO 09-05 12:34:41 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.49s, after extending gpu blocks: allocated space on GPU: 27.405 GB, reserved space on GPU: 27.914 GB, free space: 2.924GB, frag space: 0.509GB
do_liquid latency: 2.61s
INFO 09-05 12:34:41 llm_engine.py:758] Finished liquid for 62 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.53s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.82s;, current mem info on GPU0: allocated space on GPU: 27.405 GB, reserved space on GPU: 27.914 GB, free space: 2.924GB, frag space: 0.509GB
INFO 09-05 12:34:42 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:34:42 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.664 GB, free space: 16.174GB, frag space: 0.567GB
INFO 09-05 12:34:42 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.664 GB, free space: 16.174GB, frag space: 0.567GB
INFO 09-05 12:34:43 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.389 GB, free space: 10.450GB, frag space: 0.099GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.63s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 22.96GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:43 worker.py:152] send weights shards takes: 0.67s, sent out: 6.19GB, sent bw: 9.22GB/s
INFO 09-05 12:34:43 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.773 GB, free space: 10.065GB, frag space: 0.483GB
INFO 09-05 12:34:43 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.773 GB, free space: 10.065GB, frag space: 0.483GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:43 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.22GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:43 cache_engine.py:186] After deleting layer's shard, free mem: 23510.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:44 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:34:44 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.148 GB, free space: 2.690GB, frag space: 0.479GB
do_liquid latency: 3.23s
INFO 09-05 12:34:44 llm_engine.py:758] Finished liquid for 63 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.97s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 1.23s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.148 GB, free space: 2.690GB, frag space: 0.479GB
INFO 09-05 12:34:45 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:34:45 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:45 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:45 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:34:45 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-05 12:34:45 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:34:45 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:34:45 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.148 GB, free space: 2.686GB, frag space: 0.479GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:45 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.393GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:45 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.403GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:45 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.156 GB, free space: 23.991GB, frag space: 0.438GB
It takes: 0.64s to send model shards
After sending shards, there are 8.93GB remaining on GPU0
INFO 09-05 12:34:45 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.78GB/s
INFO 09-05 12:34:45 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.908 GB, free space: 8.926GB, frag space: 0.433GB
INFO 09-05 12:34:46 cache_engine.py:146] send kvc shards takes: 0.72s, sent out: 7.38GB, sent bw: 10.27GB/s
INFO 09-05 12:34:46 cache_engine.py:186] After deleting layer's shard, free mem: 9140.31MB
INFO 09-05 12:34:46 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:34:46 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.533 GB, free space: 16.301GB, frag space: 0.436GB
INFO 09-05 12:34:46 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5328
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:47 worker.py:205] extend gpu in worker takes: 0.48s
INFO 09-05 12:34:47 worker.py:205] extend gpu in worker takes: 0.48s
INFO 09-05 12:34:47 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.48s, after extending gpu blocks: allocated space on GPU: 27.530 GB, reserved space on GPU: 27.908 GB, free space: 2.926GB, frag space: 0.378GB
do_liquid latency: 2.62s
INFO 09-05 12:34:47 llm_engine.py:758] Finished liquid for 64 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.54s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.83s;, current mem info on GPU0: allocated space on GPU: 27.530 GB, reserved space on GPU: 27.908 GB, free space: 2.926GB, frag space: 0.378GB
INFO 09-05 12:34:48 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:34:48 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.533 GB, free space: 16.299GB, frag space: 0.436GB
INFO 09-05 12:34:48 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.533 GB, free space: 16.299GB, frag space: 0.436GB
INFO 09-05 12:34:49 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.383 GB, free space: 10.450GB, frag space: 0.093GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.65s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 22.95GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:49 worker.py:152] send weights shards takes: 0.69s, sent out: 6.19GB, sent bw: 8.96GB/s
INFO 09-05 12:34:49 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.768 GB, free space: 10.065GB, frag space: 0.477GB
INFO 09-05 12:34:49 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.768 GB, free space: 10.065GB, frag space: 0.477GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:50 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.23GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:50 cache_engine.py:186] After deleting layer's shard, free mem: 23504.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:50 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:34:50 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.143 GB, free space: 2.692GB, frag space: 0.473GB
do_liquid latency: 3.24s
INFO 09-05 12:34:50 llm_engine.py:758] Finished liquid for 65 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.98s, update worker latency: 0.00s, liquid model weights latency: 0.76s, init mem latency: 0.00s, liquid kvc latency: 1.22s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.143 GB, free space: 2.692GB, frag space: 0.473GB
INFO 09-05 12:34:51 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:34:51 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:51 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:51 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:34:51 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-05 12:34:51 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:34:51 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:34:51 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.143 GB, free space: 2.686GB, frag space: 0.473GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:51 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.387GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:51 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.397GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:51 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.172 GB, free space: 23.969GB, frag space: 0.454GB
It takes: 0.65s to send model shards
After sending shards, there are 8.79GB remaining on GPU0
INFO 09-05 12:34:51 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.68GB/s
INFO 09-05 12:34:51 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 22.039 GB, free space: 8.789GB, frag space: 0.563GB
INFO 09-05 12:34:52 cache_engine.py:146] send kvc shards takes: 0.71s, sent out: 7.38GB, sent bw: 10.33GB/s
INFO 09-05 12:34:52 cache_engine.py:186] After deleting layer's shard, free mem: 9000.31MB
INFO 09-05 12:34:52 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:34:52 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.664 GB, free space: 16.164GB, frag space: 0.567GB
INFO 09-05 12:34:52 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5293
INFO 09-05 12:34:53 worker.py:205] extend gpu in worker takes: 0.48s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:53 worker.py:205] extend gpu in worker takes: 0.48s
INFO 09-05 12:34:53 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.48s, after extending gpu blocks: allocated space on GPU: 27.405 GB, reserved space on GPU: 27.914 GB, free space: 2.916GB, frag space: 0.509GB
do_liquid latency: 2.62s
INFO 09-05 12:34:53 llm_engine.py:758] Finished liquid for 66 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.54s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.83s;, current mem info on GPU0: allocated space on GPU: 27.405 GB, reserved space on GPU: 27.914 GB, free space: 2.916GB, frag space: 0.509GB
INFO 09-05 12:34:54 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:34:54 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.664 GB, free space: 16.164GB, frag space: 0.567GB
INFO 09-05 12:34:54 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.664 GB, free space: 16.164GB, frag space: 0.567GB
INFO 09-05 12:34:55 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.389 GB, free space: 10.440GB, frag space: 0.099GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.63s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 22.95GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:55 worker.py:152] send weights shards takes: 0.68s, sent out: 6.19GB, sent bw: 9.17GB/s
INFO 09-05 12:34:55 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.805 GB, free space: 10.024GB, frag space: 0.514GB
INFO 09-05 12:34:55 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.805 GB, free space: 10.024GB, frag space: 0.514GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:56 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.22GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:56 cache_engine.py:186] After deleting layer's shard, free mem: 23500.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:56 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:34:56 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.180 GB, free space: 2.651GB, frag space: 0.510GB
do_liquid latency: 3.24s
INFO 09-05 12:34:56 llm_engine.py:758] Finished liquid for 67 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.97s, update worker latency: 0.00s, liquid model weights latency: 0.75s, init mem latency: 0.00s, liquid kvc latency: 1.22s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.180 GB, free space: 2.651GB, frag space: 0.510GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:57 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:57 pynccl.py:65] vLLM is using nccl==2.20.5
INFO 09-05 12:34:57 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:34:57 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:34:57 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:34:57 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:34:57 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:34:57 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.180 GB, free space: 2.645GB, frag space: 0.510GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:57 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.383GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:57 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.393GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:57 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.172 GB, free space: 23.965GB, frag space: 0.454GB
It takes: 0.64s to send model shards
After sending shards, there are 8.69GB remaining on GPU0
INFO 09-05 12:34:57 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.77GB/s
INFO 09-05 12:34:57 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 22.133 GB, free space: 8.692GB, frag space: 0.657GB
INFO 09-05 12:34:58 cache_engine.py:146] send kvc shards takes: 0.71s, sent out: 7.38GB, sent bw: 10.32GB/s
INFO 09-05 12:34:58 cache_engine.py:186] After deleting layer's shard, free mem: 8900.31MB
INFO 09-05 12:34:58 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:34:58 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.758 GB, free space: 16.067GB, frag space: 0.661GB
INFO 09-05 12:34:58 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5268
INFO 09-05 12:34:59 worker.py:205] extend gpu in worker takes: 0.47s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:34:59 worker.py:205] extend gpu in worker takes: 0.47s
INFO 09-05 12:34:59 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.47s, after extending gpu blocks: allocated space on GPU: 27.296 GB, reserved space on GPU: 27.945 GB, free space: 2.879GB, frag space: 0.649GB
do_liquid latency: 2.62s
INFO 09-05 12:34:59 llm_engine.py:758] Finished liquid for 68 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.54s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.84s;, current mem info on GPU0: allocated space on GPU: 27.296 GB, reserved space on GPU: 27.945 GB, free space: 2.879GB, frag space: 0.649GB
INFO 09-05 12:35:00 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:35:00 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.758 GB, free space: 16.067GB, frag space: 0.661GB
INFO 09-05 12:35:00 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.758 GB, free space: 16.067GB, frag space: 0.661GB
INFO 09-05 12:35:01 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.389 GB, free space: 10.436GB, frag space: 0.099GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.63s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 22.95GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:01 worker.py:152] send weights shards takes: 0.67s, sent out: 6.19GB, sent bw: 9.22GB/s
INFO 09-05 12:35:01 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.898 GB, free space: 9.926GB, frag space: 0.608GB
INFO 09-05 12:35:01 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.898 GB, free space: 9.926GB, frag space: 0.608GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:02 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.25GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:02 cache_engine.py:186] After deleting layer's shard, free mem: 23496.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:02 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:35:02 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.273 GB, free space: 2.551GB, frag space: 0.604GB
do_liquid latency: 3.25s
INFO 09-05 12:35:02 llm_engine.py:758] Finished liquid for 69 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.97s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 1.23s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.273 GB, free space: 2.551GB, frag space: 0.604GB
INFO 09-05 12:35:03 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:03 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:35:03 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:03 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:35:03 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:35:03 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:35:03 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:35:03 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.273 GB, free space: 2.545GB, frag space: 0.604GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:03 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.379GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:03 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.389GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:04 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.172 GB, free space: 23.961GB, frag space: 0.454GB
It takes: 0.65s to send model shards
After sending shards, there are 8.90GB remaining on GPU0
INFO 09-05 12:35:04 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.75GB/s
INFO 09-05 12:35:04 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.914 GB, free space: 8.905GB, frag space: 0.438GB
INFO 09-05 12:35:04 cache_engine.py:146] send kvc shards takes: 0.73s, sent out: 7.38GB, sent bw: 10.17GB/s
INFO 09-05 12:35:04 cache_engine.py:186] After deleting layer's shard, free mem: 9118.31MB
INFO 09-05 12:35:04 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:35:04 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.539 GB, free space: 16.280GB, frag space: 0.442GB
INFO 09-05 12:35:04 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5322
INFO 09-05 12:35:05 worker.py:205] extend gpu in worker takes: 0.48s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:05 worker.py:205] extend gpu in worker takes: 0.48s
INFO 09-05 12:35:05 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.48s, after extending gpu blocks: allocated space on GPU: 27.530 GB, reserved space on GPU: 27.914 GB, free space: 2.905GB, frag space: 0.384GB
do_liquid latency: 2.64s
INFO 09-05 12:35:05 llm_engine.py:758] Finished liquid for 70 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.56s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.85s;, current mem info on GPU0: allocated space on GPU: 27.530 GB, reserved space on GPU: 27.914 GB, free space: 2.905GB, frag space: 0.384GB
INFO 09-05 12:35:06 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:35:06 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.539 GB, free space: 16.280GB, frag space: 0.442GB
INFO 09-05 12:35:06 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.539 GB, free space: 16.280GB, frag space: 0.442GB
INFO 09-05 12:35:07 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.389 GB, free space: 10.430GB, frag space: 0.099GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.66s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 22.94GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:07 worker.py:152] send weights shards takes: 0.70s, sent out: 6.19GB, sent bw: 8.86GB/s
INFO 09-05 12:35:07 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.805 GB, free space: 10.014GB, frag space: 0.514GB
INFO 09-05 12:35:07 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.805 GB, free space: 10.014GB, frag space: 0.514GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:08 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.19GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:08 cache_engine.py:186] After deleting layer's shard, free mem: 23490.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:08 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:35:08 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.180 GB, free space: 2.639GB, frag space: 0.510GB
do_liquid latency: 3.25s
INFO 09-05 12:35:08 llm_engine.py:758] Finished liquid for 71 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.00s, update worker latency: 0.00s, liquid model weights latency: 0.77s, init mem latency: 0.00s, liquid kvc latency: 1.23s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.180 GB, free space: 2.639GB, frag space: 0.510GB
INFO 09-05 12:35:09 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:09 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:35:09 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:09 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:35:09 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:35:09 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:35:09 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:35:09 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.180 GB, free space: 2.635GB, frag space: 0.510GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:09 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.373GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:10 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.383GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:10 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.172 GB, free space: 23.955GB, frag space: 0.454GB
It takes: 0.65s to send model shards
After sending shards, there are 8.96GB remaining on GPU0
INFO 09-05 12:35:10 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.70GB/s
INFO 09-05 12:35:10 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.852 GB, free space: 8.963GB, frag space: 0.376GB
INFO 09-05 12:35:10 cache_engine.py:146] send kvc shards takes: 0.71s, sent out: 7.38GB, sent bw: 10.37GB/s
INFO 09-05 12:35:10 cache_engine.py:186] After deleting layer's shard, free mem: 9178.31MB
INFO 09-05 12:35:10 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:35:10 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.477 GB, free space: 16.338GB, frag space: 0.380GB
INFO 09-05 12:35:10 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5337
INFO 09-05 12:35:11 worker.py:205] extend gpu in worker takes: 0.47s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:11 worker.py:205] extend gpu in worker takes: 0.47s
INFO 09-05 12:35:11 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.47s, after extending gpu blocks: allocated space on GPU: 27.593 GB, reserved space on GPU: 27.914 GB, free space: 2.901GB, frag space: 0.321GB
do_liquid latency: 2.62s
INFO 09-05 12:35:11 llm_engine.py:758] Finished liquid for 72 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.56s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.84s;, current mem info on GPU0: allocated space on GPU: 27.593 GB, reserved space on GPU: 27.914 GB, free space: 2.901GB, frag space: 0.321GB
INFO 09-05 12:35:12 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:35:12 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.477 GB, free space: 16.338GB, frag space: 0.380GB
INFO 09-05 12:35:12 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.477 GB, free space: 16.338GB, frag space: 0.380GB
INFO 09-05 12:35:13 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.389 GB, free space: 10.426GB, frag space: 0.099GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.66s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 22.94GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:13 worker.py:152] send weights shards takes: 0.70s, sent out: 6.19GB, sent bw: 8.82GB/s
INFO 09-05 12:35:13 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.867 GB, free space: 9.948GB, frag space: 0.577GB
INFO 09-05 12:35:13 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.867 GB, free space: 9.948GB, frag space: 0.577GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:14 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.20GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:14 cache_engine.py:186] After deleting layer's shard, free mem: 23486.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:14 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:35:14 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.242 GB, free space: 2.573GB, frag space: 0.573GB
do_liquid latency: 3.23s
INFO 09-05 12:35:14 llm_engine.py:758] Finished liquid for 73 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.98s, update worker latency: 0.00s, liquid model weights latency: 0.77s, init mem latency: 0.00s, liquid kvc latency: 1.21s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.242 GB, free space: 2.573GB, frag space: 0.573GB
INFO 09-05 12:35:15 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:35:15 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:15 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:15 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:35:15 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:35:15 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:35:15 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:35:15 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.242 GB, free space: 2.569GB, frag space: 0.573GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:15 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.369GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:16 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.379GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:16 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.172 GB, free space: 23.951GB, frag space: 0.454GB
It takes: 0.65s to send model shards
After sending shards, there are 8.87GB remaining on GPU0
INFO 09-05 12:35:16 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.77GB/s
INFO 09-05 12:35:16 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.945 GB, free space: 8.866GB, frag space: 0.470GB
INFO 09-05 12:35:16 cache_engine.py:146] send kvc shards takes: 0.71s, sent out: 7.38GB, sent bw: 10.36GB/s
INFO 09-05 12:35:16 cache_engine.py:186] After deleting layer's shard, free mem: 9078.31MB
INFO 09-05 12:35:17 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:35:17 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.570 GB, free space: 16.241GB, frag space: 0.474GB
INFO 09-05 12:35:17 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5312
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:17 worker.py:205] extend gpu in worker takes: 0.47s
INFO 09-05 12:35:17 worker.py:205] extend gpu in worker takes: 0.47s
INFO 09-05 12:35:17 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.47s, after extending gpu blocks: allocated space on GPU: 27.468 GB, reserved space on GPU: 27.883 GB, free space: 2.928GB, frag space: 0.415GB
do_liquid latency: 2.61s
INFO 09-05 12:35:17 llm_engine.py:758] Finished liquid for 74 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.55s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.84s;, current mem info on GPU0: allocated space on GPU: 27.468 GB, reserved space on GPU: 27.883 GB, free space: 2.928GB, frag space: 0.415GB
INFO 09-05 12:35:18 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:35:18 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.570 GB, free space: 16.241GB, frag space: 0.474GB
INFO 09-05 12:35:18 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.570 GB, free space: 16.241GB, frag space: 0.474GB
INFO 09-05 12:35:19 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.389 GB, free space: 10.422GB, frag space: 0.099GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.66s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 22.93GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:19 worker.py:152] send weights shards takes: 0.69s, sent out: 6.19GB, sent bw: 8.93GB/s
INFO 09-05 12:35:19 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.836 GB, free space: 9.975GB, frag space: 0.546GB
INFO 09-05 12:35:19 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.836 GB, free space: 9.975GB, frag space: 0.546GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:20 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.17GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:20 cache_engine.py:186] After deleting layer's shard, free mem: 23482.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:20 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:35:20 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.211 GB, free space: 2.600GB, frag space: 0.542GB
do_liquid latency: 3.25s
INFO 09-05 12:35:20 llm_engine.py:758] Finished liquid for 75 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.98s, update worker latency: 0.00s, liquid model weights latency: 0.76s, init mem latency: 0.00s, liquid kvc latency: 1.22s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.211 GB, free space: 2.600GB, frag space: 0.542GB
INFO 09-05 12:35:21 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:35:21 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:21 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:21 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:35:21 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:35:21 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:35:21 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:35:21 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.211 GB, free space: 2.596GB, frag space: 0.542GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:21 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.366GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:22 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.375GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:22 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.172 GB, free space: 23.948GB, frag space: 0.454GB
It takes: 0.64s to send model shards
After sending shards, there are 8.67GB remaining on GPU0
INFO 09-05 12:35:22 worker.py:152] send weights shards takes: 0.70s, sent out: 6.19GB, sent bw: 8.80GB/s
INFO 09-05 12:35:22 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 22.139 GB, free space: 8.668GB, frag space: 0.663GB
INFO 09-05 12:35:22 cache_engine.py:146] send kvc shards takes: 0.71s, sent out: 7.38GB, sent bw: 10.41GB/s
INFO 09-05 12:35:22 cache_engine.py:186] After deleting layer's shard, free mem: 8876.31MB
INFO 09-05 12:35:23 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:35:23 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.764 GB, free space: 16.043GB, frag space: 0.667GB
INFO 09-05 12:35:23 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5262
INFO 09-05 12:35:23 worker.py:205] extend gpu in worker takes: 0.46s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:23 worker.py:205] extend gpu in worker takes: 0.47s
INFO 09-05 12:35:23 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.47s, after extending gpu blocks: allocated space on GPU: 27.280 GB, reserved space on GPU: 27.889 GB, free space: 2.918GB, frag space: 0.608GB
do_liquid latency: 2.60s
INFO 09-05 12:35:23 llm_engine.py:758] Finished liquid for 76 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.54s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.83s;, current mem info on GPU0: allocated space on GPU: 27.280 GB, reserved space on GPU: 27.889 GB, free space: 2.918GB, frag space: 0.608GB
INFO 09-05 12:35:24 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:35:24 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.764 GB, free space: 16.043GB, frag space: 0.667GB
INFO 09-05 12:35:24 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.764 GB, free space: 16.043GB, frag space: 0.667GB
INFO 09-05 12:35:25 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.395 GB, free space: 10.412GB, frag space: 0.105GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.63s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 22.93GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:25 worker.py:152] send weights shards takes: 0.67s, sent out: 6.19GB, sent bw: 9.23GB/s
INFO 09-05 12:35:25 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.936 GB, free space: 9.871GB, frag space: 0.645GB
INFO 09-05 12:35:25 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.936 GB, free space: 9.871GB, frag space: 0.645GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:26 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.19GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:26 cache_engine.py:186] After deleting layer's shard, free mem: 23478.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:26 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:35:26 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.311 GB, free space: 2.496GB, frag space: 0.641GB
do_liquid latency: 3.22s
INFO 09-05 12:35:26 llm_engine.py:758] Finished liquid for 77 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.96s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 1.22s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.311 GB, free space: 2.496GB, frag space: 0.641GB
INFO 09-05 12:35:27 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:35:27 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:27 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:27 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:35:27 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:35:27 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:35:27 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:35:27 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.311 GB, free space: 2.492GB, frag space: 0.641GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:27 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.362GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:28 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.371GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:28 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.172 GB, free space: 23.944GB, frag space: 0.454GB
It takes: 0.64s to send model shards
After sending shards, there are 8.82GB remaining on GPU0
INFO 09-05 12:35:28 worker.py:152] send weights shards takes: 0.70s, sent out: 6.19GB, sent bw: 8.81GB/s
INFO 09-05 12:35:28 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.982 GB, free space: 8.821GB, frag space: 0.507GB
INFO 09-05 12:35:28 cache_engine.py:146] send kvc shards takes: 0.71s, sent out: 7.38GB, sent bw: 10.35GB/s
INFO 09-05 12:35:28 cache_engine.py:186] After deleting layer's shard, free mem: 9032.31MB
INFO 09-05 12:35:29 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:35:29 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.607 GB, free space: 16.196GB, frag space: 0.511GB
INFO 09-05 12:35:29 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5301
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:29 worker.py:205] extend gpu in worker takes: 0.46s
INFO 09-05 12:35:29 worker.py:205] extend gpu in worker takes: 0.46s
INFO 09-05 12:35:29 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.46s, after extending gpu blocks: allocated space on GPU: 27.425 GB, reserved space on GPU: 27.920 GB, free space: 2.883GB, frag space: 0.495GB
do_liquid latency: 2.59s
INFO 09-05 12:35:29 llm_engine.py:758] Finished liquid for 78 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.54s, update worker latency: 0.00s, liquid model weights latency: 0.70s, init mem latency: 0.00s, liquid kvc latency: 0.84s;, current mem info on GPU0: allocated space on GPU: 27.425 GB, reserved space on GPU: 27.920 GB, free space: 2.883GB, frag space: 0.495GB
INFO 09-05 12:35:31 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:35:31 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.607 GB, free space: 16.196GB, frag space: 0.511GB
INFO 09-05 12:35:31 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.607 GB, free space: 16.196GB, frag space: 0.511GB
INFO 09-05 12:35:31 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.395 GB, free space: 10.409GB, frag space: 0.105GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.64s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 22.92GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:31 worker.py:152] send weights shards takes: 0.69s, sent out: 6.19GB, sent bw: 9.02GB/s
INFO 09-05 12:35:31 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.998 GB, free space: 9.805GB, frag space: 0.708GB
INFO 09-05 12:35:31 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.998 GB, free space: 9.805GB, frag space: 0.708GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:32 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.26GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:32 cache_engine.py:186] After deleting layer's shard, free mem: 23474.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:32 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:35:33 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.373 GB, free space: 2.430GB, frag space: 0.704GB
do_liquid latency: 3.35s
INFO 09-05 12:35:33 llm_engine.py:758] Finished liquid for 79 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.97s, update worker latency: 0.00s, liquid model weights latency: 0.76s, init mem latency: 0.00s, liquid kvc latency: 1.21s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.373 GB, free space: 2.430GB, frag space: 0.704GB
INFO 09-05 12:35:33 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:33 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:33 pynccl.py:65] vLLM is using nccl==2.20.5
INFO 09-05 12:35:33 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:35:33 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:35:33 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:35:33 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:35:33 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.373 GB, free space: 2.426GB, frag space: 0.704GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:33 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.358GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:34 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.367GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:34 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.172 GB, free space: 23.940GB, frag space: 0.454GB
It takes: 0.65s to send model shards
After sending shards, there are 8.88GB remaining on GPU0
INFO 09-05 12:35:34 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.71GB/s
INFO 09-05 12:35:34 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.920 GB, free space: 8.879GB, frag space: 0.444GB
INFO 09-05 12:35:35 cache_engine.py:146] send kvc shards takes: 0.71s, sent out: 7.38GB, sent bw: 10.34GB/s
INFO 09-05 12:35:35 cache_engine.py:186] After deleting layer's shard, free mem: 9092.31MB
INFO 09-05 12:35:35 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:35:35 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.545 GB, free space: 16.254GB, frag space: 0.448GB
INFO 09-05 12:35:35 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5316
INFO 09-05 12:35:35 worker.py:205] extend gpu in worker takes: 0.47s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:35 worker.py:205] extend gpu in worker takes: 0.47s
INFO 09-05 12:35:35 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.47s, after extending gpu blocks: allocated space on GPU: 27.483 GB, reserved space on GPU: 27.920 GB, free space: 2.879GB, frag space: 0.436GB
do_liquid latency: 2.61s
INFO 09-05 12:35:35 llm_engine.py:758] Finished liquid for 80 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.55s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.84s;, current mem info on GPU0: allocated space on GPU: 27.483 GB, reserved space on GPU: 27.920 GB, free space: 2.879GB, frag space: 0.436GB
INFO 09-05 12:35:37 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:35:37 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.545 GB, free space: 16.252GB, frag space: 0.448GB
INFO 09-05 12:35:37 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.545 GB, free space: 16.252GB, frag space: 0.448GB
INFO 09-05 12:35:37 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.395 GB, free space: 10.403GB, frag space: 0.105GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.63s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 22.92GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:37 worker.py:152] send weights shards takes: 0.67s, sent out: 6.19GB, sent bw: 9.20GB/s
INFO 09-05 12:35:37 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.873 GB, free space: 9.924GB, frag space: 0.583GB
INFO 09-05 12:35:37 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.873 GB, free space: 9.924GB, frag space: 0.583GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:38 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.20GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:38 cache_engine.py:186] After deleting layer's shard, free mem: 23468.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:38 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:35:39 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.248 GB, free space: 2.551GB, frag space: 0.579GB
do_liquid latency: 3.28s
INFO 09-05 12:35:39 llm_engine.py:758] Finished liquid for 81 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.98s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 1.23s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.248 GB, free space: 2.551GB, frag space: 0.579GB
INFO 09-05 12:35:39 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:35:39 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:39 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:39 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:35:39 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:35:39 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:35:39 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:35:39 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.248 GB, free space: 2.545GB, frag space: 0.579GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:39 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.352GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:40 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.362GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:40 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.156 GB, free space: 23.950GB, frag space: 0.438GB
It takes: 0.65s to send model shards
After sending shards, there are 8.84GB remaining on GPU0
INFO 09-05 12:35:40 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.75GB/s
INFO 09-05 12:35:40 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.957 GB, free space: 8.836GB, frag space: 0.481GB
INFO 09-05 12:35:41 cache_engine.py:146] send kvc shards takes: 0.71s, sent out: 7.38GB, sent bw: 10.33GB/s
INFO 09-05 12:35:41 cache_engine.py:186] After deleting layer's shard, free mem: 9048.31MB
INFO 09-05 12:35:41 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:35:41 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.582 GB, free space: 16.211GB, frag space: 0.485GB
INFO 09-05 12:35:41 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5305
INFO 09-05 12:35:41 worker.py:205] extend gpu in worker takes: 0.49s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:41 worker.py:205] extend gpu in worker takes: 0.49s
INFO 09-05 12:35:41 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.49s, after extending gpu blocks: allocated space on GPU: 27.468 GB, reserved space on GPU: 27.895 GB, free space: 2.901GB, frag space: 0.427GB
do_liquid latency: 2.63s
INFO 09-05 12:35:41 llm_engine.py:758] Finished liquid for 82 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.54s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.83s;, current mem info on GPU0: allocated space on GPU: 27.468 GB, reserved space on GPU: 27.895 GB, free space: 2.901GB, frag space: 0.427GB
INFO 09-05 12:35:43 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:35:43 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.582 GB, free space: 16.211GB, frag space: 0.485GB
INFO 09-05 12:35:43 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.582 GB, free space: 16.211GB, frag space: 0.485GB
INFO 09-05 12:35:43 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.400 GB, free space: 10.393GB, frag space: 0.111GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.64s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 22.91GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:43 worker.py:152] send weights shards takes: 0.68s, sent out: 6.19GB, sent bw: 9.06GB/s
INFO 09-05 12:35:43 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.973 GB, free space: 9.821GB, frag space: 0.682GB
INFO 09-05 12:35:43 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.973 GB, free space: 9.821GB, frag space: 0.682GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:44 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.23GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:44 cache_engine.py:186] After deleting layer's shard, free mem: 23464.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:44 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:35:45 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.348 GB, free space: 2.448GB, frag space: 0.678GB
do_liquid latency: 3.26s
INFO 09-05 12:35:45 llm_engine.py:758] Finished liquid for 83 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.99s, update worker latency: 0.00s, liquid model weights latency: 0.75s, init mem latency: 0.00s, liquid kvc latency: 1.24s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.348 GB, free space: 2.448GB, frag space: 0.678GB
INFO 09-05 12:35:45 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:35:45 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:45 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:45 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:35:45 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-05 12:35:45 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:35:45 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:35:45 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.348 GB, free space: 2.442GB, frag space: 0.678GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:45 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.348GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:46 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.358GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:46 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.156 GB, free space: 23.946GB, frag space: 0.438GB
It takes: 0.65s to send model shards
After sending shards, there are 8.76GB remaining on GPU0
INFO 09-05 12:35:46 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.70GB/s
INFO 09-05 12:35:46 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 22.025 GB, free space: 8.764GB, frag space: 0.550GB
INFO 09-05 12:35:47 cache_engine.py:146] send kvc shards takes: 0.71s, sent out: 7.38GB, sent bw: 10.34GB/s
INFO 09-05 12:35:47 cache_engine.py:186] After deleting layer's shard, free mem: 8974.31MB
INFO 09-05 12:35:47 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:35:47 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.650 GB, free space: 16.139GB, frag space: 0.554GB
INFO 09-05 12:35:47 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5286
INFO 09-05 12:35:47 worker.py:205] extend gpu in worker takes: 0.48s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:47 worker.py:205] extend gpu in worker takes: 0.48s
INFO 09-05 12:35:47 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.48s, after extending gpu blocks: allocated space on GPU: 27.366 GB, reserved space on GPU: 27.900 GB, free space: 2.887GB, frag space: 0.534GB
do_liquid latency: 2.62s
INFO 09-05 12:35:47 llm_engine.py:758] Finished liquid for 84 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.54s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.82s;, current mem info on GPU0: allocated space on GPU: 27.366 GB, reserved space on GPU: 27.900 GB, free space: 2.887GB, frag space: 0.534GB
INFO 09-05 12:35:49 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:35:49 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.650 GB, free space: 16.139GB, frag space: 0.554GB
INFO 09-05 12:35:49 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.650 GB, free space: 16.139GB, frag space: 0.554GB
INFO 09-05 12:35:49 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.406 GB, free space: 10.383GB, frag space: 0.117GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.66s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 22.91GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:50 worker.py:152] send weights shards takes: 0.70s, sent out: 6.19GB, sent bw: 8.83GB/s
INFO 09-05 12:35:50 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 21.135 GB, free space: 9.655GB, frag space: 0.844GB
INFO 09-05 12:35:50 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 21.135 GB, free space: 9.655GB, frag space: 0.844GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:50 cache_engine.py:146] send kvc shards takes: 0.79s, sent out: 7.38GB, sent bw: 9.28GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:50 cache_engine.py:186] After deleting layer's shard, free mem: 23460.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:50 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:35:51 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.510 GB, free space: 2.280GB, frag space: 0.840GB
do_liquid latency: 3.23s
INFO 09-05 12:35:51 llm_engine.py:758] Finished liquid for 85 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.98s, update worker latency: 0.00s, liquid model weights latency: 0.77s, init mem latency: 0.00s, liquid kvc latency: 1.21s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.510 GB, free space: 2.280GB, frag space: 0.840GB
INFO 09-05 12:35:51 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:35:51 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:51 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:51 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:35:51 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:35:51 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:35:51 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:35:51 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.510 GB, free space: 2.274GB, frag space: 0.840GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:51 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.344GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:52 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.354GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:52 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.156 GB, free space: 23.942GB, frag space: 0.438GB
It takes: 0.64s to send model shards
After sending shards, there are 8.91GB remaining on GPU0
INFO 09-05 12:35:52 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.76GB/s
INFO 09-05 12:35:52 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.869 GB, free space: 8.914GB, frag space: 0.394GB
INFO 09-05 12:35:53 cache_engine.py:146] send kvc shards takes: 0.72s, sent out: 7.38GB, sent bw: 10.32GB/s
INFO 09-05 12:35:53 cache_engine.py:186] After deleting layer's shard, free mem: 9128.31MB
INFO 09-05 12:35:53 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:35:53 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.494 GB, free space: 16.289GB, frag space: 0.397GB
INFO 09-05 12:35:53 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5325
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:53 worker.py:205] extend gpu in worker takes: 0.50s
INFO 09-05 12:35:53 worker.py:205] extend gpu in worker takes: 0.50s
INFO 09-05 12:35:53 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.50s, after extending gpu blocks: allocated space on GPU: 27.530 GB, reserved space on GPU: 27.869 GB, free space: 2.914GB, frag space: 0.339GB
do_liquid latency: 2.62s
INFO 09-05 12:35:53 llm_engine.py:758] Finished liquid for 86 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.52s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.81s;, current mem info on GPU0: allocated space on GPU: 27.530 GB, reserved space on GPU: 27.869 GB, free space: 2.914GB, frag space: 0.339GB
INFO 09-05 12:35:55 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:35:55 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.494 GB, free space: 16.289GB, frag space: 0.397GB
INFO 09-05 12:35:55 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.494 GB, free space: 16.289GB, frag space: 0.397GB
INFO 09-05 12:35:56 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.406 GB, free space: 10.377GB, frag space: 0.117GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.66s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 22.91GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:56 worker.py:152] send weights shards takes: 0.70s, sent out: 6.19GB, sent bw: 8.84GB/s
INFO 09-05 12:35:56 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.979 GB, free space: 9.805GB, frag space: 0.688GB
INFO 09-05 12:35:56 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.979 GB, free space: 9.805GB, frag space: 0.688GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:56 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.21GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:56 cache_engine.py:186] After deleting layer's shard, free mem: 23456.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:56 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:35:57 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.354 GB, free space: 2.430GB, frag space: 0.684GB
do_liquid latency: 3.25s
INFO 09-05 12:35:57 llm_engine.py:758] Finished liquid for 87 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.00s, update worker latency: 0.00s, liquid model weights latency: 0.77s, init mem latency: 0.00s, liquid kvc latency: 1.23s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.354 GB, free space: 2.430GB, frag space: 0.684GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:57 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:57 pynccl.py:65] vLLM is using nccl==2.20.5
INFO 09-05 12:35:57 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:35:57 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:35:58 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:35:58 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:35:58 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:35:58 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.354 GB, free space: 2.426GB, frag space: 0.684GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:58 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.340GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:58 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.350GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:35:58 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.188 GB, free space: 23.907GB, frag space: 0.470GB
It takes: 0.64s to send model shards
After sending shards, there are 9.01GB remaining on GPU0
INFO 09-05 12:35:58 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.76GB/s
INFO 09-05 12:35:58 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.770 GB, free space: 9.010GB, frag space: 0.294GB
INFO 09-05 12:35:59 cache_engine.py:146] send kvc shards takes: 0.71s, sent out: 7.38GB, sent bw: 10.33GB/s
INFO 09-05 12:35:59 cache_engine.py:186] After deleting layer's shard, free mem: 9226.31MB
INFO 09-05 12:35:59 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:35:59 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.395 GB, free space: 16.385GB, frag space: 0.298GB
INFO 09-05 12:35:59 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5349
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:00 worker.py:205] extend gpu in worker takes: 0.50s
INFO 09-05 12:36:00 worker.py:205] extend gpu in worker takes: 0.50s
INFO 09-05 12:36:00 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.50s, after extending gpu blocks: allocated space on GPU: 27.612 GB, reserved space on GPU: 27.895 GB, free space: 2.885GB, frag space: 0.282GB
do_liquid latency: 2.61s
INFO 09-05 12:36:00 llm_engine.py:758] Finished liquid for 88 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.52s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.81s;, current mem info on GPU0: allocated space on GPU: 27.612 GB, reserved space on GPU: 27.895 GB, free space: 2.885GB, frag space: 0.282GB
INFO 09-05 12:36:01 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:36:01 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.395 GB, free space: 16.385GB, frag space: 0.298GB
INFO 09-05 12:36:01 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.395 GB, free space: 16.385GB, frag space: 0.298GB
INFO 09-05 12:36:02 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.400 GB, free space: 10.379GB, frag space: 0.111GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.66s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 22.90GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:02 worker.py:152] send weights shards takes: 0.69s, sent out: 6.19GB, sent bw: 8.93GB/s
INFO 09-05 12:36:02 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 21.035 GB, free space: 9.744GB, frag space: 0.745GB
INFO 09-05 12:36:02 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 21.035 GB, free space: 9.744GB, frag space: 0.745GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:03 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.28GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:03 cache_engine.py:186] After deleting layer's shard, free mem: 23452.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:03 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:36:03 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.410 GB, free space: 2.369GB, frag space: 0.741GB
do_liquid latency: 3.23s
INFO 09-05 12:36:03 llm_engine.py:758] Finished liquid for 89 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.97s, update worker latency: 0.00s, liquid model weights latency: 0.76s, init mem latency: 0.00s, liquid kvc latency: 1.21s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.410 GB, free space: 2.369GB, frag space: 0.741GB
INFO 09-05 12:36:04 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:04 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:36:04 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:04 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:36:04 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:36:04 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:36:04 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:36:04 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.410 GB, free space: 2.366GB, frag space: 0.741GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:04 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.336GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:04 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.346GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:04 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.188 GB, free space: 23.903GB, frag space: 0.470GB
It takes: 0.64s to send model shards
After sending shards, there are 8.97GB remaining on GPU0
INFO 09-05 12:36:04 worker.py:152] send weights shards takes: 0.70s, sent out: 6.19GB, sent bw: 8.79GB/s
INFO 09-05 12:36:04 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.801 GB, free space: 8.975GB, frag space: 0.325GB
INFO 09-05 12:36:05 cache_engine.py:146] send kvc shards takes: 0.71s, sent out: 7.38GB, sent bw: 10.37GB/s
INFO 09-05 12:36:05 cache_engine.py:186] After deleting layer's shard, free mem: 9190.31MB
INFO 09-05 12:36:05 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:36:05 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.426 GB, free space: 16.350GB, frag space: 0.329GB
INFO 09-05 12:36:05 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5340
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:06 worker.py:205] extend gpu in worker takes: 0.49s
INFO 09-05 12:36:06 worker.py:205] extend gpu in worker takes: 0.50s
INFO 09-05 12:36:06 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.50s, after extending gpu blocks: allocated space on GPU: 27.593 GB, reserved space on GPU: 27.863 GB, free space: 2.912GB, frag space: 0.270GB
do_liquid latency: 2.59s
INFO 09-05 12:36:06 llm_engine.py:758] Finished liquid for 90 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.50s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.80s;, current mem info on GPU0: allocated space on GPU: 27.593 GB, reserved space on GPU: 27.863 GB, free space: 2.912GB, frag space: 0.270GB
INFO 09-05 12:36:07 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:36:07 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.426 GB, free space: 16.350GB, frag space: 0.329GB
INFO 09-05 12:36:07 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.426 GB, free space: 16.350GB, frag space: 0.329GB
INFO 09-05 12:36:08 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.400 GB, free space: 10.375GB, frag space: 0.111GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.65s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 22.90GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:08 worker.py:152] send weights shards takes: 0.69s, sent out: 6.19GB, sent bw: 9.03GB/s
INFO 09-05 12:36:08 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.973 GB, free space: 9.803GB, frag space: 0.682GB
INFO 09-05 12:36:08 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.973 GB, free space: 9.803GB, frag space: 0.682GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:09 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.19GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:09 cache_engine.py:186] After deleting layer's shard, free mem: 23448.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:09 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:36:09 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.348 GB, free space: 2.428GB, frag space: 0.678GB
do_liquid latency: 3.34s
INFO 09-05 12:36:09 llm_engine.py:758] Finished liquid for 91 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.01s, update worker latency: 0.00s, liquid model weights latency: 0.75s, init mem latency: 0.00s, liquid kvc latency: 1.25s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.348 GB, free space: 2.428GB, frag space: 0.678GB
INFO 09-05 12:36:10 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:10 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:10 pynccl.py:65] vLLM is using nccl==2.20.5
INFO 09-05 12:36:10 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:36:10 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:36:10 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:36:10 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:36:10 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.348 GB, free space: 2.424GB, frag space: 0.678GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:10 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.332GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:10 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.342GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:11 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.172 GB, free space: 23.914GB, frag space: 0.454GB
It takes: 0.64s to send model shards
After sending shards, there are 8.91GB remaining on GPU0
INFO 09-05 12:36:11 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.75GB/s
INFO 09-05 12:36:11 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.863 GB, free space: 8.909GB, frag space: 0.388GB
INFO 09-05 12:36:11 cache_engine.py:146] send kvc shards takes: 0.71s, sent out: 7.38GB, sent bw: 10.37GB/s
INFO 09-05 12:36:11 cache_engine.py:186] After deleting layer's shard, free mem: 9122.31MB
INFO 09-05 12:36:11 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:36:11 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.488 GB, free space: 16.284GB, frag space: 0.392GB
INFO 09-05 12:36:11 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5323
INFO 09-05 12:36:12 worker.py:205] extend gpu in worker takes: 0.46s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:12 worker.py:205] extend gpu in worker takes: 0.46s
INFO 09-05 12:36:12 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.46s, after extending gpu blocks: allocated space on GPU: 27.530 GB, reserved space on GPU: 27.863 GB, free space: 2.909GB, frag space: 0.333GB
do_liquid latency: 2.60s
INFO 09-05 12:36:12 llm_engine.py:758] Finished liquid for 92 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.55s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.84s;, current mem info on GPU0: allocated space on GPU: 27.530 GB, reserved space on GPU: 27.863 GB, free space: 2.909GB, frag space: 0.333GB
INFO 09-05 12:36:13 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:36:13 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.488 GB, free space: 16.284GB, frag space: 0.392GB
INFO 09-05 12:36:13 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.488 GB, free space: 16.284GB, frag space: 0.392GB
INFO 09-05 12:36:14 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.400 GB, free space: 10.371GB, frag space: 0.111GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.64s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 22.89GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:14 worker.py:152] send weights shards takes: 0.67s, sent out: 6.19GB, sent bw: 9.20GB/s
INFO 09-05 12:36:14 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.973 GB, free space: 9.799GB, frag space: 0.682GB
INFO 09-05 12:36:14 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.973 GB, free space: 9.799GB, frag space: 0.682GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:15 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.22GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:15 cache_engine.py:186] After deleting layer's shard, free mem: 23444.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:15 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:36:15 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.348 GB, free space: 2.424GB, frag space: 0.678GB
do_liquid latency: 3.23s
INFO 09-05 12:36:15 llm_engine.py:758] Finished liquid for 93 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.96s, update worker latency: 0.00s, liquid model weights latency: 0.74s, init mem latency: 0.00s, liquid kvc latency: 1.22s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.348 GB, free space: 2.424GB, frag space: 0.678GB
INFO 09-05 12:36:16 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:16 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:16 pynccl.py:65] vLLM is using nccl==2.20.5
INFO 09-05 12:36:16 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:36:16 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-05 12:36:16 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:36:16 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:36:16 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.348 GB, free space: 2.420GB, frag space: 0.678GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:16 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.328GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:17 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.338GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:17 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.156 GB, free space: 23.926GB, frag space: 0.438GB
It takes: 0.64s to send model shards
After sending shards, there are 8.84GB remaining on GPU0
INFO 09-05 12:36:17 worker.py:152] send weights shards takes: 0.70s, sent out: 6.19GB, sent bw: 8.80GB/s
INFO 09-05 12:36:17 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.926 GB, free space: 8.842GB, frag space: 0.450GB
INFO 09-05 12:36:17 cache_engine.py:146] send kvc shards takes: 0.71s, sent out: 7.38GB, sent bw: 10.38GB/s
INFO 09-05 12:36:17 cache_engine.py:186] After deleting layer's shard, free mem: 9054.31MB
INFO 09-05 12:36:17 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:36:17 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.551 GB, free space: 16.217GB, frag space: 0.454GB
INFO 09-05 12:36:17 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5306
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:18 worker.py:205] extend gpu in worker takes: 0.47s
INFO 09-05 12:36:18 worker.py:205] extend gpu in worker takes: 0.47s
INFO 09-05 12:36:18 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.47s, after extending gpu blocks: allocated space on GPU: 27.468 GB, reserved space on GPU: 27.863 GB, free space: 2.905GB, frag space: 0.395GB
do_liquid latency: 2.61s
INFO 09-05 12:36:18 llm_engine.py:758] Finished liquid for 94 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.54s, update worker latency: 0.00s, liquid model weights latency: 0.70s, init mem latency: 0.00s, liquid kvc latency: 0.84s;, current mem info on GPU0: allocated space on GPU: 27.468 GB, reserved space on GPU: 27.863 GB, free space: 2.905GB, frag space: 0.395GB
INFO 09-05 12:36:19 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:36:19 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.551 GB, free space: 16.217GB, frag space: 0.454GB
INFO 09-05 12:36:19 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.551 GB, free space: 16.217GB, frag space: 0.454GB
INFO 09-05 12:36:20 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.400 GB, free space: 10.367GB, frag space: 0.111GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.66s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 22.89GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:20 worker.py:152] send weights shards takes: 0.70s, sent out: 6.19GB, sent bw: 8.88GB/s
INFO 09-05 12:36:20 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.879 GB, free space: 9.889GB, frag space: 0.588GB
INFO 09-05 12:36:20 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.879 GB, free space: 9.889GB, frag space: 0.588GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:21 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.22GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:21 cache_engine.py:186] After deleting layer's shard, free mem: 23440.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:21 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:36:21 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.254 GB, free space: 2.514GB, frag space: 0.585GB
do_liquid latency: 3.23s
INFO 09-05 12:36:21 llm_engine.py:758] Finished liquid for 95 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.98s, update worker latency: 0.00s, liquid model weights latency: 0.77s, init mem latency: 0.00s, liquid kvc latency: 1.22s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.254 GB, free space: 2.514GB, frag space: 0.585GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:22 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:36:22 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:22 pynccl.py:65] vLLM is using nccl==2.20.5
INFO 09-05 12:36:22 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:36:22 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:36:22 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:36:22 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:36:22 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.254 GB, free space: 2.510GB, frag space: 0.585GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:22 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.325GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:23 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.334GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:23 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.172 GB, free space: 23.907GB, frag space: 0.454GB
It takes: 0.64s to send model shards
After sending shards, there are 8.78GB remaining on GPU0
INFO 09-05 12:36:23 worker.py:152] send weights shards takes: 0.70s, sent out: 6.19GB, sent bw: 8.81GB/s
INFO 09-05 12:36:23 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.982 GB, free space: 8.782GB, frag space: 0.507GB
INFO 09-05 12:36:23 cache_engine.py:146] send kvc shards takes: 0.71s, sent out: 7.38GB, sent bw: 10.36GB/s
INFO 09-05 12:36:23 cache_engine.py:186] After deleting layer's shard, free mem: 8992.31MB
INFO 09-05 12:36:23 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:36:23 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.607 GB, free space: 16.157GB, frag space: 0.511GB
INFO 09-05 12:36:23 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5291
INFO 09-05 12:36:24 worker.py:205] extend gpu in worker takes: 0.46s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:24 worker.py:205] extend gpu in worker takes: 0.47s
INFO 09-05 12:36:24 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.47s, after extending gpu blocks: allocated space on GPU: 27.405 GB, reserved space on GPU: 27.857 GB, free space: 2.907GB, frag space: 0.452GB
do_liquid latency: 2.60s
INFO 09-05 12:36:24 llm_engine.py:758] Finished liquid for 96 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.54s, update worker latency: 0.00s, liquid model weights latency: 0.70s, init mem latency: 0.00s, liquid kvc latency: 0.83s;, current mem info on GPU0: allocated space on GPU: 27.405 GB, reserved space on GPU: 27.857 GB, free space: 2.907GB, frag space: 0.452GB
INFO 09-05 12:36:25 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:36:25 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.607 GB, free space: 16.155GB, frag space: 0.511GB
INFO 09-05 12:36:25 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.607 GB, free space: 16.155GB, frag space: 0.511GB
INFO 09-05 12:36:26 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.395 GB, free space: 10.367GB, frag space: 0.105GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.64s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 22.89GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:26 worker.py:152] send weights shards takes: 0.68s, sent out: 6.19GB, sent bw: 9.06GB/s
INFO 09-05 12:36:26 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.904 GB, free space: 9.858GB, frag space: 0.614GB
INFO 09-05 12:36:26 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.904 GB, free space: 9.858GB, frag space: 0.614GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:27 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.27GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:27 cache_engine.py:186] After deleting layer's shard, free mem: 23434.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:27 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:36:27 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.279 GB, free space: 2.485GB, frag space: 0.610GB
do_liquid latency: 3.33s
INFO 09-05 12:36:27 llm_engine.py:758] Finished liquid for 97 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.98s, update worker latency: 0.00s, liquid model weights latency: 0.76s, init mem latency: 0.00s, liquid kvc latency: 1.22s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.279 GB, free space: 2.485GB, frag space: 0.610GB
INFO 09-05 12:36:28 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:36:28 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:28 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:28 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:36:28 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:36:28 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:36:28 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:36:28 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.279 GB, free space: 2.479GB, frag space: 0.610GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:28 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.319GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:29 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.328GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:29 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.172 GB, free space: 23.901GB, frag space: 0.454GB
It takes: 0.66s to send model shards
After sending shards, there are 8.83GB remaining on GPU0
INFO 09-05 12:36:29 worker.py:152] send weights shards takes: 0.72s, sent out: 6.19GB, sent bw: 8.58GB/s
INFO 09-05 12:36:29 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.926 GB, free space: 8.832GB, frag space: 0.450GB
INFO 09-05 12:36:29 cache_engine.py:146] send kvc shards takes: 0.71s, sent out: 7.38GB, sent bw: 10.33GB/s
INFO 09-05 12:36:30 cache_engine.py:186] After deleting layer's shard, free mem: 9044.31MB
INFO 09-05 12:36:30 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:36:30 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.551 GB, free space: 16.207GB, frag space: 0.454GB
INFO 09-05 12:36:30 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5304
INFO 09-05 12:36:30 worker.py:205] extend gpu in worker takes: 0.47s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:30 worker.py:205] extend gpu in worker takes: 0.47s
INFO 09-05 12:36:30 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.47s, after extending gpu blocks: allocated space on GPU: 27.468 GB, reserved space on GPU: 27.863 GB, free space: 2.895GB, frag space: 0.395GB
do_liquid latency: 2.64s
INFO 09-05 12:36:30 llm_engine.py:758] Finished liquid for 98 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.56s, update worker latency: 0.00s, liquid model weights latency: 0.72s, init mem latency: 0.00s, liquid kvc latency: 0.84s;, current mem info on GPU0: allocated space on GPU: 27.468 GB, reserved space on GPU: 27.863 GB, free space: 2.895GB, frag space: 0.395GB
INFO 09-05 12:36:31 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:36:31 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.551 GB, free space: 16.205GB, frag space: 0.454GB
INFO 09-05 12:36:31 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.551 GB, free space: 16.205GB, frag space: 0.454GB
INFO 09-05 12:36:32 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.400 GB, free space: 10.356GB, frag space: 0.111GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.65s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 22.88GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:32 worker.py:152] send weights shards takes: 0.69s, sent out: 6.19GB, sent bw: 8.95GB/s
INFO 09-05 12:36:32 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.879 GB, free space: 9.877GB, frag space: 0.588GB
INFO 09-05 12:36:32 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.879 GB, free space: 9.877GB, frag space: 0.588GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:33 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.22GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:33 cache_engine.py:186] After deleting layer's shard, free mem: 23430.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:33 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:36:33 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.254 GB, free space: 2.504GB, frag space: 0.585GB
do_liquid latency: 3.25s
INFO 09-05 12:36:33 llm_engine.py:758] Finished liquid for 99 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.99s, update worker latency: 0.00s, liquid model weights latency: 0.76s, init mem latency: 0.00s, liquid kvc latency: 1.23s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.254 GB, free space: 2.504GB, frag space: 0.585GB
INFO 09-05 12:36:34 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:34 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:36:34 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:34 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:36:34 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:36:34 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:36:34 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:36:34 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.254 GB, free space: 2.498GB, frag space: 0.585GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:34 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.315GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:35 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.323GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:35 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.172 GB, free space: 23.895GB, frag space: 0.454GB
It takes: 0.65s to send model shards
After sending shards, there are 8.70GB remaining on GPU0
INFO 09-05 12:36:35 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.74GB/s
INFO 09-05 12:36:35 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 22.051 GB, free space: 8.701GB, frag space: 0.575GB
INFO 09-05 12:36:36 cache_engine.py:146] send kvc shards takes: 0.71s, sent out: 7.38GB, sent bw: 10.33GB/s
INFO 09-05 12:36:36 cache_engine.py:186] After deleting layer's shard, free mem: 8910.31MB
INFO 09-05 12:36:36 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:36:36 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.676 GB, free space: 16.076GB, frag space: 0.579GB
INFO 09-05 12:36:36 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5270
INFO 09-05 12:36:36 worker.py:205] extend gpu in worker takes: 0.46s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:36 worker.py:205] extend gpu in worker takes: 0.46s
INFO 09-05 12:36:36 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.46s, after extending gpu blocks: allocated space on GPU: 27.304 GB, reserved space on GPU: 27.863 GB, free space: 2.889GB, frag space: 0.560GB
do_liquid latency: 2.60s
INFO 09-05 12:36:36 llm_engine.py:758] Finished liquid for 100 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.55s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.84s;, current mem info on GPU0: allocated space on GPU: 27.304 GB, reserved space on GPU: 27.863 GB, free space: 2.889GB, frag space: 0.560GB
INFO 09-05 12:36:38 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:36:38 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.676 GB, free space: 16.076GB, frag space: 0.579GB
INFO 09-05 12:36:38 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.676 GB, free space: 16.076GB, frag space: 0.579GB
INFO 09-05 12:36:38 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.400 GB, free space: 10.352GB, frag space: 0.111GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.67s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 22.88GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:38 worker.py:152] send weights shards takes: 0.70s, sent out: 6.19GB, sent bw: 8.80GB/s
INFO 09-05 12:36:38 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 21.004 GB, free space: 9.748GB, frag space: 0.713GB
INFO 09-05 12:36:38 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 21.004 GB, free space: 9.748GB, frag space: 0.713GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:39 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.20GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:39 cache_engine.py:186] After deleting layer's shard, free mem: 23424.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:39 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:36:40 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.379 GB, free space: 2.373GB, frag space: 0.710GB
do_liquid latency: 3.29s
INFO 09-05 12:36:40 llm_engine.py:758] Finished liquid for 101 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.00s, update worker latency: 0.00s, liquid model weights latency: 0.77s, init mem latency: 0.00s, liquid kvc latency: 1.23s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.379 GB, free space: 2.373GB, frag space: 0.710GB
INFO 09-05 12:36:40 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:36:40 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:40 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:40 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:36:40 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:36:40 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:36:40 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:36:40 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.379 GB, free space: 2.369GB, frag space: 0.710GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:40 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.309GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:41 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.319GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:41 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.172 GB, free space: 23.891GB, frag space: 0.454GB
It takes: 0.65s to send model shards
After sending shards, there are 8.79GB remaining on GPU0
INFO 09-05 12:36:41 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.73GB/s
INFO 09-05 12:36:41 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.957 GB, free space: 8.791GB, frag space: 0.481GB
INFO 09-05 12:36:42 cache_engine.py:146] send kvc shards takes: 0.72s, sent out: 7.38GB, sent bw: 10.31GB/s
INFO 09-05 12:36:42 cache_engine.py:186] After deleting layer's shard, free mem: 9002.31MB
INFO 09-05 12:36:42 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:36:42 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.582 GB, free space: 16.166GB, frag space: 0.485GB
INFO 09-05 12:36:42 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5293
INFO 09-05 12:36:42 worker.py:205] extend gpu in worker takes: 0.49s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:42 worker.py:205] extend gpu in worker takes: 0.49s
INFO 09-05 12:36:42 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.49s, after extending gpu blocks: allocated space on GPU: 27.405 GB, reserved space on GPU: 27.832 GB, free space: 2.916GB, frag space: 0.427GB
do_liquid latency: 2.62s
INFO 09-05 12:36:42 llm_engine.py:758] Finished liquid for 102 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.54s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.83s;, current mem info on GPU0: allocated space on GPU: 27.405 GB, reserved space on GPU: 27.832 GB, free space: 2.916GB, frag space: 0.427GB
INFO 09-05 12:36:44 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:36:44 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.582 GB, free space: 16.166GB, frag space: 0.485GB
INFO 09-05 12:36:44 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.582 GB, free space: 16.166GB, frag space: 0.485GB
INFO 09-05 12:36:44 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.400 GB, free space: 10.348GB, frag space: 0.111GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.66s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 22.87GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:44 worker.py:152] send weights shards takes: 0.70s, sent out: 6.19GB, sent bw: 8.91GB/s
INFO 09-05 12:36:44 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.879 GB, free space: 9.869GB, frag space: 0.588GB
INFO 09-05 12:36:44 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.879 GB, free space: 9.869GB, frag space: 0.588GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:45 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.17GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:45 cache_engine.py:186] After deleting layer's shard, free mem: 23420.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:45 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:36:46 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.254 GB, free space: 2.494GB, frag space: 0.585GB
do_liquid latency: 3.26s
INFO 09-05 12:36:46 llm_engine.py:758] Finished liquid for 103 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.99s, update worker latency: 0.00s, liquid model weights latency: 0.76s, init mem latency: 0.00s, liquid kvc latency: 1.23s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.254 GB, free space: 2.494GB, frag space: 0.585GB
INFO 09-05 12:36:46 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:36:46 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:46 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:46 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:36:46 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:36:46 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:36:46 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:36:46 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.254 GB, free space: 2.491GB, frag space: 0.585GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:46 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.305GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:47 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.315GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:47 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.172 GB, free space: 23.887GB, frag space: 0.454GB
It takes: 0.64s to send model shards
After sending shards, there are 8.72GB remaining on GPU0
INFO 09-05 12:36:47 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.77GB/s
INFO 09-05 12:36:47 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 22.020 GB, free space: 8.725GB, frag space: 0.544GB
INFO 09-05 12:36:48 cache_engine.py:146] send kvc shards takes: 0.71s, sent out: 7.38GB, sent bw: 10.36GB/s
INFO 09-05 12:36:48 cache_engine.py:186] After deleting layer's shard, free mem: 8934.31MB
INFO 09-05 12:36:48 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:36:48 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.645 GB, free space: 16.100GB, frag space: 0.548GB
INFO 09-05 12:36:48 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5276
INFO 09-05 12:36:48 worker.py:205] extend gpu in worker takes: 0.48s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:48 worker.py:205] extend gpu in worker takes: 0.48s
INFO 09-05 12:36:48 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.48s, after extending gpu blocks: allocated space on GPU: 27.343 GB, reserved space on GPU: 27.832 GB, free space: 2.912GB, frag space: 0.489GB
do_liquid latency: 2.62s
INFO 09-05 12:36:48 llm_engine.py:758] Finished liquid for 104 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.54s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.84s;, current mem info on GPU0: allocated space on GPU: 27.343 GB, reserved space on GPU: 27.832 GB, free space: 2.912GB, frag space: 0.489GB
INFO 09-05 12:36:50 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:36:50 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.645 GB, free space: 16.100GB, frag space: 0.548GB
INFO 09-05 12:36:50 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.645 GB, free space: 16.100GB, frag space: 0.548GB
INFO 09-05 12:36:50 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.400 GB, free space: 10.344GB, frag space: 0.111GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.67s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 22.87GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:50 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.78GB/s
INFO 09-05 12:36:51 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.941 GB, free space: 9.803GB, frag space: 0.651GB
INFO 09-05 12:36:51 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.941 GB, free space: 9.803GB, frag space: 0.651GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:51 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.18GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:51 cache_engine.py:186] After deleting layer's shard, free mem: 23416.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:51 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:36:52 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.316 GB, free space: 2.428GB, frag space: 0.647GB
do_liquid latency: 3.24s
INFO 09-05 12:36:52 llm_engine.py:758] Finished liquid for 105 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.99s, update worker latency: 0.00s, liquid model weights latency: 0.77s, init mem latency: 0.00s, liquid kvc latency: 1.22s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.316 GB, free space: 2.428GB, frag space: 0.647GB
INFO 09-05 12:36:52 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:36:52 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:52 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:52 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:36:52 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:36:52 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:36:52 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:36:52 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.316 GB, free space: 2.424GB, frag space: 0.647GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:52 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.301GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:53 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.311GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:53 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.172 GB, free space: 23.883GB, frag space: 0.454GB
It takes: 0.65s to send model shards
After sending shards, there are 8.88GB remaining on GPU0
INFO 09-05 12:36:53 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.70GB/s
INFO 09-05 12:36:53 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.857 GB, free space: 8.883GB, frag space: 0.382GB
INFO 09-05 12:36:54 cache_engine.py:146] send kvc shards takes: 0.72s, sent out: 7.38GB, sent bw: 10.31GB/s
INFO 09-05 12:36:54 cache_engine.py:186] After deleting layer's shard, free mem: 9096.31MB
INFO 09-05 12:36:54 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:36:54 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.482 GB, free space: 16.258GB, frag space: 0.386GB
INFO 09-05 12:36:54 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5317
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:54 worker.py:205] extend gpu in worker takes: 0.48s
INFO 09-05 12:36:54 worker.py:205] extend gpu in worker takes: 0.48s
INFO 09-05 12:36:54 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.49s, after extending gpu blocks: allocated space on GPU: 27.487 GB, reserved space on GPU: 27.857 GB, free space: 2.883GB, frag space: 0.370GB
do_liquid latency: 2.61s
INFO 09-05 12:36:54 llm_engine.py:758] Finished liquid for 106 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.53s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.82s;, current mem info on GPU0: allocated space on GPU: 27.487 GB, reserved space on GPU: 27.857 GB, free space: 2.883GB, frag space: 0.370GB
INFO 09-05 12:36:56 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:36:56 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.482 GB, free space: 16.258GB, frag space: 0.386GB
INFO 09-05 12:36:56 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.482 GB, free space: 16.258GB, frag space: 0.386GB
INFO 09-05 12:36:57 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.395 GB, free space: 10.346GB, frag space: 0.105GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.65s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 22.86GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:57 worker.py:152] send weights shards takes: 0.69s, sent out: 6.19GB, sent bw: 9.03GB/s
INFO 09-05 12:36:57 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.936 GB, free space: 9.805GB, frag space: 0.645GB
INFO 09-05 12:36:57 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.936 GB, free space: 9.805GB, frag space: 0.645GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:57 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.21GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:57 cache_engine.py:186] After deleting layer's shard, free mem: 23412.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:57 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:36:58 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.311 GB, free space: 2.430GB, frag space: 0.641GB
do_liquid latency: 3.24s
INFO 09-05 12:36:58 llm_engine.py:758] Finished liquid for 107 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.96s, update worker latency: 0.00s, liquid model weights latency: 0.75s, init mem latency: 0.00s, liquid kvc latency: 1.22s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.311 GB, free space: 2.430GB, frag space: 0.641GB
INFO 09-05 12:36:58 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:58 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:36:58 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:58 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:36:59 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:36:59 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:36:59 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:36:59 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.311 GB, free space: 2.426GB, frag space: 0.641GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:59 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.297GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:59 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.307GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:36:59 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.172 GB, free space: 23.879GB, frag space: 0.454GB
It takes: 0.65s to send model shards
After sending shards, there are 8.88GB remaining on GPU0
INFO 09-05 12:36:59 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.67GB/s
INFO 09-05 12:36:59 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.857 GB, free space: 8.879GB, frag space: 0.382GB
INFO 09-05 12:37:00 cache_engine.py:146] send kvc shards takes: 0.71s, sent out: 7.38GB, sent bw: 10.33GB/s
INFO 09-05 12:37:00 cache_engine.py:186] After deleting layer's shard, free mem: 9092.31MB
INFO 09-05 12:37:00 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:37:00 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.482 GB, free space: 16.254GB, frag space: 0.386GB
INFO 09-05 12:37:00 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5316
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:01 worker.py:205] extend gpu in worker takes: 0.47s
INFO 09-05 12:37:01 worker.py:205] extend gpu in worker takes: 0.47s
INFO 09-05 12:37:01 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.47s, after extending gpu blocks: allocated space on GPU: 27.483 GB, reserved space on GPU: 27.857 GB, free space: 2.879GB, frag space: 0.374GB
do_liquid latency: 2.63s
INFO 09-05 12:37:01 llm_engine.py:758] Finished liquid for 108 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.57s, update worker latency: 0.00s, liquid model weights latency: 0.72s, init mem latency: 0.00s, liquid kvc latency: 0.85s;, current mem info on GPU0: allocated space on GPU: 27.483 GB, reserved space on GPU: 27.857 GB, free space: 2.879GB, frag space: 0.374GB
INFO 09-05 12:37:02 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:37:02 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.482 GB, free space: 16.254GB, frag space: 0.386GB
INFO 09-05 12:37:02 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.482 GB, free space: 16.254GB, frag space: 0.386GB
INFO 09-05 12:37:03 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.395 GB, free space: 10.342GB, frag space: 0.105GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.66s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 22.86GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:03 worker.py:152] send weights shards takes: 0.70s, sent out: 6.19GB, sent bw: 8.80GB/s
INFO 09-05 12:37:03 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.873 GB, free space: 9.864GB, frag space: 0.583GB
INFO 09-05 12:37:03 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.873 GB, free space: 9.864GB, frag space: 0.583GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:04 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.19GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:04 cache_engine.py:186] After deleting layer's shard, free mem: 23408.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:04 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:37:04 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.248 GB, free space: 2.489GB, frag space: 0.579GB
do_liquid latency: 3.26s
INFO 09-05 12:37:04 llm_engine.py:758] Finished liquid for 109 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.00s, update worker latency: 0.00s, liquid model weights latency: 0.78s, init mem latency: 0.00s, liquid kvc latency: 1.22s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.248 GB, free space: 2.489GB, frag space: 0.579GB
INFO 09-05 12:37:05 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:05 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:37:05 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:05 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:37:05 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-05 12:37:05 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:37:05 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:37:05 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.248 GB, free space: 2.485GB, frag space: 0.579GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:05 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.293GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:05 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.303GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:05 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.172 GB, free space: 23.875GB, frag space: 0.454GB
It takes: 0.65s to send model shards
After sending shards, there are 8.72GB remaining on GPU0
INFO 09-05 12:37:05 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.76GB/s
INFO 09-05 12:37:05 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 22.008 GB, free space: 8.725GB, frag space: 0.532GB
INFO 09-05 12:37:06 cache_engine.py:146] send kvc shards takes: 0.72s, sent out: 7.38GB, sent bw: 10.28GB/s
INFO 09-05 12:37:06 cache_engine.py:186] After deleting layer's shard, free mem: 8934.31MB
INFO 09-05 12:37:06 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:37:06 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.633 GB, free space: 16.100GB, frag space: 0.536GB
INFO 09-05 12:37:06 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5276
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:07 worker.py:205] extend gpu in worker takes: 0.49s
INFO 09-05 12:37:07 worker.py:205] extend gpu in worker takes: 0.49s
INFO 09-05 12:37:07 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.49s, after extending gpu blocks: allocated space on GPU: 27.343 GB, reserved space on GPU: 27.820 GB, free space: 2.912GB, frag space: 0.478GB
do_liquid latency: 2.62s
INFO 09-05 12:37:07 llm_engine.py:758] Finished liquid for 110 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.53s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.82s;, current mem info on GPU0: allocated space on GPU: 27.343 GB, reserved space on GPU: 27.820 GB, free space: 2.912GB, frag space: 0.478GB
INFO 09-05 12:37:08 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:37:08 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.633 GB, free space: 16.100GB, frag space: 0.536GB
INFO 09-05 12:37:08 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.633 GB, free space: 16.100GB, frag space: 0.536GB
INFO 09-05 12:37:09 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.389 GB, free space: 10.344GB, frag space: 0.099GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.67s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 22.86GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:09 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.74GB/s
INFO 09-05 12:37:09 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.867 GB, free space: 9.866GB, frag space: 0.577GB
INFO 09-05 12:37:09 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.867 GB, free space: 9.866GB, frag space: 0.577GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:10 cache_engine.py:146] send kvc shards takes: 0.81s, sent out: 7.38GB, sent bw: 9.17GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:10 cache_engine.py:186] After deleting layer's shard, free mem: 23404.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:10 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:37:10 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.242 GB, free space: 2.491GB, frag space: 0.573GB
do_liquid latency: 3.26s
INFO 09-05 12:37:10 llm_engine.py:758] Finished liquid for 111 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.02s, update worker latency: 0.00s, liquid model weights latency: 0.78s, init mem latency: 0.00s, liquid kvc latency: 1.24s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.242 GB, free space: 2.491GB, frag space: 0.573GB
INFO 09-05 12:37:11 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:11 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:37:11 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:11 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:37:11 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:37:11 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:37:11 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:37:11 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.242 GB, free space: 2.487GB, frag space: 0.573GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:11 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.289GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:11 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.299GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:11 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.172 GB, free space: 23.871GB, frag space: 0.454GB
It takes: 0.65s to send model shards
After sending shards, there are 8.81GB remaining on GPU0
INFO 09-05 12:37:11 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.75GB/s
INFO 09-05 12:37:11 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.914 GB, free space: 8.815GB, frag space: 0.438GB
INFO 09-05 12:37:12 cache_engine.py:146] send kvc shards takes: 0.71s, sent out: 7.38GB, sent bw: 10.32GB/s
INFO 09-05 12:37:12 cache_engine.py:186] After deleting layer's shard, free mem: 9026.31MB
INFO 09-05 12:37:12 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:37:12 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.539 GB, free space: 16.190GB, frag space: 0.442GB
INFO 09-05 12:37:12 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5299
INFO 09-05 12:37:13 worker.py:205] extend gpu in worker takes: 0.47s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:13 worker.py:205] extend gpu in worker takes: 0.47s
INFO 09-05 12:37:13 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.47s, after extending gpu blocks: allocated space on GPU: 27.417 GB, reserved space on GPU: 27.852 GB, free space: 2.875GB, frag space: 0.435GB
do_liquid latency: 2.61s
INFO 09-05 12:37:13 llm_engine.py:758] Finished liquid for 112 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.54s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.84s;, current mem info on GPU0: allocated space on GPU: 27.417 GB, reserved space on GPU: 27.852 GB, free space: 2.875GB, frag space: 0.435GB
INFO 09-05 12:37:14 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:37:14 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.539 GB, free space: 16.184GB, frag space: 0.442GB
INFO 09-05 12:37:14 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.539 GB, free space: 16.184GB, frag space: 0.442GB
INFO 09-05 12:37:15 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.389 GB, free space: 10.334GB, frag space: 0.099GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.68s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 22.85GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:15 worker.py:152] send weights shards takes: 0.72s, sent out: 6.19GB, sent bw: 8.58GB/s
INFO 09-05 12:37:15 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.805 GB, free space: 9.918GB, frag space: 0.514GB
INFO 09-05 12:37:15 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.805 GB, free space: 9.918GB, frag space: 0.514GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:16 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.20GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:16 cache_engine.py:186] After deleting layer's shard, free mem: 23396.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:16 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:37:16 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.180 GB, free space: 2.545GB, frag space: 0.510GB
do_liquid latency: 3.27s
INFO 09-05 12:37:16 llm_engine.py:758] Finished liquid for 113 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.03s, update worker latency: 0.00s, liquid model weights latency: 0.79s, init mem latency: 0.00s, liquid kvc latency: 1.24s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.180 GB, free space: 2.545GB, frag space: 0.510GB
INFO 09-05 12:37:17 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:37:17 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:17 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:17 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:37:17 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:37:17 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:37:17 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:37:17 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.180 GB, free space: 2.539GB, frag space: 0.510GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:17 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.282GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:17 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.291GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:18 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.156 GB, free space: 23.879GB, frag space: 0.438GB
It takes: 0.65s to send model shards
After sending shards, there are 8.86GB remaining on GPU0
INFO 09-05 12:37:18 worker.py:152] send weights shards takes: 0.72s, sent out: 6.19GB, sent bw: 8.64GB/s
INFO 09-05 12:37:18 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.857 GB, free space: 8.862GB, frag space: 0.382GB
INFO 09-05 12:37:18 cache_engine.py:146] send kvc shards takes: 0.72s, sent out: 7.38GB, sent bw: 10.31GB/s
INFO 09-05 12:37:18 cache_engine.py:186] After deleting layer's shard, free mem: 9074.31MB
INFO 09-05 12:37:18 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:37:18 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.482 GB, free space: 16.237GB, frag space: 0.386GB
INFO 09-05 12:37:18 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5311
INFO 09-05 12:37:19 worker.py:205] extend gpu in worker takes: 0.46s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:19 worker.py:205] extend gpu in worker takes: 0.46s
INFO 09-05 12:37:19 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.46s, after extending gpu blocks: allocated space on GPU: 27.468 GB, reserved space on GPU: 27.795 GB, free space: 2.926GB, frag space: 0.327GB
do_liquid latency: 2.62s
INFO 09-05 12:37:19 llm_engine.py:758] Finished liquid for 114 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.56s, update worker latency: 0.00s, liquid model weights latency: 0.72s, init mem latency: 0.00s, liquid kvc latency: 0.85s;, current mem info on GPU0: allocated space on GPU: 27.468 GB, reserved space on GPU: 27.795 GB, free space: 2.926GB, frag space: 0.327GB
INFO 09-05 12:37:20 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:37:20 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.482 GB, free space: 16.237GB, frag space: 0.386GB
INFO 09-05 12:37:20 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.482 GB, free space: 16.237GB, frag space: 0.386GB
INFO 09-05 12:37:21 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.395 GB, free space: 10.325GB, frag space: 0.105GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.67s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 22.84GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:21 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.72GB/s
INFO 09-05 12:37:21 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.998 GB, free space: 9.721GB, frag space: 0.708GB
INFO 09-05 12:37:21 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.998 GB, free space: 9.721GB, frag space: 0.708GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:22 cache_engine.py:146] send kvc shards takes: 0.79s, sent out: 7.38GB, sent bw: 9.31GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:22 cache_engine.py:186] After deleting layer's shard, free mem: 23392.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:22 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:37:22 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.373 GB, free space: 2.348GB, frag space: 0.704GB
do_liquid latency: 3.26s
INFO 09-05 12:37:22 llm_engine.py:758] Finished liquid for 115 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.99s, update worker latency: 0.00s, liquid model weights latency: 0.78s, init mem latency: 0.00s, liquid kvc latency: 1.21s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.373 GB, free space: 2.348GB, frag space: 0.704GB
INFO 09-05 12:37:23 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:23 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:37:23 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:23 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:37:23 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:37:23 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:37:23 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:37:23 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.373 GB, free space: 2.342GB, frag space: 0.704GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:23 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.278GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:24 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.287GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:24 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.156 GB, free space: 23.875GB, frag space: 0.438GB
It takes: 0.67s to send model shards
After sending shards, there are 8.89GB remaining on GPU0
INFO 09-05 12:37:24 worker.py:152] send weights shards takes: 0.73s, sent out: 6.19GB, sent bw: 8.48GB/s
INFO 09-05 12:37:24 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.826 GB, free space: 8.889GB, frag space: 0.351GB
INFO 09-05 12:37:24 cache_engine.py:146] send kvc shards takes: 0.72s, sent out: 7.38GB, sent bw: 10.31GB/s
INFO 09-05 12:37:24 cache_engine.py:186] After deleting layer's shard, free mem: 9102.31MB
INFO 09-05 12:37:24 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:37:24 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.451 GB, free space: 16.264GB, frag space: 0.354GB
INFO 09-05 12:37:24 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5318
INFO 09-05 12:37:25 worker.py:205] extend gpu in worker takes: 0.48s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:25 worker.py:205] extend gpu in worker takes: 0.48s
INFO 09-05 12:37:25 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.48s, after extending gpu blocks: allocated space on GPU: 27.491 GB, reserved space on GPU: 27.826 GB, free space: 2.889GB, frag space: 0.335GB
do_liquid latency: 2.64s
INFO 09-05 12:37:25 llm_engine.py:758] Finished liquid for 116 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.56s, update worker latency: 0.00s, liquid model weights latency: 0.73s, init mem latency: 0.00s, liquid kvc latency: 0.83s;, current mem info on GPU0: allocated space on GPU: 27.491 GB, reserved space on GPU: 27.826 GB, free space: 2.889GB, frag space: 0.335GB
INFO 09-05 12:37:27 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:37:27 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.451 GB, free space: 16.264GB, frag space: 0.354GB
INFO 09-05 12:37:27 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.451 GB, free space: 16.264GB, frag space: 0.354GB
INFO 09-05 12:37:27 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.395 GB, free space: 10.321GB, frag space: 0.105GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.66s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 22.84GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:27 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.76GB/s
INFO 09-05 12:37:27 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.904 GB, free space: 9.811GB, frag space: 0.614GB
INFO 09-05 12:37:27 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.904 GB, free space: 9.811GB, frag space: 0.614GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:28 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.21GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:28 cache_engine.py:186] After deleting layer's shard, free mem: 23388.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:28 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:37:29 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.279 GB, free space: 2.436GB, frag space: 0.610GB
do_liquid latency: 3.33s
INFO 09-05 12:37:29 llm_engine.py:758] Finished liquid for 117 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.00s, update worker latency: 0.00s, liquid model weights latency: 0.77s, init mem latency: 0.00s, liquid kvc latency: 1.23s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.279 GB, free space: 2.436GB, frag space: 0.610GB
INFO 09-05 12:37:29 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:29 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:37:29 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:29 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:37:29 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:37:29 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:37:29 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:37:29 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.279 GB, free space: 2.432GB, frag space: 0.610GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:29 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.274GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:30 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.284GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:30 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.156 GB, free space: 23.871GB, frag space: 0.438GB
It takes: 0.64s to send model shards
After sending shards, there are 8.89GB remaining on GPU0
INFO 09-05 12:37:30 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.78GB/s
INFO 09-05 12:37:30 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.826 GB, free space: 8.885GB, frag space: 0.351GB
INFO 09-05 12:37:31 cache_engine.py:146] send kvc shards takes: 0.71s, sent out: 7.38GB, sent bw: 10.36GB/s
INFO 09-05 12:37:31 cache_engine.py:186] After deleting layer's shard, free mem: 9098.31MB
INFO 09-05 12:37:31 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:37:31 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.451 GB, free space: 16.260GB, frag space: 0.354GB
INFO 09-05 12:37:31 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5317
INFO 09-05 12:37:31 worker.py:205] extend gpu in worker takes: 0.45s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:31 worker.py:205] extend gpu in worker takes: 0.46s
INFO 09-05 12:37:31 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.46s, after extending gpu blocks: allocated space on GPU: 27.487 GB, reserved space on GPU: 27.826 GB, free space: 2.885GB, frag space: 0.339GB
do_liquid latency: 2.60s
INFO 09-05 12:37:31 llm_engine.py:758] Finished liquid for 118 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.55s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.85s;, current mem info on GPU0: allocated space on GPU: 27.487 GB, reserved space on GPU: 27.826 GB, free space: 2.885GB, frag space: 0.339GB
INFO 09-05 12:37:33 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:37:33 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.451 GB, free space: 16.260GB, frag space: 0.354GB
INFO 09-05 12:37:33 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.451 GB, free space: 16.260GB, frag space: 0.354GB
INFO 09-05 12:37:33 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.395 GB, free space: 10.317GB, frag space: 0.105GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.64s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 22.84GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:33 worker.py:152] send weights shards takes: 0.68s, sent out: 6.19GB, sent bw: 9.13GB/s
INFO 09-05 12:37:33 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.936 GB, free space: 9.776GB, frag space: 0.645GB
INFO 09-05 12:37:33 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.936 GB, free space: 9.776GB, frag space: 0.645GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:34 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.23GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:34 cache_engine.py:186] After deleting layer's shard, free mem: 23384.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:34 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:37:35 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.311 GB, free space: 2.401GB, frag space: 0.641GB
do_liquid latency: 3.26s
INFO 09-05 12:37:35 llm_engine.py:758] Finished liquid for 119 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.99s, update worker latency: 0.00s, liquid model weights latency: 0.75s, init mem latency: 0.00s, liquid kvc latency: 1.24s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.311 GB, free space: 2.401GB, frag space: 0.641GB
INFO 09-05 12:37:35 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:35 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:37:35 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:35 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:37:35 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-05 12:37:35 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:37:35 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:37:35 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.311 GB, free space: 2.397GB, frag space: 0.641GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:35 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.270GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:36 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.280GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:36 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.188 GB, free space: 23.836GB, frag space: 0.470GB
It takes: 0.64s to send model shards
After sending shards, there are 8.98GB remaining on GPU0
INFO 09-05 12:37:36 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.76GB/s
INFO 09-05 12:37:36 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.727 GB, free space: 8.981GB, frag space: 0.251GB
INFO 09-05 12:37:37 cache_engine.py:146] send kvc shards takes: 0.71s, sent out: 7.38GB, sent bw: 10.33GB/s
INFO 09-05 12:37:37 cache_engine.py:186] After deleting layer's shard, free mem: 9196.31MB
INFO 09-05 12:37:37 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:37:37 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.352 GB, free space: 16.356GB, frag space: 0.255GB
INFO 09-05 12:37:37 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5342
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:37 worker.py:205] extend gpu in worker takes: 0.48s
INFO 09-05 12:37:37 worker.py:205] extend gpu in worker takes: 0.48s
INFO 09-05 12:37:37 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.48s, after extending gpu blocks: allocated space on GPU: 27.593 GB, reserved space on GPU: 27.789 GB, free space: 2.918GB, frag space: 0.196GB
do_liquid latency: 2.62s
INFO 09-05 12:37:37 llm_engine.py:758] Finished liquid for 120 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.54s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.84s;, current mem info on GPU0: allocated space on GPU: 27.593 GB, reserved space on GPU: 27.789 GB, free space: 2.918GB, frag space: 0.196GB
INFO 09-05 12:37:39 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:37:39 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.352 GB, free space: 16.356GB, frag space: 0.255GB
INFO 09-05 12:37:39 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.352 GB, free space: 16.356GB, frag space: 0.255GB
INFO 09-05 12:37:39 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.389 GB, free space: 10.319GB, frag space: 0.099GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.66s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 22.83GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:39 worker.py:152] send weights shards takes: 0.70s, sent out: 6.19GB, sent bw: 8.89GB/s
INFO 09-05 12:37:39 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.898 GB, free space: 9.809GB, frag space: 0.608GB
INFO 09-05 12:37:39 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.898 GB, free space: 9.809GB, frag space: 0.608GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:40 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.21GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:40 cache_engine.py:186] After deleting layer's shard, free mem: 23380.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:40 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:37:41 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.273 GB, free space: 2.434GB, frag space: 0.604GB
do_liquid latency: 3.28s
INFO 09-05 12:37:41 llm_engine.py:758] Finished liquid for 121 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.02s, update worker latency: 0.00s, liquid model weights latency: 0.76s, init mem latency: 0.00s, liquid kvc latency: 1.25s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.273 GB, free space: 2.434GB, frag space: 0.604GB
INFO 09-05 12:37:41 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:41 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:37:41 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:41 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:37:41 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:37:41 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:37:41 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:37:41 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.273 GB, free space: 2.430GB, frag space: 0.604GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:41 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.266GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:42 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.276GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:42 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.188 GB, free space: 23.832GB, frag space: 0.470GB
It takes: 0.65s to send model shards
After sending shards, there are 8.73GB remaining on GPU0
INFO 09-05 12:37:42 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.75GB/s
INFO 09-05 12:37:42 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.977 GB, free space: 8.727GB, frag space: 0.501GB
INFO 09-05 12:37:43 cache_engine.py:146] send kvc shards takes: 0.71s, sent out: 7.38GB, sent bw: 10.37GB/s
INFO 09-05 12:37:43 cache_engine.py:186] After deleting layer's shard, free mem: 8936.31MB
INFO 09-05 12:37:43 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:37:43 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.602 GB, free space: 16.102GB, frag space: 0.505GB
INFO 09-05 12:37:43 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5277
INFO 09-05 12:37:43 worker.py:205] extend gpu in worker takes: 0.49s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:43 worker.py:205] extend gpu in worker takes: 0.49s
INFO 09-05 12:37:43 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.49s, after extending gpu blocks: allocated space on GPU: 27.343 GB, reserved space on GPU: 27.789 GB, free space: 2.914GB, frag space: 0.446GB
do_liquid latency: 2.62s
INFO 09-05 12:37:43 llm_engine.py:758] Finished liquid for 122 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.53s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.82s;, current mem info on GPU0: allocated space on GPU: 27.343 GB, reserved space on GPU: 27.789 GB, free space: 2.914GB, frag space: 0.446GB
INFO 09-05 12:37:45 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:37:45 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.602 GB, free space: 16.102GB, frag space: 0.505GB
INFO 09-05 12:37:45 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.602 GB, free space: 16.102GB, frag space: 0.505GB
INFO 09-05 12:37:45 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.389 GB, free space: 10.315GB, frag space: 0.099GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.66s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 22.83GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:45 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.75GB/s
INFO 09-05 12:37:46 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.898 GB, free space: 9.805GB, frag space: 0.608GB
INFO 09-05 12:37:46 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.898 GB, free space: 9.805GB, frag space: 0.608GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:46 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.17GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:46 cache_engine.py:186] After deleting layer's shard, free mem: 23376.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:46 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:37:47 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.273 GB, free space: 2.430GB, frag space: 0.604GB
do_liquid latency: 3.26s
INFO 09-05 12:37:47 llm_engine.py:758] Finished liquid for 123 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 2.01s, update worker latency: 0.00s, liquid model weights latency: 0.77s, init mem latency: 0.00s, liquid kvc latency: 1.24s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.273 GB, free space: 2.430GB, frag space: 0.604GB
INFO 09-05 12:37:47 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:47 utils.py:623] Found nccl from library libnccl.so.2
INFO 09-05 12:37:47 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:47 pynccl.py:65] vLLM is using nccl==2.20.5
WARNING 09-05 12:37:47 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:37:47 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:37:47 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:37:47 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.273 GB, free space: 2.426GB, frag space: 0.604GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:47 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.262GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:48 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.272GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:48 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.172 GB, free space: 23.844GB, frag space: 0.454GB
It takes: 0.65s to send model shards
After sending shards, there are 8.79GB remaining on GPU0
INFO 09-05 12:37:48 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.70GB/s
INFO 09-05 12:37:48 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 21.914 GB, free space: 8.785GB, frag space: 0.438GB
INFO 09-05 12:37:49 cache_engine.py:146] send kvc shards takes: 0.72s, sent out: 7.38GB, sent bw: 10.25GB/s
INFO 09-05 12:37:49 cache_engine.py:186] After deleting layer's shard, free mem: 8996.31MB
INFO 09-05 12:37:49 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:37:49 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.539 GB, free space: 16.160GB, frag space: 0.442GB
INFO 09-05 12:37:49 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5292
INFO 09-05 12:37:50 worker.py:205] extend gpu in worker takes: 0.47s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:50 worker.py:205] extend gpu in worker takes: 0.47s
INFO 09-05 12:37:50 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.47s, after extending gpu blocks: allocated space on GPU: 27.405 GB, reserved space on GPU: 27.789 GB, free space: 2.910GB, frag space: 0.384GB
do_liquid latency: 2.63s
INFO 09-05 12:37:50 llm_engine.py:758] Finished liquid for 124 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.55s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.84s;, current mem info on GPU0: allocated space on GPU: 27.405 GB, reserved space on GPU: 27.789 GB, free space: 2.910GB, frag space: 0.384GB
INFO 09-05 12:37:51 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:37:51 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.539 GB, free space: 16.160GB, frag space: 0.442GB
INFO 09-05 12:37:51 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.539 GB, free space: 16.160GB, frag space: 0.442GB
INFO 09-05 12:37:52 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.389 GB, free space: 10.311GB, frag space: 0.099GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.66s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 22.82GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:52 worker.py:152] send weights shards takes: 0.70s, sent out: 6.19GB, sent bw: 8.85GB/s
INFO 09-05 12:37:52 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.773 GB, free space: 9.926GB, frag space: 0.483GB
INFO 09-05 12:37:52 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.773 GB, free space: 9.926GB, frag space: 0.483GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:52 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.19GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:52 cache_engine.py:186] After deleting layer's shard, free mem: 23372.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:52 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:37:53 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.148 GB, free space: 2.551GB, frag space: 0.479GB
do_liquid latency: 3.24s
INFO 09-05 12:37:53 llm_engine.py:758] Finished liquid for 125 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.98s, update worker latency: 0.00s, liquid model weights latency: 0.77s, init mem latency: 0.00s, liquid kvc latency: 1.22s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.148 GB, free space: 2.551GB, frag space: 0.479GB
INFO 09-05 12:37:53 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:53 utils.py:623] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:53 pynccl.py:65] vLLM is using nccl==2.20.5
INFO 09-05 12:37:53 pynccl.py:65] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=340191)[0;0m WARNING 09-05 12:37:54 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-05 12:37:54 custom_all_reduce.py:179] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-05 12:37:54 multiproc_gpu_executor.py:249] Start to do liquid from src: 0 to dst: 1 with shard_ids: [1]
INFO 09-05 12:37:54 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.148 GB, free space: 2.547GB, frag space: 0.479GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:54 worker.py:165] Before appending weights shards, allocated space on GPU: 0.525 GB, reserved space on GPU: 0.754 GB, free space: 30.258GB, frag space: 0.229GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:54 worker.py:168] After recving weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 6.744 GB, free space: 24.268GB, frag space: 0.026GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:54 worker.py:171] After appending weights shards, allocated space on GPU: 6.718 GB, reserved space on GPU: 7.156 GB, free space: 23.856GB, frag space: 0.438GB
It takes: 0.65s to send model shards
After sending shards, there are 8.69GB remaining on GPU0
INFO 09-05 12:37:54 worker.py:152] send weights shards takes: 0.71s, sent out: 6.19GB, sent bw: 8.76GB/s
INFO 09-05 12:37:54 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 21.476 GB, reserved space on GPU: 22.002 GB, free space: 8.694GB, frag space: 0.526GB
INFO 09-05 12:37:55 cache_engine.py:146] send kvc shards takes: 0.71s, sent out: 7.38GB, sent bw: 10.35GB/s
INFO 09-05 12:37:55 cache_engine.py:186] After deleting layer's shard, free mem: 8902.31MB
INFO 09-05 12:37:55 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 1
INFO 09-05 12:37:55 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.627 GB, free space: 16.069GB, frag space: 0.530GB
INFO 09-05 12:37:55 multiproc_gpu_executor.py:167] After scale out, num_gpu_blocks: #5268
INFO 09-05 12:37:56 worker.py:205] extend gpu in worker takes: 0.46s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:56 worker.py:205] extend gpu in worker takes: 0.45s
INFO 09-05 12:37:56 multiproc_gpu_executor.py:171] extending gpu blocks takes: 0.46s, after extending gpu blocks: allocated space on GPU: 27.296 GB, reserved space on GPU: 27.814 GB, free space: 2.879GB, frag space: 0.519GB
do_liquid latency: 2.60s
INFO 09-05 12:37:56 llm_engine.py:758] Finished liquid for 126 times, output: Completed! Move shard: [1] from 0 to 1;liquid e2e latency: 1.56s, update worker latency: 0.00s, liquid model weights latency: 0.71s, init mem latency: 0.00s, liquid kvc latency: 0.85s;, current mem info on GPU0: allocated space on GPU: 27.296 GB, reserved space on GPU: 27.814 GB, free space: 2.879GB, frag space: 0.519GB
INFO 09-05 12:37:57 multiproc_gpu_executor.py:249] Start to do liquid from src: 1 to dst: 0 with shard_ids: [1]
INFO 09-05 12:37:57 multiproc_gpu_executor.py:261] Before liquid model weights, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.627 GB, free space: 16.067GB, frag space: 0.530GB
INFO 09-05 12:37:57 worker.py:165] Before appending weights shards, allocated space on GPU: 14.097 GB, reserved space on GPU: 14.627 GB, free space: 16.067GB, frag space: 0.530GB
INFO 09-05 12:37:58 worker.py:168] After recving weights shards, allocated space on GPU: 20.289 GB, reserved space on GPU: 20.383 GB, free space: 10.311GB, frag space: 0.093GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m It takes: 0.66s to send model shards
[1;36m(VllmWorkerProcess pid=340191)[0;0m After sending shards, there are 22.82GB remaining on GPU0
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:58 worker.py:152] send weights shards takes: 0.69s, sent out: 6.19GB, sent bw: 8.93GB/s
INFO 09-05 12:37:58 worker.py:171] After appending weights shards, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.736 GB, free space: 9.957GB, frag space: 0.446GB
INFO 09-05 12:37:58 multiproc_gpu_executor.py:266] After liquid model weights, allocated space on GPU: 20.290 GB, reserved space on GPU: 20.736 GB, free space: 9.957GB, frag space: 0.446GB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:58 cache_engine.py:146] send kvc shards takes: 0.80s, sent out: 7.38GB, sent bw: 9.26GB/s
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:58 cache_engine.py:186] After deleting layer's shard, free mem: 23366.31MB
[1;36m(VllmWorkerProcess pid=340191)[0;0m INFO 09-05 12:37:59 cache_engine.py:152] Successfully send kv cache shards: [1] to rank: 0
INFO 09-05 12:37:59 multiproc_gpu_executor.py:276] After liquid kvc, allocated space on GPU: 27.669 GB, reserved space on GPU: 28.111 GB, free space: 2.582GB, frag space: 0.442GB
do_liquid latency: 3.23s
INFO 09-05 12:37:59 llm_engine.py:758] Finished liquid for 127 times, output: Completed! Move shard: [1] from 1 to 0;liquid e2e latency: 1.98s, update worker latency: 0.00s, liquid model weights latency: 0.76s, init mem latency: 0.00s, liquid kvc latency: 1.22s;, current mem info on GPU0: allocated space on GPU: 27.669 GB, reserved space on GPU: 28.111 GB, free space: 2.582GB, frag space: 0.442GB
output: what is LLM?what is LLM?what is LLM?what is LLM?what is LLM?what is LLM?what is LLM?what is LLM?what is LLM?what is LLM?what is LLM?what is LLM?what is LLM?what is LLM?what is LLM?what is LLM?what is LLM?what is LLM?what is LLM?what is LLM?what is LLM?what is LLM?what is LLM?what is LLM?what is LLM?what is LL
INFO 09-05 12:38:00 multiproc_worker_utils.py:123] Killing local vLLM worker processes
